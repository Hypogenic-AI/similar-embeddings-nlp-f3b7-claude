{"title": "A Survey of Cross-lingual Word Embedding Models", "year": 2017, "authors": "Sebastian Ruder, Ivan Vulic, Anders S\u00f8gaard", "url": "https://api.semanticscholar.org/CorpusId:26127787", "relevance": 3, "abstract": "Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent, modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.", "citations": 566}
{"title": "Unsupervised Multilingual Word Embeddings", "year": 2018, "authors": "Xilun Chen, Claire Cardie", "url": "https://api.semanticscholar.org/CorpusId:52099904", "relevance": 3, "abstract": "Multilingual Word Embeddings (MWEs) represent words from multiple languages in a single distributional vector space. Unsupervised MWE (UMWE) methods acquire multilingual embeddings without cross-lingual supervision, which is a significant advantage over traditional supervised approaches and opens many new possibilities for low-resource languages. Prior art for learning UMWEs, however, merely relies on a number of independently trained Unsupervised Bilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These methods fail to leverage the interdependencies that exist among many languages. To address this shortcoming, we propose a fully unsupervised framework for learning MWEs that directly exploits the relations between all language pairs. Our model substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition, our model even beats supervised approaches trained with cross-lingual resources.", "citations": 138}
{"title": "Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment", "year": 2018, "authors": "Muhao Chen, Yingtao Tian, Kai-Wei Chang, S. Skiena, C. Zaniolo", "url": "https://www.semanticscholar.org/paper/5902fa7e6f637fea16ff325ab0e25c88bf31f27a", "relevance": 3, "abstract": "Multilingual knowledge graph (KG) embeddings provide latent semantic representations of entities and structured knowledge with cross-lingual inferences, which benefit various knowledge-driven cross-lingual NLP tasks. However, precisely learning such cross-lingual inferences is usually hindered by the low coverage of entity alignment in many KGs. Since many multilingual KGs also provide literal descriptions of entities, in this paper, we introduce an embedding-based approach which leverages a weakly aligned multilingual KG for semi-supervised cross-lingual learning using entity descriptions. Our approach performs co-training of two embedding models, i.e. a multilingual KG embedding model and a multilingual literal description embedding model. The models are trained on a large Wikipedia-based trilingual dataset where most entity alignment is unknown to training. Experimental results show that the performance of the proposed approach on the entity alignment task improves at each iteration of co-training, and eventually reaches a stage at which it significantly surpasses previous approaches. We also show that our approach has promising abilities for zero-shot entity alignment, and cross-lingual KG completion.", "citations": 247}
{"title": "Multilingual Models for Compositional Distributed Semantics", "year": 2014, "authors": "Karl Moritz Hermann, Phil Blunsom", "url": "https://api.semanticscholar.org/CorpusId:17589422", "relevance": 3, "abstract": "We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data.", "citations": 320}
{"title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer", "year": 2020, "authors": "Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang, Ahmed Hassan Awadallah", "url": "https://api.semanticscholar.org/CorpusId:218487087", "relevance": 3, "abstract": "Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.", "citations": 95}
{"title": "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity", "year": 2017, "authors": "Jos\u00e9 Camacho-Collados, Mohammad Taher Pilehvar, Nigel Collier, Roberto Navigli", "url": "https://api.semanticscholar.org/CorpusId:7665329", "relevance": 3, "abstract": "This paper introduces a new task on Multilingual and Cross-lingual SemanticThis paper introduces a new task on Multilingual and Cross-lingual Semantic Word Similarity which measures the semantic similarity of word pairs within and across five languages: English, Farsi, German, Italian and Spanish. High quality datasets were manually curated for the five languages with high inter-annotator agreements (consistently in the 0.9 ballpark). These were used for semi-automatic construction of ten cross-lingual datasets. 17 teams participated in the task, submitting 24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri.org/semeval2017/task2/", "citations": 157}
{"title": "Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples", "year": 2025, "authors": "Andrianos Michail, Simon Clematide, Rico Sennrich", "url": "https://api.semanticscholar.org/CorpusId:276287513", "relevance": 3, "abstract": "The evaluation of cross-lingual semantic search models is often limited to existing datasets from tasks such as information retrieval and semantic textual similarity. We introduce Cross-Lingual Semantic Discrimination (CLSD), a lightweight evaluation task that requires only parallel sentences and a Large Language Model (LLM) to generate adversarial distractors. CLSD measures an embedding model's ability to rank the true parallel sentence above semantically misleading but lexically similar alternatives. As a case study, we construct CLSD datasets for German--French in the news domain. Our experiments show that models fine-tuned for retrieval tasks benefit from pivoting through English, whereas bitext mining models perform best in direct cross-lingual settings. A fine-grained similarity analysis further reveals that embedding models differ in their sensitivity to linguistic perturbations. We release our code and datasets under AGPL-3.0: https://github.com/impresso/cross_lingual_semantic_discrimination", "citations": 4}
{"title": "Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models", "year": 2019, "authors": "Takashi Wada, Tomoharu Iwata, Yuji Matsumoto", "url": "https://api.semanticscholar.org/CorpusId:150149297", "relevance": 3, "abstract": "Recently, a variety of unsupervised methods have been proposed that map pre-trained word embeddings of different languages into the same space without any parallel data. These methods aim to find a linear transformation based on the assumption that monolingual word embeddings are approximately isomorphic between languages. However, it has been demonstrated that this assumption holds true only on specific conditions, and with limited resources, the performance of these methods decreases drastically. To overcome this problem, we propose a new unsupervised multilingual embedding method that does not rely on such assumption and performs well under resource-poor scenarios, namely when only a small amount of monolingual data (i.e., 50k sentences) are available, or when the domains of monolingual data are different across languages. Our proposed model, which we call \u2018Multilingual Neural Language Models\u2019, shares some of the network parameters among multiple languages, and encodes sentences of multiple languages into the same space. The model jointly learns word embeddings of different languages in the same space, and generates multilingual embeddings without any parallel data or pre-training. Our experiments on word alignment tasks have demonstrated that, on the low-resource condition, our model substantially outperforms existing unsupervised and even supervised methods trained with 500 bilingual pairs of words. Our model also outperforms unsupervised methods given different-domain corpora across languages. Our code is publicly available.", "citations": 25}
{"title": "Cross-lingual transfer of sentiment classifiers", "year": 2021, "authors": "M. Robnik-Sikonja, Kristjan Reba, I. Mozeti\u010d", "url": "https://api.semanticscholar.org/CorpusId:237838256", "relevance": 3, "abstract": "Word embeddings represent words in a numeric space so that semantic relations between words are represented as distances and directions in the vector space. Cross-lingual word embeddings transform vector spaces of different languages so that similar words are aligned. This is done by mapping one language\u2019s vector space to the vector space of another language or by construction of a joint vector space for multiple languages. Cross-lingual embeddings can be used to transfer machine learning models between languages, thereby compensating for insufficient data in less-resourced languages. We use cross-lingual word embeddings to transfer machine learning prediction models for Twitter sentiment between 13 languages. We focus on two transfer mechanisms that recently show superior transfer performance. The first mechanism uses the trained models whose input is the joint numerical space for many languages as implemented in the LASER library. The second mechanism uses large pretrained multilingual BERT language models. Our experiments show that the transfer of models between similar languages is sensible, even with no target language data. The performance of cross-lingual models obtained with the multilingual BERT and LASER library is comparable, and the differences are language-dependent. The transfer with CroSloEngual BERT, pretrained on only three languages, is superior on these and some closely related languages.", "citations": 9}
{"title": "Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment", "year": 2024, "authors": "Zhongtao Miao, Qiyu Wu, Kaiyan Zhao, Zilong Wu, Yoshimasa Tsuruoka", "url": "https://api.semanticscholar.org/CorpusId:268875764", "relevance": 3, "abstract": "The field of cross-lingual sentence embeddings has recently experienced significant advancements, but research concerning low-resource languages has lagged due to the scarcity of parallel corpora. This paper shows that cross-lingual word representation in low-resource languages is notably under-aligned with that in high-resource languages in current models. To address this, we introduce a novel framework that explicitly aligns words between English and eight low-resource languages, utilizing off-the-shelf word alignment models. This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking. We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on sentence embeddings in low-resource languages. In addition, the competitive performance of the proposed model across a broader range of tasks in high-resource languages underscores its practicality.", "citations": 20}
{"title": "Learning Unsupervised Multilingual Word Embeddings with Incremental Multilingual Hubs", "year": 2019, "authors": "Geert Heyman, B. Verreet, Ivan Vulic, Marie-Francine Moens", "url": "https://api.semanticscholar.org/CorpusId:174800783", "relevance": 3, "abstract": "Recent research has discovered that a shared bilingual word embedding space can be induced by projecting monolingual word embedding spaces from two languages using a self-learning paradigm without any bilingual supervision. However, it has also been shown that for distant language pairs such fully unsupervised self-learning methods are unstable and often get stuck in poor local optima due to reduced isomorphism between starting monolingual spaces. In this work, we propose a new robust framework for learning unsupervised multilingual word embeddings that mitigates the instability issues. We learn a shared multilingual embedding space for a variable number of languages by incrementally adding new languages one by one to the current multilingual space. Through the gradual language addition the method can leverage the interdependencies between the new language and all other languages in the current multilingual space. We find that it is beneficial to project more distant languages later in the iterative process. Our fully unsupervised multilingual embedding spaces yield results that are on par with the state-of-the-art methods in the bilingual lexicon induction (BLI) task, and simultaneously obtain state-of-the-art scores on two downstream tasks: multilingual document classification and multilingual dependency parsing, outperforming even supervised baselines. This finding also accentuates the need to establish evaluation protocols for cross-lingual word embeddings beyond the omnipresent intrinsic BLI task in future work.", "citations": 23}
{"title": "Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language Models", "year": 2018, "authors": "Takashi Wada, Tomoharu Iwata", "url": "https://api.semanticscholar.org/CorpusId:52179115", "relevance": 3, "abstract": "We propose an unsupervised method to obtain cross-lingual embeddings without any parallel data or pre-trained word embeddings. The proposed model, which we call multilingual neural language models, takes sentences of multiple languages as an input. The proposed model contains bidirectional LSTMs that perform as forward and backward language models, and these networks are shared among all the languages. The other parameters, i.e. word embeddings and linear transformation between hidden states and outputs, are specific to each language. The shared LSTMs can capture the common sentence structure among all languages. Accordingly, word embeddings of each language are mapped into a common latent space, making it possible to measure the similarity of words across multiple languages. We evaluate the quality of the cross-lingual word embeddings on a word alignment task. Our experiments demonstrate that our model can obtain cross-lingual embeddings of much higher quality than existing unsupervised models when only a small amount of monolingual data (i.e. 50k sentences) are available, or the domains of monolingual data are different across languages.", "citations": 28}
{"title": "Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation", "year": 2023, "authors": "Di Wu, C. Monz", "url": "https://api.semanticscholar.org/CorpusId:258841499", "relevance": 3, "abstract": "Using a vocabulary that is shared across languages is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, assuming that shared tokens refer to similar meanings across languages. However, when word overlap is small, especially due to different writing systems, transfer is inhibited. In this paper, we define word-level information transfer pathways via word equivalence classes and rely on graph networks to fuse word embeddings across languages. Our experiments demonstrate the advantages of our approach: 1) embeddings of words with similar meanings are better aligned across languages, 2) our method achieves consistent BLEU improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less than 1.0\\% additional trainable parameters are required with a limited increase in computational costs, while inference time remains identical to the baseline. We release the codebase to the community.", "citations": 10}
{"title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?", "year": 2024, "authors": "Xinyu Crystina Zhang, Jing Lu, Vinh Q. Tran, Tal Schuster, Donald Metzler, Jimmy Lin", "url": "https://api.semanticscholar.org/CorpusId:273877383", "relevance": 3, "abstract": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form\"semantic tokens\"by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.", "citations": 2}
{"title": "Cross-lingual Transfer of Twitter Sentiment Models Using a Common Vector Space", "year": 2020, "authors": "M. Robnik-Sikonja, Kristjan Reba, I. Mozeti\u010d", "url": "https://api.semanticscholar.org/CorpusId:218665268", "relevance": 3, "abstract": "Word embeddings represent words in a numeric space in such a way that semantic relations between words are encoded as distances and directions in the vector space. Cross-lingual word embeddings map words from one language to the vector space of another language, or words from multiple languages to the same vector space where similar words are aligned. Cross-lingual embeddings can be used to transfer machine learning models between languages and thereby compensate for insufficient data in less-resourced languages. We use cross-lingual word embeddings to transfer machine learning prediction models for Twitter sentiment between 13 languages. We focus on two transfer mechanisms using the joint numerical space for many languages as implemented in the LASER library: the transfer of trained models, and expansion of training sets with instances from other languages. Our experiments show that the transfer of models between similar languages is sensible, while dataset expansion did not increase the predictive performance.", "citations": 6}
{"title": "Cross-Lingual BERT Contextual Embedding Space Mapping with Isotropic and Isometric Conditions", "year": 2021, "authors": "Haoran Xu, Philipp Koehn", "url": "https://api.semanticscholar.org/CorpusId:236133964", "relevance": 3, "abstract": "Typically, a linearly orthogonal transformation mapping is learned by aligning static type-level embeddings to build a shared semantic space. In view of the analysis that contextual embeddings contain richer semantic features, we investigate a context-aware and dictionary-free mapping approach by leveraging parallel corpora. We illustrate that our contextual embedding space mapping significantly outperforms previous multilingual word embedding methods on the bilingual dictionary induction (BDI) task by providing a higher degree of isomorphism. To improve the quality of mapping, we also explore sense-level embeddings that are split from type-level representations, which can align spaces in a finer resolution and yield more precise mapping. Moreover, we reveal that contextual embedding spaces suffer from their natural properties -- anisotropy and anisometry. To mitigate these two problems, we introduce the iterative normalization algorithm as an imperative preprocessing step. Our findings unfold the tight relationship between isotropy, isometry, and isomorphism in normalized contextual embedding spaces.", "citations": 13}
{"title": "A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings", "year": 2019, "authors": "Niels van der Heijden, Samira Abnar, Ekaterina Shutova", "url": "https://api.semanticscholar.org/CorpusId:209444184", "relevance": 3, "abstract": "The lack of annotated data in many languages is a well-known challenge within the field of multilingual natural language processing (NLP). Therefore, many recent studies focus on zero-shot transfer learning and joint training across languages to overcome data scarcity for low-resource languages. In this work we (i) perform a comprehensive comparison of state-of-the-art multilingual word and sentence encoders on the tasks of named entity recognition (NER) and part of speech (POS) tagging; and (ii) propose a new method for creating multilingual contextualized word embeddings, compare it to multiple baselines and show that it performs at or above state-of-the-art level in zero-shot transfer settings. Finally, we show that our method allows for better knowledge sharing across languages in a joint training setting.", "citations": 16}
{"title": "When Word Embeddings Become Endangered", "year": 2021, "authors": "Khalid Alnajjar", "url": "https://api.semanticscholar.org/CorpusId:232335507", "relevance": 3, "abstract": "Big languages such as English and Finnish have many natural language processing (NLP) resources and models, but this is not the case for low-resourced and endangered languages as such resources are so scarce despite the great advantages they would provide for the language communities. The most common types of resources available for low-resourced and endangered languages are translation dictionaries and universal dependencies. In this paper, we present a method for constructing word embeddings for endangered languages using existing word embeddings of different resource-rich languages and the translation dictionaries of resource-poor languages. Thereafter, the embeddings are fine-tuned using the sentences in the universal dependencies and aligned to match the semantic spaces of the big languages; resulting in cross-lingual embeddings. The endangered languages we work with here are Erzya, Moksha, Komi-Zyrian and Skolt Sami. Furthermore, we build a universal sentiment analysis model for all the languages that are part of this study, whether endangered or not, by utilizing cross-lingual word embeddings. The evaluation conducted shows that our word embeddings for endangered languages are well-aligned with the resource-rich languages, and they are suitable for training task-specific models as demonstrated by our sentiment analysis models which achieved high accuracies. All our cross-lingual word embeddings and sentiment analysis models will be released openly via an easy-to-use Python library.", "citations": 11}
{"title": "Learning Multilingual Word Embeddings Using Image-Text Data", "year": 2019, "authors": "K. Singhal, K. Raman, B. T. Cate", "url": "https://api.semanticscholar.org/CorpusId:168170148", "relevance": 3, "abstract": "There has been significant interest recently in learning multilingual word embeddings \u2013 in which semantically similar words across languages have similar embeddings. State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings. In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data. In particular, we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text. Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.", "citations": 10}
{"title": "Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings", "year": 2021, "authors": "Linlin Liu, Thien Hai Nguyen, Shafiq R. Joty, Lidong Bing, Luo Si", "url": "https://api.semanticscholar.org/CorpusId:232185390", "relevance": 3, "abstract": "Cross-lingual word embeddings (CLWE) have been proven useful in many cross-lingual tasks. However, most existing approaches to learn CLWE including the ones with contextual embeddings are sense agnostic. In this work, we propose a novel framework to align contextual embeddings at the sense level by leveraging cross-lingual signal from bilingual dictionaries only. We operationalize our framework by first proposing a novel sense-aware cross entropy loss to model word senses explicitly. The monolingual ELMo and BERT models pretrained with our sense-aware cross entropy loss demonstrate significant performance improvement for word sense disambiguation tasks. We then propose a sense alignment objective on top of the sense-aware cross entropy loss for cross-lingual model pretraining, and pretrain cross-lingual models for several language pairs (English to German/Spanish/Japanese/Chinese). Compared with the best baseline results, our cross-lingual models achieve 0.52%, 2.09% and 1.29% average performance improvements on zero-shot cross-lingual NER, sentiment classification and XNLI tasks, respectively.", "citations": 6}
{"title": "Meemi: A Simple Method for Post-processing Cross-lingual Word Embeddings", "year": 2019, "authors": "Yerai Doval, Jos\u00e9 Camacho-Collados, Luis Espinosa Anke, S. Schockaert", "url": "https://api.semanticscholar.org/CorpusId:204824077", "relevance": 3, "abstract": "Word embeddings have become a standard resource in the toolset of any Natural Language Processing practitioner. While monolingual word embeddings encode information about words in the context of a particular language, cross-lingual embeddings define a multilingual space where word embeddings from two or more languages are integrated together. Current state-of-the-art approaches learn these embeddings by aligning two disjoint monolingual vector spaces through an orthogonal transformation which preserves the structure of the monolingual counterparts. In this work, we propose to apply an additional transformation after this initial alignment step, which aims to bring the vector representations of a given word and its translations closer to their average. Since this additional transformation is non-orthogonal, it also affects the structure of the monolingual spaces. We show that our approach both improves the integration of the monolingual spaces as well as the quality of the monolingual spaces themselves. Furthermore, because our transformation can be applied to an arbitrary number of languages, we are able to effectively obtain a truly multilingual space. The resulting (monolingual and multilingual) spaces show consistent gains over the current state-of-the-art in standard intrinsic tasks, namely dictionary induction and word similarity, as well as in extrinsic tasks such as cross-lingual hypernym discovery and cross-lingual natural language inference.", "citations": 8}
{"title": "Multilingual Model Using Cross-Task Embedding Projection", "year": 2019, "authors": "Jin Sakuma, Naoki Yoshinaga", "url": "https://api.semanticscholar.org/CorpusId:196172425", "relevance": 3, "abstract": "We present a method for applying a neural network trained on one (resource-rich) language for a given task to other (resource-poor) languages. We accomplish this by inducing a mapping from pre-trained cross-lingual word embeddings to the embedding layer of the neural network trained on the resource-rich language. To perform element-wise cross-task embedding projection, we invent locally linear mapping which assumes and preserves the local topology across the semantic spaces before and after the projection. Experimental results on topic classification task and sentiment analysis task showed that the fully task-specific multilingual model obtained using our method outperformed the existing multilingual models with embedding layers fixed to pre-trained cross-lingual word embeddings.", "citations": 2}
{"title": "A Multi-task Approach to Learning Multilingual Representations", "year": 2018, "authors": "Karan Singla, Dogan Can, Shrikanth S. Narayanan", "url": "https://api.semanticscholar.org/CorpusId:51873570", "relevance": 3, "abstract": "We present a novel multi-task modeling approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a limited resource scenario.", "citations": 21}
{"title": "Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora", "year": 2020, "authors": "Takashi Wada, Tomoharu Iwata, Yuji Matsumoto, Timothy Baldwin, Jey Han Lau", "url": "https://api.semanticscholar.org/CorpusId:225094299", "relevance": 3, "abstract": "We propose a new approach for learning contextualised cross-lingual word embeddings based on a small parallel corpus (e.g. a few hundred sentence pairs). Our method obtains word embeddings via an LSTM encoder-decoder model that simultaneously translates and reconstructs an input sentence. Through sharing model parameters among different languages, our model jointly trains the word embeddings in a common cross-lingual space. We also propose to combine word and subword embeddings to make use of orthographic similarities across different languages. We base our experiments on real-world data from endangered languages, namely Yongning Na, Shipibo-Konibo, and Griko. Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs. These results demonstrate that, contrary to common belief, an encoder-decoder translation model is beneficial for learning cross-lingual representations even in extremely low-resource conditions. Furthermore, our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.", "citations": 8}
{"title": "Cross-Lingual Word Embeddings and the Structure of the Human Bilingual Lexicon", "year": 2019, "authors": "Paola Merlo, M. A. Rodriguez", "url": "https://api.semanticscholar.org/CorpusId:208253752", "relevance": 3, "abstract": "Research on the bilingual lexicon has uncovered fascinating interactions between the lexicons of the native language and of the second language in bilingual speakers. In particular, it has been found that the lexicon of the underlying native language affects the organisation of the second language. In the spirit of interpreting current distributed representations, this paper investigates two models of cross-lingual word embeddings, comparing them to the shared-translation effect and the cross-lingual coactivation effects of false and true friends (cognates) found in humans. We find that the similarity structure of the cross-lingual word embeddings space yields the same effects as the human bilingual lexicon.", "citations": 7}
{"title": "Multilingual Factor Analysis", "year": 2019, "authors": "Francisco Vargas, Kamen Brestnichki, Alexandros Papadopoulos-Korfiatis, Nils Y. Hammerla", "url": "https://api.semanticscholar.org/CorpusId:153311735", "relevance": 3, "abstract": "In this work we approach the task of learning multilingual word representations in an offline manner by fitting a generative latent variable model to a multilingual dictionary. We model equivalent words in different languages as different views of the same word generated by a common latent variable representing their latent lexical meaning. We explore the task of alignment by querying the fitted model for multilingual embeddings achieving competitive results across a variety of tasks. The proposed model is robust to noise in the embedding space making it a suitable method for distributed representations learned from noisy corpora.", "citations": 1}
{"title": "Lost in Alignment: A Survey on Cross-Lingual Alignment Methods for Contextualized Representation", "year": 2025, "authors": "Filippo Pallucchini, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica", "url": "https://www.semanticscholar.org/paper/13955b06b63f23837fe5963b9b28beb0660d2cb1", "relevance": 3, "abstract": "Cross-lingual word representations allow us to analyse word meanings across diverse language settings. It is crucial in aiding cross-lingual knowledge transfer when constructing natural language processing (NLP) models for languages with limited resources. This survey presents a comprehensive classification of cross-lingual contextual embedding models. We assess their data requirements and objective functions, and we introduce a taxonomy for categorising these approaches. Then, we present a comprehensive table containing a set of hierarchical criteria to compare them better, along with information regarding the availability of code and data to enable replication of the research. Furthermore, we delve into the evaluation methodologies employed for cross-lingual embeddings, exploring their practical applications and addressing their current associated challenges.", "citations": 2}
{"title": "GlobalTrait: Personality Alignment of Multilingual Word Embeddings", "year": 2018, "authors": "Farhad Bin Siddique, D. Bertero, Pascale Fung", "url": "https://api.semanticscholar.org/CorpusId:53165920", "relevance": 3, "abstract": "We propose a multilingual model to recognize Big Five Personality traits from text data in four different languages: English, Spanish, Dutch and Italian. Our analysis shows that words having a similar semantic meaning in different languages do not necessarily correspond to the same personality traits. Therefore, we propose a personality alignment method, GlobalTrait, which has a mapping for each trait from the source language to the target language (English), such that words that correlate positively to each trait are close together in the multilingual vector space. Using these aligned embeddings for training, we can transfer personality related training features from high-resource languages such as English to other low-resource languages, and get better multilingual results, when compared to using simple monolingual and unaligned multilingual embeddings. We achieve an average F-score increase (across all three languages except English) from 65 to 73.4 (+8.4), when comparing our monolingual model to multilingual using CNN with personality aligned embeddings. We also show relatively good performance in the regression tasks, and better classification results when evaluating our model on a separate Chinese dataset.", "citations": 5}
{"title": "WEWD: A Combined Approach for Measuring Cross-lingual Semantic Word Similarity Based on Word Embeddings and Word Definitions", "year": 2021, "authors": "Van-Tan Bui, Phuong-Thai Nguyen", "url": "https://www.semanticscholar.org/paper/4eac90c00659bd46355f77d141428b72636da74d", "relevance": 3, "abstract": "Cross-lingual semantic word similarity (CLSW) ad- dresses the task of estimating the semantic distance between two words across languages. This task is an important component in many natural language processing applications. Recent studies have proposed several effective CLSW models for resource- rich language pairs such as English-German, English-French. However, This task has not been effectively addressed for language pairs consisting of Vietnamese and another one. In this paper, we propose a neural network model that exploits cross- lingual lexical resources to learn high-quality cross-lingual word embedding models. Since our neural network model is language- independent, it can learn a truly multilingual space. Furthermore, we introduce a novel cross-lingual semantic word similarity measurement method based on Word Embeddings and Word Definitions (WEWD). Last but not least, we introduce a standard Vietnamese-English dataset for the cross-lingual semantic word similarity measurement task (VESim-1000). The experimental results show that our proposed method is more robust and outperforms current state-of-the-art methods that are only based on word embeddings or lexical resources.", "citations": 0}
{"title": "Detecting Cross-Lingual Plagiarism Using Simulated Word Embeddings", "year": 2017, "authors": "V. Thompson, C. Bowerman", "url": "https://api.semanticscholar.org/CorpusId:25834866", "relevance": 3, "abstract": "Cross-lingual plagiarism (CLP) occurs when texts written in one language are translated into a different language and used without acknowledgement. One of the most common methods used for detecting CLP requires online machine translators (such as Google or Microsoft translate) which are not always available, and given that plagiarism detection typically involves large document comparison, the amount of translations would overwhelm an online machine translator, especially when detecting plagiarism over the web. In addition, when translated texts are replaced with their synonyms, using online machine translators to detect CLP would result in poor performance. This paper addresses the problem of cross-lingual plagiarism detection (CLPD) by proposing a model that uses simulated word embeddings to reproduce the predictions of an online machine translator (Google translate) when detecting CLP without relying on online translators. The simulated embeddings comprise of translated words in different languages mapped in a common space, and replicated to increase the prediction probability of retrieving the translations of a word (and their synonyms) from the model. Unlike most existing models, the proposed model does not require parallel corpora, and accommodates multiple languages (multi-lingual). We demonstrated the effectiveness of the proposed model in detecting CLP in standard datasets that contain CLP cases, and evaluated its performance against a state-of-the-art baseline that relies on online machine translator (T+MA model). Evaluation results revealed that the proposed model is not only effective in detecting CLP, it outperformed the baseline. The results indicate that CLP could be detected with state-of-the-art performances by leveraging the prediction accuracy of an internet translator with word embeddings without relying on internet translators.", "citations": 3}
{"title": "How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions", "year": 2019, "authors": "Goran Glavas, Robert Litschko, Sebastian Ruder, Ivan Vulic", "url": "https://api.semanticscholar.org/CorpusId:59553563", "relevance": 2, "abstract": "Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models: we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis.", "citations": 188}
{"title": "Cross-lingual alignments of ELMo contextual embeddings", "year": 2021, "authors": "Matej Ul\u010dar, Marko Robnik-Sikonja", "url": "https://api.semanticscholar.org/CorpusId:235683108", "relevance": 2, "abstract": "Building machine learning prediction models for a specific natural language processing (NLP) task requires sufficient training data, which can be difficult to obtain for less-resourced languages. Cross-lingual embeddings map word embeddings from a less-resourced language to a resource-rich language so that a prediction model trained on data from the resource-rich language can also be used in the less-resourced language. To produce cross-lingual mappings of recent contextual embeddings, anchor points between the embedding spaces have to be words in the same context. We address this issue with a novel method for creating cross-lingual contextual alignment datasets. Based on that, we propose several cross-lingual mapping methods for ELMo embeddings. The proposed linear mapping methods use existing Vecmap and MUSE alignments on contextual ELMo embeddings. Novel nonlinear ELMoGAN mapping methods are based on generative adversarial networks (GANs) and do not assume isomorphic embedding spaces. We evaluate the proposed mapping methods on nine languages, using four downstream tasks: named entity recognition (NER), dependency parsing (DP), terminology alignment, and sentiment analysis. The ELMoGAN methods perform very well on the NER and terminology alignment tasks, with a lower cross-lingual loss for NER compared to the direct training on some languages. In DP and sentiment analysis, linear contextual alignment variants are more successful.", "citations": 22}
{"title": "Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?", "year": 2019, "authors": "Ivan Vulic, Goran Glavas, Roi Reichart, A. Korhonen", "url": "https://api.semanticscholar.org/CorpusId:202539655", "relevance": 2, "abstract": "Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any supervision makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resource-poor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.", "citations": 92}
{"title": "ArbEngVec : Arabic-English Cross-Lingual Word Embedding Model", "year": 2019, "authors": "Raki Lachraf, El Moatez Billah Nagoudi, Youcef Ayachi, Ahmed Abdelali, D. Schwab", "url": "https://api.semanticscholar.org/CorpusId:196174722", "relevance": 2, "abstract": "Word Embeddings (WE) are getting increasingly popular and widely applied in many Natural Language Processing (NLP) applications due to their effectiveness in capturing semantic properties of words; Machine Translation (MT), Information Retrieval (IR) and Information Extraction (IE) are among such areas. In this paper, we propose an open source ArbEngVec which provides several Arabic-English cross-lingual word embedding models. To train our bilingual models, we use a large dataset with more than 93 million pairs of Arabic-English parallel sentences. In addition, we perform both extrinsic and intrinsic evaluations for the different word embedding model variants. The extrinsic evaluation assesses the performance of models on the cross-language Semantic Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task.", "citations": 18}
{"title": "Robust Unsupervised Cross-Lingual Word Embedding using Domain Flow Interpolation", "year": 2022, "authors": "Liping Tang, Zhen Li, Zhiquan Luo, H. Meng", "url": "https://www.semanticscholar.org/paper/db9cb3749585d7a735540f7b2c15130cd12368bd", "relevance": 2, "abstract": "This paper investigates an unsupervised approach towards deriving a universal, cross-lingual word embedding space, where words with similar semantics from different languages are close to one another. Previous adversarial approaches have shown promising results in inducing cross-lingual word embedding without parallel data. However, the training stage shows instability for distant language pairs. Instead of mapping the source language space directly to the target language space, we propose to make use of a sequence of intermediate spaces for smooth bridging. Each intermediate space may be conceived as a pseudo-language space and is introduced via simple linear interpolation. This approach is modeled after domain flow in computer vision, but with a modified objective function. Experiments on intrinsic Bilingual Dictionary Induction tasks show that the proposed approach can improve the robustness of adversarial models with comparable and even better precision. Further experiments on the downstream task of Cross-Lingual Natural Language Inference show that the proposed model achieves significant performance improvement for distant language pairs in downstream tasks compared to state-of-the-art adversarial and non-adversarial models.", "citations": 0}
{"title": "CLUSE: Cross-Lingual Unsupervised Sense Embeddings", "year": 2018, "authors": "Ta-Chung Chi, Yun-Nung (Vivian) Chen", "url": "https://api.semanticscholar.org/CorpusId:52289318", "relevance": 2, "abstract": "This paper proposes a modularized sense induction and representation learning model that jointly learns bilingual sense embeddings that align well in the vector space, where the cross-lingual signal in the English-Chinese parallel corpus is exploited to capture the collocation and distributed characteristics in the language pair. The model is evaluated on the Stanford Contextual Word Similarity (SCWS) dataset to ensure the quality of monolingual sense embeddings. In addition, we introduce Bilingual Contextual Word Similarity (BCWS), a large and high-quality dataset for evaluating cross-lingual sense embeddings, which is the first attempt of measuring whether the learned embeddings are indeed aligned well in the vector space. The proposed approach shows the superior quality of sense embeddings evaluated in both monolingual and bilingual spaces.", "citations": 6}
{"title": "Cross-Lingual Word Embeddings for Morphologically Rich Languages", "year": 2019, "authors": "A. Ustun, G. Bouma, Gertjan van Noord", "url": "https://www.semanticscholar.org/paper/9579d953d3eced070210870b52549ddc950cccd2", "relevance": 2, "abstract": "Cross-lingual word embedding models learn a shared vector space for two or more languages so that words with similar meaning are represented by similar vectors regardless of their language. Although the existing models achieve high performance on pairs of morphologically simple languages, they perform very poorly on morphologically rich languages such as Turkish and Finnish. In this paper, we propose a morpheme-based model in order to increase the performance of cross-lingual word embeddings on morphologically rich languages. Our model includes a simple extension which enables us to exploit morphemes for cross-lingual mapping. We applied our model for the Turkish-Finnish language pair on the bilingual word translation task. Results show that our model outperforms the baseline models by 2% in the nearest neighbour ranking.", "citations": 2}
{"title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "year": 2016, "authors": "Shyam Upadhyay, Manaal Faruqui, Chris Dyer, Dan Roth", "url": "https://api.semanticscholar.org/CorpusId:5357629", "relevance": 2, "abstract": "Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.", "citations": 189}
{"title": "Bilingual Word Embeddings from Non-Parallel Document-Aligned Data Applied to Bilingual Lexicon Induction", "year": 2015, "authors": "Ivan Vulic, Marie-Francine Moens", "url": "https://api.semanticscholar.org/CorpusId:14183678", "relevance": 2, "abstract": "We propose a simple yet effective approach to learning bilingual word embeddings (BWEs) from non-parallel document-aligned data (based on the omnipresent skip-gram model), and its application to bilingual lexicon induction (BLI). We demonstrate the utility of the induced BWEs in the BLI task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training.", "citations": 136}
{"title": "A Call for More Rigor in Unsupervised Cross-lingual Learning", "year": 2020, "authors": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka, Eneko Agirre", "url": "https://api.semanticscholar.org/CorpusId:216914383", "relevance": 2, "abstract": "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.", "citations": 74}
{"title": "Bilingual Distributed Word Representations from Document-Aligned Comparable Data", "year": 2015, "authors": "Ivan Vulic, Marie-Francine Moens", "url": "https://api.semanticscholar.org/CorpusId:16452496", "relevance": 2, "abstract": "We propose a new model for learning bilingual word representations from nonparallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.", "citations": 106}
{"title": "Learning Bilingual Word Embedding Mappings with Similar Words in Related Languages Using GAN", "year": 2022, "authors": "Ghafour Alipour, J. B. Mohasefi, M. Feizi-Derakhshi", "url": "https://api.semanticscholar.org/CorpusId:246688573", "relevance": 2, "abstract": "ABSTRACT Cross-lingual word embeddings display words from different languages in the same vector space. They provide reasoning about semantics, compare the meaning of words across languages and word meaning in multilingual contexts, necessary to bilingual lexicon induction, machine translation, and cross-lingual information retrieval. This paper proposes an efficient approach to learn bilingual transform mapping between monolingual word embeddings in language pairs. We choose ten different languages from three different language families and downloaded their last update Wikipedia dumps1 1. https://dumps.wikimedia.org. Then, with some pre-processing steps and using word2vec, we produce word embeddings for them. We select seven language pairs from chosen languages. Since the selected languages are relative, they have thousands of identical words with similar meanings. With these identical dictation words and word embedding models of each language, we create training, validation and, test sets for the language pairs. We then use a generative adversarial network (GAN) to learn the transform mapping between word embeddings of source and target languages. The average accuracy of our proposed method in all language pairs is 71.34%. The highest accuracy is achieved for the Turkish-Azerbaijani language pair with the accuracy 78.32%., which is noticeably higher than prior methods.", "citations": 11}
{"title": "Improving Cross-Lingual Word Embeddings by Meeting in the Middle", "year": 2018, "authors": "Yerai Doval, Jos\u00e9 Camacho-Collados, Luis Espinosa Anke, S. Schockaert", "url": "https://api.semanticscholar.org/CorpusId:52094910", "relevance": 2, "abstract": "Cross-lingual word embeddings are becoming increasingly important in multilingual NLP. Recently, it has been shown that these embeddings can be effectively learned by aligning two disjoint monolingual vector spaces through linear transformations, using no more than a small bilingual dictionary as supervision. In this work, we propose to apply an additional transformation after the initial alignment step, which moves cross-lingual synonyms towards a middle point between them. By applying this transformation our aim is to obtain a better cross-lingual integration of the vector spaces. In addition, and perhaps surprisingly, the monolingual spaces also improve by this transformation. This is in contrast to the original alignment, which is typically learned such that the structure of the monolingual spaces is preserved. Our experiments confirm that the resulting cross-lingual embeddings outperform state-of-the-art models in both monolingual and cross-lingual evaluation tasks.", "citations": 62}
{"title": "LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction through Non-Linear Mapping in Latent Space", "year": 2020, "authors": "Tasnim Mohiuddin, M Saiful Bari, Shafiq R. Joty", "url": "https://api.semanticscholar.org/CorpusId:216641774", "relevance": 2, "abstract": "Most of the successful and predominant methods for bilingual lexicon induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e., approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages. In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses nonlinear mapping in the latent space of two independently trained auto-encoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping.", "citations": 52}
{"title": "On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning", "year": 2019, "authors": "Yerai Doval, Jos\u00e9 Camacho-Collados, Luis Espinosa Anke, S. Schockaert", "url": "https://api.semanticscholar.org/CorpusId:201124803", "relevance": 2, "abstract": "Cross-lingual word embeddings are vector representations of words in different languages where words with similar meaning are represented by similar vectors, regardless of the language. Recent developments which construct these embeddings by aligning monolingual spaces have shown that accurate alignments can be obtained with little or no supervision, which usually comes in the form of bilingual dictionaries. However, the focus has been on a particular controlled scenario for evaluation, and there is no strong evidence on how current state-of-the-art systems would fare with noisy text or for language pairs with major linguistic differences. In this paper we present an extensive evaluation over multiple cross-lingual embedding models, analyzing their strengths and limitations with respect to different variables such as target language, training corpora and amount of supervision. Our conclusions put in doubt the view that high-quality cross-lingual embeddings can always be learned without much supervision.", "citations": 21}
{"title": "A survey of neural-network-based methods utilising comparable data for finding translation equivalents", "year": 2024, "authors": "Michaela Denisov'a, Pavel Rychl'y", "url": "https://api.semanticscholar.org/CorpusId:273502039", "relevance": 2, "abstract": "The importance of inducing bilingual dictionary components in many natural language processing (NLP) applications is indisputable. However, the dictionary compilation process requires extensive work and combines two disciplines, NLP and lexicography, while the former often omits the latter. In this paper, we present the most common approaches from NLP that endeavour to automatically induce one of the essential dictionary components, translation equivalents and focus on the neural-network-based methods using comparable data. We analyse them from a lexicographic perspective since their viewpoints are crucial for improving the described methods. Moreover, we identify the methods that integrate these viewpoints and can be further exploited in various applications that require them. This survey encourages a connection between the NLP and lexicography fields as the NLP field can benefit from lexicographic insights, and it serves as a helping and inspiring material for further research in the context of neural-network-based methods utilising comparable data.", "citations": 0}
{"title": "Quantized Wasserstein Procrustes Alignment of Word Embedding Spaces", "year": 2022, "authors": "P. Aboagye, Yan-luan Zheng, Michael Yeh, Junpeng Wang, Zhongfang Zhuang, Huiyuan Chen, Liang Wang, Wei Zhang, J. M. Phillips", "url": "https://api.semanticscholar.org/CorpusId:252186399", "relevance": 2, "abstract": "Motivated by the widespread interest in the cross-lingual transfer of NLP models from high resource to low resource languages, research on Cross-lingual word embeddings (CLWEs) has gained much popularity over the years. Among the most successful and attractive CLWE models are the unsupervised CLWE models. These unsupervised CLWE models pose the alignment task as a Wasserstein-Procrustes problem aiming to estimate a permutation matrix and an orthogonal matrix jointly. Most existing unsupervised CLWE models resort to Optimal Transport (OT) based methods to estimate the permutation matrix. However, linear programming algorithms and approximate OT solvers via Sinkhorn for computing the permutation matrix scale cubically and quadratically, respectively, in the input size. This makes it impractical and infeasible to compute OT distances exactly for larger sample size, resulting in a poor approximation quality of the permutation matrix and subsequently a less robust learned transfer function or mapper. This paper proposes an unsupervised projection-based CLWE model called quantized Wasserstein Procrustes (qWP) that jointly estimates a permutation matrix and an orthogonal matrix. qWP relies on a quantization step to estimate the permutation matrix between two probability distributions or measures. This approach substantially improves the approximation quality of empirical OT solvers given fixed computational cost. We demonstrate that qWP achieves state-of-the-art results on the Bilingual lexicon Induction (BLI) task.", "citations": 6}
{"title": "Density Matching for Bilingual Word Embedding", "year": 2019, "authors": "Chunting Zhou, Xuezhe Ma, Di Wang, Graham Neubig", "url": "https://api.semanticscholar.org/CorpusId:102354931", "relevance": 2, "abstract": "Recent approaches to cross-lingual word embedding have generally been based on linear transformations between the sets of embedding vectors in the two languages. In this paper, we propose an approach that instead expresses the two monolingual embedding spaces as probability densities defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow. The method requires no explicit supervision, and can be learned with only a seed dictionary of words that have identical strings. We argue that this formulation has several intuitively attractive properties, particularly with the respect to improving robustness and generalization to mappings between difficult language pairs or word pairs. On a benchmark data set of bilingual lexicon induction and cross-lingual word similarity, our approach can achieve competitive or superior performance compared to state-of-the-art published results, with particularly strong results being found on etymologically distant and/or morphologically rich languages.", "citations": 40}
{"title": "English-Malay Cross-Lingual Embedding Alignment using Bilingual Lexicon Augmentation", "year": 2022, "authors": "Ying Hao Lim, Jasy Suet Yan Liew", "url": "https://api.semanticscholar.org/CorpusId:248780179", "relevance": 2, "abstract": "As high-quality Malay language resources are still a scarcity, cross lingual word embeddings make it possible for richer English resources to be leveraged for downstream Malay text classification tasks. This paper focuses on creating an English-Malay cross-lingual word embeddings using embedding alignment by exploiting existing language resources. We augmented the training bilingual lexicons using machine translation with the goal to improve the alignment precision of our cross-lingual word embeddings. We investigated the quality of the current state-of-the-art English-Malay bilingual lexicon and worked on improving its quality using Google Translate. We also examined the effect of Malay word coverage on the quality of cross-lingual word embeddings. Experimental results with a precision up till 28.17% show that the alignment precision of the cross-lingual word embeddings would inevitably degrade after 1-NN but a better seed lexicon and cleaner nearest neighbours can reduce the number of word pairs required to achieve satisfactory performance. As the English and Malay monolingual embeddings are pre-trained on informal language corpora, our proposed English-Malay embeddings alignment approach is also able to map non-standard Malay translations in the English nearest neighbours.", "citations": 0}
{"title": "Semantic Recommendation System for Bilingual Corpus of Academic Papers", "year": 2021, "authors": "Anna Safaryan, Petr Filchenkov, Weijia Yan, Andrey Kutuzov, Irina Nikishina", "url": "https://api.semanticscholar.org/CorpusId:232336430", "relevance": 2, "abstract": "We tested four methods of making document representations cross-lingual for the task of semantic search for the similar papers based on the corpus of papers from three Russian conferences on NLP: Dialogue, AIST and AINL. The pipeline consisted of three stages: preprocessing, word-by-word vectorisation using models obtained with various methods to map vectors from two independent vector spaces to a common one, and search for the most similar papers based on the cosine similarity of text vectors. The four methods used can be grouped into two approaches: 1) aligning two pretrained monolingual word embedding models with a bilingual dictionary on our own (for example, with the VecMap algorithm) and 2) using pre-aligned cross-lingual word embedding models (MUSE). To find out, which approach brings more benefit to the task, we conducted a manual evaluation of the results and calculated the average precision of recommendations for all the methods mentioned above. MUSE turned out to have the highest search relevance, but the other methods produced more recommendations in a language other than the one of the target paper.", "citations": 2}
{"title": "Unsupervised Evaluation of Human Translation Quality", "year": 2019, "authors": "Yi Zhou, Danushka Bollegala", "url": "https://api.semanticscholar.org/CorpusId:204754744", "relevance": 2, "abstract": "Even though machine translation (MT) systems have reached impressive performances in cross-lingual translation tasks, the quality of MT is still far behind professional human translations (HTs) due to the complexity in natural languages, especially for terminologies in different domains. Therefore, HTs are still widely demanded in practice. However, the quality of HT is also imperfect and vary significantly depending on the experience and knowledge of the translators. Evaluating the quality of HT in an automatic manner has faced many challenges. Although bilingual speakers are able to assess the translation quality, manually checking the accuracy of translations is expensive and time-consuming. In this paper, we propose an unsupervised method to evaluate the quality of HT without requiring any labelled data. We compare a range of methods for automatically grading HTs and observe the Bidirectional Minimum Word Mover\u2019s distance (BiMWMD) to produce gradings that correlate well with humans.", "citations": 5}
{"title": "A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity", "year": 2019, "authors": "Yoshinari Fujinuma, Jordan L. Boyd-Graber, Michael J. Paul", "url": "https://api.semanticscholar.org/CorpusId:174799386", "relevance": 2, "abstract": "Cross-lingual word embeddings encode the meaning of words from different languages into a shared low-dimensional space. An important requirement for many downstream tasks is that word similarity should be independent of language\u2014i.e., word vectors within one language should not be more similar to each other than to words in another language. We measure this characteristic using modularity, a network measurement that measures the strength of clusters in a graph. Modularity has a moderate to strong correlation with three downstream tasks, even though modularity is based only on the structure of embeddings and does not require any external resources. We show through experiments that modularity can serve as an intrinsic validation metric to improve unsupervised cross-lingual word embeddings, particularly on distant language pairs in low-resource settings.", "citations": 28}
{"title": "BCWS: Bilingual Contextual Word Similarity", "year": 2018, "authors": "Ta-Chung Chi, Ching-Yen Shih, Yun-Nung (Vivian) Chen", "url": "https://api.semanticscholar.org/CorpusId:53047117", "relevance": 2, "abstract": "This paper introduces the first dataset for evaluating English-Chinese Bilingual Contextual Word Similarity, namely BCWS (this https URL). The dataset consists of 2,091 English-Chinese word pairs with the corresponding sentential contexts and their similarity scores annotated by the human. Our annotated dataset has higher consistency compared to other similar datasets. We establish several baselines for the bilingual embedding task to benchmark the experiments. Modeling cross-lingual sense representations as provided in this dataset has the potential of moving artificial intelligence from monolingual understanding towards multilingual understanding.", "citations": 3}
{"title": "Learning to Represent Bilingual Dictionaries", "year": 2018, "authors": "Muhao Chen, Yingtao Tian, Haochen Chen, Kai-Wei Chang, S. Skiena, C. Zaniolo", "url": "https://api.semanticscholar.org/CorpusId:51984213", "relevance": 2, "abstract": "Bilingual word embeddings have been widely used to capture the correspondence of lexical semantics in different human languages. However, the cross-lingual correspondence between sentences and words is less studied, despite that this correspondence can significantly benefit many applications such as crosslingual semantic search and textual inference. To bridge this gap, we propose a neural embedding model that leverages bilingual dictionaries. The proposed model is trained to map the lexical definitions to the cross-lingual target words, for which we explore with different sentence encoding techniques. To enhance the learning process on limited resources, our model adopts several critical learning strategies, including multi-task learning on different bridges of languages, and joint learning of the dictionary model with a bilingual word embedding model. We conduct experiments on two new tasks. In the cross-lingual reverse dictionary retrieval task, we demonstrate that our model is capable of comprehending bilingual concepts based on descriptions, and the proposed learning strategies are effective. In the bilingual paraphrase identification task, we show that our model effectively associates sentences in different languages via a shared embedding space, and outperforms existing approaches in identifying bilingual paraphrases.", "citations": 13}
{"title": "Cross-Lingual Contextual Word Embeddings Mapping With Multi-Sense Words In Mind", "year": 2019, "authors": "Zheng Zhang, Ruiqing Yin, Jun Zhu, Pierre Zweigenbaum", "url": "https://api.semanticscholar.org/CorpusId:202677272", "relevance": 2, "abstract": "Recent work in cross-lingual contextual word embedding learning cannot handle multi-sense words well. In this work, we explore the characteristics of contextual word embeddings and show the link between contextual word embeddings and word senses. We propose two improving solutions by considering contextual multi-sense word embeddings as noise (removal) and by generating cluster level average anchor embeddings for contextual multi-sense word embeddings (replacement). Experiments show that our solutions can improve the supervised contextual word embeddings alignment for multi-sense words in a microscopic perspective without hurting the macroscopic performance on the bilingual lexicon induction task. For unsupervised alignment, our methods significantly improve the performance on the bilingual lexicon induction task for more than 10 points.", "citations": 4}
{"title": "Expanding the Text Classification Toolbox with Cross-Lingual Embeddings", "year": 2019, "authors": "Meryem M'hamdi, Robert West, Andreea Hossmann, Michael Baeriswyl, C. Musat", "url": "https://api.semanticscholar.org/CorpusId:85502789", "relevance": 2, "abstract": "Most work in text classification and Natural Language Processing (NLP) focuses on English or a handful of other languages that have text corpora of hundreds of millions of words. This is creating a new version of the digital divide: the artificial intelligence (AI) divide. Transfer-based approaches, such as Cross-Lingual Text Classification (CLTC) - the task of categorizing texts written in different languages into a common taxonomy, are a promising solution to the emerging AI divide. Recent work on CLTC has focused on demonstrating the benefits of using bilingual word embeddings as features, relegating the CLTC problem to a mere benchmark based on a simple averaged perceptron. \nIn this paper, we explore more extensively and systematically two flavors of the CLTC problem: news topic classification and textual churn intent detection (TCID) in social media. In particular, we test the hypothesis that embeddings with context are more effective, by multi-tasking the learning of multilingual word embeddings and text classification; we explore neural architectures for CLTC; and we move from bi- to multi-lingual word embeddings. For all architectures, types of word embeddings and datasets, we notice a consistent gain trend in favor of multilingual joint training, especially for low-resourced languages.", "citations": 1}
{"title": "Cross-Lingual Word Embedding Refinement by \\ell_{1} Norm Optimisation", "year": 2021, "authors": "Xu Lin, Mark Stevenson", "url": "https://www.semanticscholar.org/paper/17fa1a5a4b12eb312a6f3f069e013d644708e437", "relevance": 2, "abstract": "Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the \u21132 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. \u21131 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the \u21131 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this strategy be adopted as a standard for CLWE methods.", "citations": 14}
{"title": "Refinement of Unsupervised Cross-Lingual Word Embeddings", "year": 2020, "authors": "Magdalena Biesialska, M. Costa-juss\u00e0", "url": "https://www.semanticscholar.org/paper/3356b149591fc6f0d206f94cf585236f8e1d3736", "relevance": 2, "abstract": "Cross-lingual word embeddings aim to bridge the gap between high-resource and low-resource languages by allowing to learn multilingual word representations even without using any direct bilingual signal. The lion's share of the methods are projection-based approaches that map pre-trained embeddings into a shared latent space. These methods are mostly based on the orthogonal transformation, which assumes language vector spaces to be isomorphic. However, this criterion does not necessarily hold, especially for morphologically-rich languages. In this paper, we propose a self-supervised method to refine the alignment of unsupervised bilingual word embeddings. The proposed model moves vectors of words and their corresponding translations closer to each other as well as enforces length- and center-invariance, thus allowing to better align cross-lingual embeddings. The experimental results demonstrate the effectiveness of our approach, as in most cases it outperforms state-of-the-art methods in a bilingual lexicon induction task.", "citations": 4}
{"title": "GLUECoS: An Evaluation Benchmark for Code-Switched NLP", "year": 2020, "authors": "Simran Khanuja, Sandipan Dandapat, A. Srinivasan, Sunayana Sitaram, M. Choudhury", "url": "https://api.semanticscholar.org/CorpusId:216553264", "relevance": 2, "abstract": "Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference. We present results on all these tasks using cross-lingual word embedding models and multilingual models. In addition, we fine-tune multilingual models on artificially generated code-switched data. Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.", "citations": 178}
{"title": "Multilingual Training of Crosslingual Word Embeddings", "year": 2017, "authors": "H. Kanayama, Trevor Cohn, Tengfei Ma, Steven Bird, Long Duong", "url": "https://api.semanticscholar.org/CorpusId:17908320", "relevance": 2, "abstract": "Crosslingual word embeddings represent lexical items from different languages using the same vector space, enabling crosslingual transfer. Most prior work constructs embeddings for a pair of languages, with English on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space.In this way, we can exploit and combine strength of many languages. We obtained high performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks.", "citations": 48}
{"title": "Cross-lingual Word Embeddings in Hyperbolic Space", "year": 2022, "authors": "Chandni Saxena, Mudit Chaudhary, H. Meng", "url": "https://api.semanticscholar.org/CorpusId:248512913", "relevance": 2, "abstract": "Cross-lingual word embeddings can be applied to several natural language processing applications across multiple languages. Unlike prior works that use word embeddings based on the Euclidean space, this short paper presents a simple and effective cross-lingual Word2Vec model that adapts to the Poincar\\'e ball model of hyperbolic space to learn unsupervised cross-lingual word representations from a German-English parallel corpus. It has been shown that hyperbolic embeddings can capture and preserve hierarchical relationships. We evaluate the model on both hypernymy and analogy tasks. The proposed model achieves comparable performance with the vanilla Word2Vec model on the cross-lingual analogy task, the hypernymy task shows that the cross-lingual Poincar\\'e Word2Vec model can capture latent hierarchical structure from free text across languages, which are absent from the Euclidean-based Word2Vec representations. Our results show that by preserving the latent hierarchical information, hyperbolic spaces can offer better representations for cross-lingual embeddings.", "citations": 4}
{"title": "Bilingual Topic Models for Comparable Corpora", "year": 2021, "authors": "Georgios Balikas, Massih-Reza Amini, M. Clausel", "url": "https://api.semanticscholar.org/CorpusId:244729087", "relevance": 2, "abstract": "Probabilistic topic models like Latent Dirichlet Allocation (LDA) have been previously extended to the bilingual setting. A fundamental modeling assumption in several of these extensions is that the input corpora are in the form of document pairs whose constituent documents share a single topic distribution. However, this assumption is strong for comparable corpora that consist of documents thematically similar to an extent only, which are, in turn, the most commonly available or easy to obtain. In this paper we relax this assumption by proposing for the paired documents to have separate, yet bound topic distributions. % a binding mechanism between the distributions of the paired documents. We suggest that the strength of the bound should depend on each pair's semantic similarity. To estimate the similarity of documents that are written in different languages we use cross-lingual word embeddings that are learned with shallow neural networks. We evaluate the proposed binding mechanism by extending two topic models: a bilingual adaptation of LDA that assumes bag-of-words inputs and a model that incorporates part of the text structure in the form of boundaries of semantically coherent segments. To assess the performance of the novel topic models we conduct intrinsic and extrinsic experiments on five bilingual, comparable corpora of English documents with French, German, Italian, Spanish and Portuguese documents. The results demonstrate the efficiency of our approach in terms of both topic coherence measured by the normalized point-wise mutual information, and generalization performance measured by perplexity and in terms of Mean Reciprocal Rank in a cross-lingual document retrieval task for each of the language pairs.", "citations": 0}
{"title": "What makes multilingual BERT multilingual?", "year": 2020, "authors": "Chi-Liang Liu, Tsung-Yuan Hsu, Yung-Sung Chuang, Hung-yi Lee", "url": "https://api.semanticscholar.org/CorpusId:224819080", "relevance": 2, "abstract": "Recently, multilingual BERT works remarkably well on cross-lingual transfer tasks, superior to static non-contextualized word embeddings. In this work, we provide an in-depth experimental study to supplement the existing literature of cross-lingual ability. We compare the cross-lingual ability of non-contextualized and contextualized representation model with the same data. We found that datasize and context window size are crucial factors to the transferability.", "citations": 6}
{"title": "Inducing Crosslingual Distributed Representations of Words", "year": 2012, "authors": "A. Klementiev, Ivan Titov, Binod Bhattarai", "url": "https://www.semanticscholar.org/paper/d1f37d9cab68eb8cda669cc949394732f33264b4", "relevance": 1, "abstract": "", "citations": 390}
{"title": "Learning Crosslingual Word Embeddings without Bilingual Corpora", "year": 2016, "authors": "Long Duong, H. Kanayama, Tengfei Ma, Steven Bird, Trevor Cohn", "url": "https://api.semanticscholar.org/CorpusId:13888952", "relevance": 1, "abstract": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.", "citations": 116}
{"title": "Data-Augmentation for Bangla-English Code-Mixed Sentiment Analysis: Enhancing Cross Linguistic Contextual Understanding", "year": 2023, "authors": "Mohammad Tareq, Md. Fokhrul Islam, Swakshar Deb, Sejuti Rahman, A. Mahmud", "url": "https://api.semanticscholar.org/CorpusId:258797890", "relevance": 1, "abstract": "In today\u2019s digital world, automated sentiment analysis from online reviews can contribute to a wide variety of decision-making processes. One example is examining typical perceptions of a product based on customer feedbacks to have a better understanding of consumer expectations, which can help enhance everything from customer service to product offerings. Online review comments, on the other hand, frequently mix different languages, use non-native scripts and do not adhere to strict grammar norms. For a low-resource language like Bangla, the lack of annotated code-mixed data makes automated sentiment analysis more challenging. To address this, we collect online reviews of different products and construct an annotated Bangla-English code mix (BE-CM) dataset (Dataset and other resources are available at https://github.com/fokhruli/CM-seti-anlysis). On our sentiment corpus, we also compare several alternative models from the existing literature. We present a simple but effective data augmentation method that can be utilized with existing word embedding algorithms without the need for a parallel corpus to improve cross-lingual contextual understanding. Our experimental results suggest that training word embedding models (e.g., Word2vec, FastText) with our data augmentation strategy can help the model in capturing the cross-lingual relationship for code-mixed sentences, thereby improving the overall performance of existing classifiers in both supervised learning and zero-shot cross-lingual adaptability. With extensive experimentations, we found that XGBoost with Fasttext embedding trained on our proposed data augmentation method outperforms other alternative models in automated sentiment analysis on code-mixed Bangla-English dataset, with a weighted F1 score of 87%.", "citations": 27}
{"title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments", "year": 2016, "authors": "Anders S\u00f8gaard, Yoav Goldberg, Omer Levy", "url": "https://api.semanticscholar.org/CorpusId:16479424", "relevance": 1, "abstract": "While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account.", "citations": 66}
{"title": "Word Embeddings for Code-Mixed Language Processing", "year": 2018, "authors": "Adithya Pratapa, M. Choudhury, Sunayana Sitaram", "url": "https://api.semanticscholar.org/CorpusId:53080549", "relevance": 1, "abstract": "We compare three existing bilingual word embedding approaches, and a novel approach of training skip-grams on synthetic code-mixed text generated through linguistic models of code-mixing, on two tasks - sentiment analysis and POS tagging for code-mixed text. Our results show that while CVM and CCA based embeddings perform as well as the proposed embedding technique on semantic and syntactic tasks respectively, the proposed approach provides the best performance for both tasks overall. Thus, this study demonstrates that existing bilingual embedding techniques are not ideal for code-mixed text processing and there is a need for learning multilingual word embedding from the code-mixed text.", "citations": 62}
{"title": "Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping", "year": 2019, "authors": "Jian Ni, Radu Florian", "url": "https://api.semanticscholar.org/CorpusId:202769250", "relevance": 1, "abstract": "Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-of-the-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a resource-poor language. In this paper, we propose a new approach for cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language, so that a well-trained source-language neural network RE model can be directly applied to the target language. Experiment results show that the proposed approach achieves very good performance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs.", "citations": 27}
{"title": "En-Ar Bilingual Word Embeddings without Word Alignment: Factors Effects", "year": 2019, "authors": "T. Alqaisi, Simon E. M. O'Keefe", "url": "https://api.semanticscholar.org/CorpusId:199379338", "relevance": 1, "abstract": "This paper introduces the first attempt to investigate morphological segmentation on En-Ar bilingual word embeddings using bilingual word embeddings model without word alignment (BilBOWA). We investigate the effect of sentence length and embedding size on the learning process. Our experiment shows that using the D3 segmentation scheme improves the accuracy of learning bilingual word embeddings up to 10 percentage points compared to the ATB and D0 schemes in all different training settings.", "citations": 8}
{"title": "Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder", "year": 2019, "authors": "Yunsu Kim, Jiahui Geng, H. Ney", "url": "https://api.semanticscholar.org/CorpusId:52237660", "relevance": 1, "abstract": "Unsupervised learning of cross-lingual word embedding offers elegant matching of words across languages, but has fundamental limitations in translating sentences. In this paper, we propose simple yet effective methods to improve word-by-word translation of cross-lingual embeddings, using only monolingual corpora but without any back-translation. We integrate a language model for context-aware search, and use a novel denoising autoencoder to handle reordering. Our system surpasses state-of-the-art unsupervised translation systems without costly iterative training. We also analyze the effect of vocabulary size and denoising type on the translation performance, which provides better understanding of learning the cross-lingual word embedding and its usage in translation.", "citations": 43}
{"title": "Unsupervised Cross-lingual Representation Learning at Scale", "year": 2019, "authors": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco (Paco) Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov", "url": "https://www.semanticscholar.org/paper/6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "relevance": 1, "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.", "citations": 7836}
{"title": "Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability", "year": 2022, "authors": "Yoshinari Fujinuma, Jordan L. Boyd-Graber, Katharina Kann", "url": "https://api.semanticscholar.org/CorpusId:247594723", "relevance": 1, "abstract": "Pretrained multilingual models enable zero-shot learning even for unseen languages, and that performance can be further improved via adaptation prior to finetuning. However, it is unclear how the number of pretraining languages influences a model\u2019s zero-shot learning for languages unseen during pretraining. To fill this gap, we ask the following research questions: (1) How does the number of pretraining languages influence zero-shot performance on unseen target languages? (2) Does the answer to that question change with model adaptation? (3) Do the findings for our first question change if the languages used for pretraining are all related? Our experiments on pretraining with related languages indicate that choosing a diverse set of languages is crucial. Without model adaptation, surprisingly, increasing the number of pretraining languages yields better results up to adding related languages, after which performance plateaus.In contrast, with model adaptation via continued pretraining, pretraining on a larger number of languages often gives further improvement, suggesting that model adaptation is crucial to exploit additional pretraining languages.", "citations": 29}
{"title": "An Empirical Study on Crosslingual Transfer in Probabilistic Topic Models", "year": 2018, "authors": "Shudong Hao, Michael J. Paul", "url": "https://api.semanticscholar.org/CorpusId:182953256", "relevance": 1, "abstract": "Probabilistic topic modeling is a common first step in crosslingual tasks to enable knowledge transfer and extract multilingual features. Although many multilingual topic models have been developed, their assumptions about the training corpus are quite varied, and it is not clear how well the different models can be utilized under various training conditions. In this article, the knowledge transfer mechanisms behind different multilingual topic models are systematically studied, and through a broad set of experiments with four models on ten languages, we provide empirical insights that can inform the selection and future development of multilingual topic models.", "citations": 8}
