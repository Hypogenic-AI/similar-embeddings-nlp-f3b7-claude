{"title": "SensEmBERT: Context-Enhanced Sense Embeddings for Multilingual Word Sense Disambiguation", "year": 2020, "authors": "Bianca Scarlini, Tommaso Pasini, Roberto Navigli", "url": "https://api.semanticscholar.org/CorpusId:211628120", "relevance": 3, "abstract": "Contextual representations of words derived by neural language models have proven to effectively encode the subtle distinctions that might occur between different meanings of the same word. However, these representations are not tied to a semantic network, hence they leave the word meanings implicit and thereby neglect the information that can be derived from the knowledge base itself. In this paper, we propose SensEmBERT, a knowledge-based approach that brings together the expressive power of language modelling and the vast amount of knowledge contained in a semantic network to produce high-quality latent semantic representations of word meanings in multiple languages. Our vectors lie in a space comparable with that of contextualized word embeddings, thus allowing a word occurrence to be easily linked to its meaning by applying a simple nearest neighbour approach.We show that, whilst not relying on manual semantic annotations, SensEmBERT is able to either achieve or surpass state-of-the-art results attained by most of the supervised neural approaches on the English Word Sense Disambiguation task. When scaling to other languages, our representations prove to be equally effective as their English counterpart and outperform the existing state of the art on all the Word Sense Disambiguation multilingual datasets. The embeddings are released in five different languages at http://sensembert.org.", "citations": 121}
{"title": "SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC)", "year": 2021, "authors": "Federico Martelli, N. Kalach, Gabriele Tola, Roberto Navigli", "url": "https://api.semanticscholar.org/CorpusId:234756243", "relevance": 3, "abstract": "In this paper, we introduce the first SemEval task on Multilingual and Cross-Lingual Word-in-Context disambiguation (MCL-WiC). This task allows the largely under-investigated inherent ability of systems to discriminate between word senses within and across languages to be evaluated, dropping the requirement of a fixed sense inventory. Framed as a binary classification, our task is divided into two parts. In the multilingual sub-task, participating systems are required to determine whether two target words, each occurring in a different context within the same language, express the same meaning or not. Instead, in the cross-lingual part, systems are asked to perform the task in a cross-lingual scenario, in which the two target words and their corresponding contexts are provided in two different languages. We illustrate our task, as well as the construction of our manually-created dataset including five languages, namely Arabic, Chinese, English, French and Russian, and the results of the participating systems. Datasets and results are available at: https://github.com/SapienzaNLP/mcl-wic.", "citations": 64}
{"title": "Multilingual Word Sense Disambiguation with Unified Sense Representation", "year": 2022, "authors": "Ying Su, Hongming Zhang, Yangqiu Song, Tong Zhang", "url": "https://api.semanticscholar.org/CorpusId:252818944", "relevance": 3, "abstract": "As a key natural language processing (NLP) task, word sense disambiguation (WSD) evaluates how well NLP models can understand the fine-grained semantics of words under specific contexts. Benefited from the large-scale annotation, current WSD systems have achieved impressive performances in English by combining supervised learning with lexical knowledge. However, such success is hard to be replicated in other languages, where we only have very limited annotations. In this paper, based on that the multilingual lexicon BabelNet describing the same set of concepts across languages, we propose to build knowledge and supervised based Multilingual Word Sense Disambiguation (MWSD) systems. We build unified sense representations for multiple languages and address the annotation scarcity problem for MWSD by transferring annotations from rich sourced languages. With the unified sense representations, annotations from multiple languages can be jointly trained to benefit the MWSD tasks. Evaluations of SemEval-13 and SemEval-15 datasets demonstrate the effectiveness of our methodology.", "citations": 10}
{"title": "Word Sense Disambiguation for 158 Languages using Word Embeddings Only", "year": 2020, "authors": "V. Logacheva, Denis Teslenko, Artem Shelmanov, Steffen Remus, Dmitry Ustalov, Andrey Kutuzov, E. Artemova, Christian Biemann, Simone Paolo Ponzetto, Alexander Panchenko", "url": "https://api.semanticscholar.org/CorpusId:212725472", "relevance": 3, "abstract": "Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this task. However, (i) the inherent Zipfian distribution of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al., (2018), enabling WSD in these languages. Models and system are available online.", "citations": 10}
{"title": "Cross-lingual Word Sense Disambiguation using mBERT Embeddings with Syntactic Dependencies", "year": 2020, "authors": "Xingran Zhu", "url": "https://api.semanticscholar.org/CorpusId:228083808", "relevance": 3, "abstract": "Cross-lingual word sense disambiguation (WSD) tackles the challenge of disambiguating ambiguous words across languages given context. The pre-trained BERT embedding model has been proven to be effective in extracting contextual information of words, and have been incorporated as features into many state-of-the-art WSD systems. In order to investigate how syntactic information can be added into the BERT embeddings to result in both semantics- and syntax-incorporated word embeddings, this project proposes the concatenated embeddings by producing dependency parse tress and encoding the relative relationships of words into the input embeddings. Two methods are also proposed to reduce the size of the concatenated embeddings. The experimental results show that the high dimensionality of the syntax-incorporated embeddings constitute an obstacle for the classification task, which needs to be further addressed in future studies.", "citations": 6}
{"title": "Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks", "year": 2025, "authors": "Roksana Goworek, Haim Dubossarsky", "url": "https://api.semanticscholar.org/CorpusId:279070911", "relevance": 3, "abstract": "Cross-lingual transfer is central to modern NLP, enabling models to perform tasks in languages different from those they were trained on. A common assumption is that training on more languages improves zero-shot transfer. We test this on sense-aware tasks-polysemy and lexical semantic change-and find that multilinguality is not necessary for effective transfer. Our large-scale analysis across 28 languages reveals that other factors, such as differences in pretraining and fine-tuning data and evaluation artifacts, better explain the perceived benefits of multilinguality. We also release fine-tuned models and provide empirical baselines to support future research. While focused on two sense-aware tasks, our findings offer broader insights into cross-lingual transfer, especially for low-resource languages.", "citations": 0}
{"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "year": 2017, "authors": "Shyam Upadhyay, Kai-Wei Chang, Matt Taddy, A. Kalai, James Y. Zou", "url": "https://api.semanticscholar.org/CorpusId:10211692", "relevance": 3, "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense wor d embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields comparable performance to a state of the art monolingual model trained on five times more training data.", "citations": 27}
{"title": "Evaluating Distributed Representations for Multi-Level Lexical Semantics: A Research Proposal", "year": 2024, "authors": "Zhu Liu", "url": "https://api.semanticscholar.org/CorpusId:270211581", "relevance": 3, "abstract": "Modern neural networks (NNs), trained on extensive raw sentence data, construct distributed representations by compressing individual words into dense, continuous, high-dimensional vectors. These representations are expected to capture multi-level lexical meaning. In this thesis, our objective is to examine the efficacy of distributed representations from NNs in encoding lexical meaning. Initially, we identify and formalize three levels of lexical semantics: \\textit{local}, \\textit{global}, and \\textit{mixed} levels. Then, for each level, we evaluate language models by collecting or constructing multilingual datasets, leveraging various language models, and employing linguistic analysis theories. This thesis builds a bridge between computational models and lexical semantics, aiming to complement each other.", "citations": 0}
{"title": "CLUSE: Cross-Lingual Unsupervised Sense Embeddings", "year": 2018, "authors": "Ta-Chung Chi, Yun-Nung (Vivian) Chen", "url": "https://api.semanticscholar.org/CorpusId:52289318", "relevance": 3, "abstract": "This paper proposes a modularized sense induction and representation learning model that jointly learns bilingual sense embeddings that align well in the vector space, where the cross-lingual signal in the English-Chinese parallel corpus is exploited to capture the collocation and distributed characteristics in the language pair. The model is evaluated on the Stanford Contextual Word Similarity (SCWS) dataset to ensure the quality of monolingual sense embeddings. In addition, we introduce Bilingual Contextual Word Similarity (BCWS), a large and high-quality dataset for evaluating cross-lingual sense embeddings, which is the first attempt of measuring whether the learned embeddings are indeed aligned well in the vector space. The proposed approach shows the superior quality of sense embeddings evaluated in both monolingual and bilingual spaces.", "citations": 6}
{"title": "SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods", "year": 2025, "authors": "Roksana Goworek, Harpal Karlcut, M. Shezad, Nijaguna Darshana, Abhishek Mane, Syam Bondada, R. Sikka, Ulvi Mammadov, Rauf Allahverdiyev, Sriram Purighella, Paridhi Gupta, M. Ndegwa, Haim Dubossarsky", "url": "https://api.semanticscholar.org/CorpusId:278997247", "relevance": 2, "abstract": "This paper addresses the critical need for high-quality evaluation datasets in low-resource languages to advance cross-lingual transfer. While cross-lingual transfer offers a key strategy for leveraging multilingual pretraining to expand language technologies to understudied and typologically diverse languages, its effectiveness is dependent on quality and suitable benchmarks. We release new sense-annotated datasets of sentences containing polysemous words, spanning ten low-resource languages across diverse language families and scripts. To facilitate dataset creation, the paper presents a demonstrably beneficial semi-automatic annotation method. The utility of the datasets is demonstrated through Word-in-Context (WiC) formatted experiments that evaluate transfer on these low-resource languages. Results highlight the importance of targeted dataset creation and evaluation for effective polysemy disambiguation in low-resource settings and transfer studies. The released datasets and code aim to support further research into fair, robust, and truly multilingual NLP.", "citations": 2}
{"title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "year": 2016, "authors": "Simon Suster, Ivan Titov, Gertjan van Noord", "url": "https://api.semanticscholar.org/CorpusId:459717", "relevance": 2, "abstract": "We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time.", "citations": 43}
{"title": "MuLaN: Multilingual Label propagatioN for Word Sense Disambiguation", "year": 2020, "authors": "Edoardo Barba, Luigi Procopio, Niccol\u00f2 Campolungo, Tommaso Pasini, Roberto Navigli", "url": "https://www.semanticscholar.org/paper/63d499702f5f8e4777149d7886b61ff2310e1dac", "relevance": 1, "abstract": "The knowledge acquisition bottleneck strongly affects the creation of multilingual sense-annotated data, hence limiting the power of supervised systems when applied to multilingual Word Sense Disambiguation. In this paper, we propose a semi-supervised approach based upon a novel label propagation scheme, which, by jointly leveraging contextualized word embeddings and the multilingual information enclosed in a knowledge base, projects sense labels from a high-resource language, i.e., English, to lower-resourced ones. Backed by several experiments, we provide empirical evidence that our automatically created datasets are of a higher quality than those generated by other competitors and lead a supervised model to achieve state-of-the-art performances in all multilingual Word Sense Disambiguation tasks. We make our datasets available for research purposes at https://github.com/SapienzaNLP/mulan.", "citations": 28}
{"title": "Multilingual Wordnet sense Ranking using nearest context", "year": 2018, "authors": "E. U. Vasanthakumar, Francis Bond", "url": "https://api.semanticscholar.org/CorpusId:34312692", "relevance": 1, "abstract": "In this paper, we combine methods to estimate sense rankings from raw text with recent work on word embeddings to provide sense ranking estimates for the entries in the Open Multilingual WordNet (OMW). The existing Word2Vec pre-trained models from Polygot2 are only built for single word entries, we, therefore, re-train them with multiword expressions from the wordnets, so that multiword expressions can also be ranked. Thus this trained model gives embeddings for both single words and multiwords. The resulting lexicon gives a WSD baseline for five languages. The results are evaluated for Semcor sense corpora for 5 languages using Word2Vec and Glove models. The Glove model achieves an average accuracy of 0.47 and Word2Vec achieves 0.31 for languages such as English, Italian, Indonesian, Chinese and Japanese. The experimentation on OMW sense ranking proves that the rank correlation is generally similar to the human ranking. Hence distributional semantics can aid in Wordnet Sense Ranking.", "citations": 2}
{"title": "A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation", "year": 2016, "authors": "Hong Jin Kang, Tao Chen, Muthu Kumar Chandrasekaran, Min-Yen Kan", "url": "https://api.semanticscholar.org/CorpusId:2640922", "relevance": 1, "abstract": "Word embeddings are now ubiquitous forms of word representation in natural language processing. There have been applications of word embeddings for monolingual word sense disambiguation (WSD) in English, but few comparisons have been done. This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD. Our simplified method leads to comparable state-of-the-art performance without expensive retraining. Cross-Lingual WSD \u2013 where the word senses of a word in a source language come from a separate target translation language \u2013 can also assist in language learning; for example, when providing translations of target vocabulary for learners. Thus we have also applied word embeddings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking. We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does not work well. We discuss the ramifications of this outcome.", "citations": 4}
{"title": "Let\u2019s Play Mono-Poly: BERT Can Reveal Words\u2019 Polysemy Level and Partitionability into Senses", "year": 2021, "authors": "Aina Gar\u00ed Soler, Marianna Apidianaki", "url": "https://www.semanticscholar.org/paper/befbef9f4e4c6269fa712294430ff916cf2fd51c", "relevance": 1, "abstract": "Pre-trained language models (LMs) encode rich information about linguistic structure but their knowledge about lexical polysemy remains unclear. We propose a novel experimental setup for analyzing this knowledge in LMs specifically trained for different languages (English, French, Spanish, and Greek) and in multilingual BERT. We perform our analysis on datasets carefully designed to reflect different sense distributions, and control for parameters that are highly correlated with polysemy such as frequency and grammatical category. We demonstrate that BERT-derived representations reflect words\u2019 polysemy level and their partitionability into senses. Polysemy-related information is more clearly present in English BERT embeddings, but models in other languages also manage to establish relevant distinctions between words at different polysemy levels. Our results contribute to a better understanding of the knowledge encoded in contextualized representations and open up new avenues for multilingual lexical semantics research.", "citations": 76}
{"title": "With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation", "year": 2020, "authors": "Bianca Scarlini, Tommaso Pasini, Roberto Navigli", "url": "https://api.semanticscholar.org/CorpusId:226262325", "relevance": 1, "abstract": "Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors. ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only. We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures. ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.", "citations": 111}
{"title": "Chinese Word Sense Embedding with SememeWSD and Synonym Set", "year": 2022, "authors": "Yangxi Zhou, Junping Du, Zhe Xue, Ang Li, Zeli Guan", "url": "https://api.semanticscholar.org/CorpusId:250113692", "relevance": 1, "abstract": "Word embedding is a fundamental natural language processing task which can learn feature of words. However, most word embedding methods assign only one vector to a word, even if polysemous words have multi-senses. To address this limitation, we propose SememeWSD Synonym (SWSDS) model to assign a different vector to every sense of polysemous words with the help of word sense disambiguation (WSD) and synonym set in OpenHowNet. We use the SememeWSD model, an unsupervised word sense disambiguation model based on OpenHowNet, to do word sense disambiguation and annotate the polysemous word with sense id. Then, we obtain top 10 synonyms of the word sense from OpenHowNet and calculate the average vector of synonyms as the vector of the word sense. In experiments, We evaluate the SWSDS model on semantic similarity calculation with Gensim's wmdistance method. It achieves improvement of accuracy. We also examine the SememeWSD model on different BERT models to find the more effective model.", "citations": 5}
{"title": "Improving Word Sense Disambiguation in Neural Machine Translation with Sense Embeddings", "year": 2017, "authors": "Annette Rios Gonzales, Laura Mascarell, Rico Sennrich", "url": "https://api.semanticscholar.org/CorpusId:529114", "relevance": 1, "abstract": "Word sense disambiguation is necessary in translation because different word senses often have different translations. Neural machine translation models learn different senses of words as part of an end-to-end translation task, and their capability to perform word sense disambiguation has so far not been quantified. We exploit the fact that neural translation models can score arbitrary translations to design a novel cross-lingual word sense disambiguation task that is tailored towards evaluating neural machine translation models. We present a test set of 7,200 lexical ambiguities for German \u2192 English, and 6,700 for German \u2192 French, and report baseline results. With 70% of lexical ambiguities correctly disambiguated, we find that word sense disambiguation remains a challenging problem for neural machine translation, especially for rare word senses. To improve word sense disambiguation in neural machine translation, we experiment with two methods to integrate sense embeddings. In a first approach we pass sense embeddings as additional input to the neural machine translation system. For the second experiment, we extract lexical chains based on sense embeddings from the document and integrate this information into the NMT model. While a baseline NMT system disambiguates frequent word senses quite reliably, the annotation with both sense labels and lexical chains improves the neural models\u2019 performance on rare word senses.", "citations": 133}
{"title": "Rutgers Multimedia Image Processing Lab at SemEval-2023 Task-1: Text-Augmentation-based Approach for Visual Word Sense Disambiguation", "year": 2023, "authors": "Keyi Li, Sen Yang, Chenyu Gao, I. Marsic", "url": "https://api.semanticscholar.org/CorpusId:259376584", "relevance": 1, "abstract": "This paper describes our system used in SemEval-2023 Task-1: Visual Word Sense Disambiguation (VWSD). The VWSD task is to identify the correct image that corresponds to an ambiguous target word given limited textual context. To reduce word ambiguity and enhance image selection, we proposed several text augmentation techniques, such as prompting, WordNet synonyms, and text generation. We experimented with different vision-language pre-trained models to capture the joint features of the augmented text and image. Our approach achieved the best performance using a combination of GPT-3 text generation and the CLIP model. On the multilingual test sets, our system achieved an average hit rate (at top-1) of 51.11 and a mean reciprocal rank of 65.69.", "citations": 1}
{"title": "Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives", "year": 2021, "authors": "Ming Wang, Yinglin Wang", "url": "https://api.semanticscholar.org/CorpusId:236460274", "relevance": 1, "abstract": "Lately proposed Word Sense Disambiguation (WSD) systems have approached the estimated upper bound of the task on standard evaluation benchmarks. However, these systems typically implement the disambiguation of words in a document almost independently, underutilizing sense and word dependency in context. In this paper, we convert the nearly isolated decisions into interrelated ones by exposing senses in context when learning sense embeddings in a similarity-based Sense Aware Context Exploitation (SACE) architecture. Meanwhile, we enhance the context embedding learning with selected sentences from the same document, rather than utilizing only the sentence where each ambiguous word appears. Experiments on both English and multilingual WSD datasets have shown the effectiveness of our approach, surpassing previous state-of-the-art by large margins (3.7% and 1.2% respectively), especially on few-shot (14.3%) and zero-shot (35.9%) scenarios.", "citations": 17}
{"title": "SemEval-2010 Task 3: Cross-lingual Word Sense Disambiguation", "year": 2009, "authors": "Els Lefever, Veronique Hoste", "url": "https://api.semanticscholar.org/CorpusId:2085863", "relevance": 1, "abstract": "We propose a multilingual unsupervised Word Sense Disambiguation (WSD) task for a sample of English nouns. Instead of providing manually sensetagged examples for each sense of a polysemous noun, our sense inventory is built up on the basis of the Europarl parallel corpus. The multilingual setup involves the translations of a given English polysemous noun in five supported languages, viz. Dutch, French, German, Spanish and Italian. \n \nThe task targets the following goals: (a) the manual creation of a multilingual sense inventory for a lexical sample of English nouns and (b) the evaluation of systems on their ability to disambiguate new occurrences of the selected polysemous nouns. For the creation of the hand-tagged gold standard, all translations of a given polysemous English noun are retrieved in the five languages and clustered by meaning. Systems can participate in 5 bilingual evaluation subtasks (English -- Dutch, English -- German, etc.) and in a multilingual subtask covering all language pairs. \n \nAs WSD from cross-lingual evidence is gaining popularity, we believe it is important to create a multilingual gold standard and run cross-lingual WSD benchmark tests.", "citations": 131}
{"title": "Generating sense inventories for ambiguous arabic words", "year": 2021, "authors": "Marwah Alian, A. Awajan", "url": "https://api.semanticscholar.org/CorpusId:235427991", "relevance": 1, "abstract": "The process of selecting the appropriate meaning of an ambigous word according to its context is known as word sense disambiguation. In this research, we generate a number of Arabic sense inventories based on an unsupervised approach and different pre-trained embeddings, such as Aravec, Fast text, and Arabic-News embeddings. The resulted inventories from the pre-trained embeddings are evaluated to investigate their efficiency in Arabic word sense disambiguation and sentence similarity. The sense inventories are generated using an unsupervised approach that is based on a graph-based word sense induction algorithm. Results show that the Aravec-Twitter inventory achieves the best accuracy of 0.47 for 50 neighbors and a close accuracy to the Fast text inventory for 200 neighbors while it provides similar accuracy to the Arabic-News inventory for 100neighbors. The experiment of replacing ambiguous words with their sense vectors is tested for sentence similarity using all sense inventories and the results show that using Aravec-Twitter sense inventory provides a better correlation value", "citations": 2}
{"title": "EViLBERT: Learning Task-Agnostic Multimodal Sense Embeddings", "year": 2020, "authors": "Agostina Calabrese, Michele Bevilacqua, Roberto Navigli", "url": "https://www.semanticscholar.org/paper/366c3f0c35ddce5e10a7e262f09c1f1518b58e27", "relevance": 1, "abstract": "The problem of grounding language in vision is increasingly attracting scholarly efforts. As of now, however, most of the approaches have been limited to word embeddings, which are not capable of handling polysemous words. This is mainly due to the limited coverage of the available semantically-annotated datasets, hence forcing research to rely on alternative technologies (i.e., image search engines). To address this issue, we introduce EViLBERT, an approach which is able to perform image classification over an open set of concepts, both concrete and non-concrete. Our approach is based on the recently introduced Vision-Language Pretraining (VLP) model, and builds upon a manually-annotated dataset of concept-image pairs. We use our technique to clean up the image-to-concept mapping that is provided within a multilingual knowledge base, resulting in over 258,000 images associated with 42,500 concepts. \n\nWe show that our VLP-based model can be used to create multimodal sense embeddings starting from our automatically-created dataset. In turn, we also show that these multimodal embeddings improve the performance of a Word Sense Disambiguation architecture over a strong unimodal baseline. We release code, dataset and embeddings at http://babelpic.org.", "citations": 9}
{"title": "Multilingual Word Sense Disambiguation for Semantic Annotations: Fusing Knowledge Graphs, Lexical Resources, and Large Language Models", "year": 2024, "authors": "Robert David, Anna Kernerman, Ilan Kernerman, Nicolas Ferranti, Assaf Siani", "url": "https://www.semanticscholar.org/paper/1242c00475ae03057d15f5ceb2df640de3f6bf5a", "relevance": 1, "abstract": "", "citations": 1}
{"title": "A Hybrid Contextual Embedding and Hierarchical Attention for Improving the Performance of Word Sense Disambiguation", "year": 2025, "authors": "Robbel Habtamu Yigzaw, Beakal Gizachew Assefa, Elefelious Getachew Belay", "url": "https://www.semanticscholar.org/paper/bc3e76b700846e49f253ad147267e57914c7a111", "relevance": 1, "abstract": "Word Sense Disambiguation is determining the correct sense of an ambiguous word within context. It plays a crucial role in natural language applications such as machine translation, question-answering, chatbots, information retrieval, sentiment analysis, and overall language comprehension. Recent advancements in this area have focused on utilizing deep contextual models to address these challenges. However, despite this positive progress, semantical and syntactical ambiguity remains a challenge, especially when dealing with polysomy words, and it is considered an AI-complete problem. In this work, we propose an approach that integrates hierarchical attention mechanisms and BERT embeddings to enhance WSD performance. Our model, incorporating local and global attention, demonstrates significant improvements in accuracy, particularly in complex sentence structures. To the best of our knowledge, our model is the first to incorporate hierarchical attention mechanisms integrated with contextual embedding. We conducted experiment on publicly available datasets for English and Italian language. Experimental results show that our model achieves state-of-the-art results in WSD, surpassing baseline models up to 2.9% F1 accuracy on English WSD. Additionally, it demonstrates superior performance in Italian WSD, outperforming existing papers up to 0.7% F1 accuracy. We further adapted the model for Amharic word sense disambiguation. Despite the absence of a standard benchmark dataset for Amharic WSD, our model achieved an accuracy of 92.4% on a dataset we prepared ourselves. Our findings underscore the significance of linguistic features in contextual information capture for WSD. While Part-of-Speech (POS) tagging has a limited impact, word embeddings significantly influence performance. Local and global attention further improve results, particularly at the word level. Overall the results emphasize the importance of context in WSD, advancing context-aware natural language processing systems.", "citations": 3}
{"title": "From Word to Sense Embeddings: A Survey on Vector Representations of Meaning", "year": 2018, "authors": "Jos\u00e9 Camacho-Collados, Mohammad Taher Pilehvar", "url": "https://api.semanticscholar.org/CorpusId:13696533", "relevance": 1, "abstract": "\n \n \nOver the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality. \n \n \n", "citations": 363}
{"title": "EDS-MEMBED: Multi-sense embeddings based on enhanced distributional semantic structures via a graph walk over word senses", "year": 2021, "authors": "E. F. Ayetiran, Petr Sojka, V'it Novotn'y", "url": "https://api.semanticscholar.org/CorpusId:232075721", "relevance": 1, "abstract": "", "citations": 10}
{"title": "LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and Beyond", "year": 2021, "authors": "Daniel Loureiro, A. Jorge, Jos\u00e9 Camacho-Collados", "url": "https://api.semanticscholar.org/CorpusId:235196103", "relevance": 1, "abstract": "", "citations": 30}
{"title": "Sentence Semantic Similarity based Complex Network approach for Word Sense Disambiguation", "year": 2023, "authors": "Et al. Gopal Mohadikar", "url": "https://api.semanticscholar.org/CorpusId:265896546", "relevance": 1, "abstract": "Word Sense Disambiguation is a branch of Natural Language Processing(NLP) that deals with multi-sense words. The multi-sense words are referred to as the polysemous words. The term lexical ambiguity is introduced by the multi-sense words. The existing sense disambiguation module works effectively for single sentences with available context information. The word embedding plays a vital role in the process of disambiguation. The context-dependent word embedding model is used for disambiguation. The main goal of this research paper is to disambiguate the polysemous words by considering available context information. The main identified challenge of disambiguation is the ambiguous word without context information. The discussed complex network approach is disambiguating ambiguous sentences by considering the semantic similarities. The sentence semantic similarity-based network is constructed for disambiguating ambiguous sentences. The proposed methodology is trained with SemCor, Adaptive-Lex, and OMSTI standard lexical resources. The findings state that the discussed methodology is working fine for disambiguating large documents where the sense of ambiguous sentences is on the adjacent sentences.", "citations": 0}
{"title": "Integrating techniques of social network analysis and word embedding for word sense disambiguation", "year": 2025, "authors": "Chihli Hung, Chihli Hung, Hsien-Ming Chou", "url": "https://www.semanticscholar.org/paper/ab704eac35f65feafbff01677cb9afd9dafc8793", "relevance": 1, "abstract": "PurposeThis research addresses the challenge of polysemous words in word embedding techniques, which are commonly used in text mining. It aims to resolve word sense ambiguity by introducing a social network sense disambiguation (SNSD) model based on social network analysis (SNA).Design/methodology/approachThe SNSD model treats words as members of a social network and their co-occurrence relationships as interactions. By analyzing these interactions, the model identifies words with high betweenness centrality, which may act as bridges between different word sense communities, indicating polysemy. This unsupervised method does not rely on pre-tagged resources and is validated using the IMDb dataset.FindingsThe SNSD model effectively resolves word sense ambiguity in word embeddings, proving to be a cost-effective and adaptable solution to this issue. The experimental results demonstrate that the model enhances the accuracy of word embeddings by accurately identifying the correct meanings of polysemous words.Originality/valueThis study is the first to apply SNA to word sense disambiguation (WSD). The SNSD model offers a novel, unsupervised approach that overcomes the limitations of traditional supervised or knowledge-based methods, providing a valuable contribution to the field of text mining.", "citations": 0}
{"title": "BCWS: Bilingual Contextual Word Similarity", "year": 2018, "authors": "Ta-Chung Chi, Ching-Yen Shih, Yun-Nung (Vivian) Chen", "url": "https://api.semanticscholar.org/CorpusId:53047117", "relevance": 1, "abstract": "This paper introduces the first dataset for evaluating English-Chinese Bilingual Contextual Word Similarity, namely BCWS (this https URL). The dataset consists of 2,091 English-Chinese word pairs with the corresponding sentential contexts and their similarity scores annotated by the human. Our annotated dataset has higher consistency compared to other similar datasets. We establish several baselines for the bilingual embedding task to benchmark the experiments. Modeling cross-lingual sense representations as provided in this dataset has the potential of moving artificial intelligence from monolingual understanding towards multilingual understanding.", "citations": 3}
{"title": "From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation", "year": 2022, "authors": "Marianna Apidianaki", "url": "https://api.semanticscholar.org/CorpusId:254960044", "relevance": 1, "abstract": "Vector-based word representation paradigms situate lexical meaning at different levels of abstraction. Distributional and static embedding models generate a single vector per word type, which is an aggregate across the instances of the word in a corpus. Contextual language models, on the contrary, directly capture the meaning of individual word instances. The goal of this survey is to provide an overview of word meaning representation methods, and of the strategies that have been proposed for improving the quality of the generated vectors. These often involve injecting external knowledge about lexical semantic relationships, or refining the vectors to describe different senses. The survey also covers recent approaches for obtaining word type-level representations from token-level ones, and for combining static and contextualized representations. Special focus is given to probing and interpretation studies aimed at discovering the lexical semantic knowledge that is encoded in contextualized representations. The challenges posed by this exploration have motivated the interest towards static embedding derivation from contextualized embeddings, and for methods aimed at improving the similarity estimates that can be drawn from the space of contextual language models.", "citations": 37}
{"title": "Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings", "year": 2020, "authors": "Christos Xypolopoulos, A. Tixier, M. Vazirgiannis", "url": "https://api.semanticscholar.org/CorpusId:214612434", "relevance": 1, "abstract": "The number of senses of a given word, or polysemy, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate polysemy based on simple geometry in the contextual embedding space. Our approach is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as WordNet, OntoNotes, Oxford, Wikipedia, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes it applicable to any language. Code and data are publicly available https://github.com/ksipos/polysemy-assessment .", "citations": 10}
{"title": "Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy", "year": 2021, "authors": "Marcos Garcia", "url": "https://api.semanticscholar.org/CorpusId:235652017", "relevance": 1, "abstract": "This paper presents a multilingual study of word meaning representations in context. We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy. To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context. However, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences. Experiments are performed in Galician, Portuguese, English, and Spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study.", "citations": 25}
{"title": "NLP Research and Resources at DaSciM, Ecole Polytechnique", "year": 2021, "authors": "Hadi Abdine, Yanzhu Guo, Moussa Kamal Eddine, Giannis Nikolentzos, Stamatis Outsios, Guokan Shang, Christos Xypolopoulos, M. Vazirgiannis", "url": "https://api.semanticscholar.org/CorpusId:244773040", "relevance": 1, "abstract": "DaSciM (Data Science and Mining) part of LIX at Ecole Polytechnique, established in 2013 and since then producing research results in the area of large scale data analysis via methods of machine and deep learning. The group has been specifically active in the area of NLP and text mining with interesting results at methodological and resources level. Here follow our different contributions of interest to the AFIA community. 1 Graph based representations for NLP and Text Mining In recent years, graphs have become a widely used tool for modeling structured data. To enable the application of graph-based approaches to textual data, members of the DaSciM team developed the graph-of-words approach [RV13] which maps text to a graph where vertices correspond to terms and edges represent co-occurrences between the connected terms within a fixed-size window. Once a document is represented as a graph, traditional, but also modern algorithms designed for graph-structured data can be applied to natural language texts. The researchers of the DaSciM team have explored how different text mining tasks can benefit from graph-based algorithms. One such task is keyword extraction. Capitalizing on the concept of graph degeneracy, the k-core algorithm was applied to the graph representation of text to identify cohesive subgraphs [RV15; TMV16]. The vertices of these subgraphs can be considered as the most important terms (i.e., keywords) of a given textual document. The members of the DaSciM team have also utilized machine learning algorithms that operate on graphs to deal with tasks such as text categorization and sentiment analysis. Both graph kernels [Nik+17] and graph neural networks [NTV20], the two dominant methodologies for performing machine learning on graphs, have been applied to these problems with great success. The effectiveness of the graph-based representations has also been evaluated in the task of detecting sub-events from data collected from Twitter [Mel+15; Mel+18]. The occurrence of a sub-event is usually associated with a change in the content of the messages posted recently by users compared to the content of the messages posted in the past. Such a significant change of content is reflected in the structure of the graph representation of tweets and can be captured by graph-based approaches.", "citations": 0}
{"title": "Evaluating multi-sense embeddings for semantic resolution monolingually and in word translation", "year": 2016, "authors": "G\u00e1bor Borb\u00e9ly, M\u00e1rton Makrai, D. Nemeskey, Andr\u00e1s Kornai", "url": "https://api.semanticscholar.org/CorpusId:18614158", "relevance": 1, "abstract": "Multi-sense word embeddings (MSEs) model different meanings of word forms with different vectors. We propose two new methods for evaluating MSEs, one based on monolingual dictionaries, and the other exploiting the principle that words may be ambiguous as far as the postulated senses translate to different words in some other language.", "citations": 5}
{"title": "An Unsupervised, Geometric and Syntax-aware Quantification of Polysemy", "year": 2022, "authors": "Anmol Goel, Charu Sharma, P. Kumaraguru", "url": "https://api.semanticscholar.org/CorpusId:256461217", "relevance": 1, "abstract": "Polysemy is the phenomenon where a single word form possesses two or more related senses. It is an extremely ubiquitous part of natural language and analyzing it has sparked rich discussions in the linguistics, psychology and philosophy communities alike. With scarce attention paid to polysemy in computational linguistics, and even scarcer attention toward quantifying polysemy, in this paper, we propose a novel, unsupervised framework to compute and estimate polysemy scores for words in multiple languages. We infuse our proposed quantification with syntactic knowledge in the form of dependency structures. This informs the final polysemy scores of the lexicon motivated by recent linguistic findings that suggest there is an implicit relation between syntax and ambiguity/polysemy. We adopt a graph based approach by computing the discrete Ollivier Ricci curvature on a graph of the contextual nearest neighbors. We test our framework on curated datasets controlling for different sense distributions of words in 3 typologically diverse languages - English, French and Spanish. The effectiveness of our framework is demonstrated by significant correlations of our quantification with expert human annotated language resources like WordNet. We observe a 0.3 point increase in the correlation coefficient as compared to previous quantification studies in English. Our research leverages contextual language models and syntactic structures to empirically support the widely held theoretical linguistic notion that syntax is intricately linked to ambiguity/polysemy.", "citations": 5}
