[
  {
    "title": "A Survey of Cross-lingual Word Embedding Models",
    "year": 2017,
    "authors": "Sebastian Ruder, Ivan Vulic, Anders S\u00f8gaard",
    "url": "https://api.semanticscholar.org/CorpusId:26127787",
    "relevance": 3,
    "abstract": "Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent, modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.",
    "citations": 566
  },
  {
    "title": "Unsupervised Multilingual Word Embeddings",
    "year": 2018,
    "authors": "Xilun Chen, Claire Cardie",
    "url": "https://api.semanticscholar.org/CorpusId:52099904",
    "relevance": 3,
    "abstract": "Multilingual Word Embeddings (MWEs) represent words from multiple languages in a single distributional vector space. Unsupervised MWE (UMWE) methods acquire multilingual embeddings without cross-lingual supervision, which is a significant advantage over traditional supervised approaches and opens many new possibilities for low-resource languages. Prior art for learning UMWEs, however, merely relies on a number of independently trained Unsupervised Bilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These methods fail to leverage the interdependencies that exist among many languages. To address this shortcoming, we propose a fully unsupervised framework for learning MWEs that directly exploits the relations between all language pairs. Our model substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition, our model even beats supervised approaches trained with cross-lingual resources.",
    "citations": 138
  },
  {
    "title": "Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment",
    "year": 2018,
    "authors": "Muhao Chen, Yingtao Tian, Kai-Wei Chang, S. Skiena, C. Zaniolo",
    "url": "https://www.semanticscholar.org/paper/5902fa7e6f637fea16ff325ab0e25c88bf31f27a",
    "relevance": 3,
    "abstract": "Multilingual knowledge graph (KG) embeddings provide latent semantic representations of entities and structured knowledge with cross-lingual inferences, which benefit various knowledge-driven cross-lingual NLP tasks. However, precisely learning such cross-lingual inferences is usually hindered by the low coverage of entity alignment in many KGs. Since many multilingual KGs also provide literal descriptions of entities, in this paper, we introduce an embedding-based approach which leverages a weakly aligned multilingual KG for semi-supervised cross-lingual learning using entity descriptions. Our approach performs co-training of two embedding models, i.e. a multilingual KG embedding model and a multilingual literal description embedding model. The models are trained on a large Wikipedia-based trilingual dataset where most entity alignment is unknown to training. Experimental results show that the performance of the proposed approach on the entity alignment task improves at each iteration of co-training, and eventually reaches a stage at which it significantly surpasses previous approaches. We also show that our approach has promising abilities for zero-shot entity alignment, and cross-lingual KG completion.",
    "citations": 247
  },
  {
    "title": "Multilingual Models for Compositional Distributed Semantics",
    "year": 2014,
    "authors": "Karl Moritz Hermann, Phil Blunsom",
    "url": "https://api.semanticscholar.org/CorpusId:17589422",
    "relevance": 3,
    "abstract": "We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data.",
    "citations": 320
  },
  {
    "title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer",
    "year": 2020,
    "authors": "Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang, Ahmed Hassan Awadallah",
    "url": "https://api.semanticscholar.org/CorpusId:218487087",
    "relevance": 3,
    "abstract": "Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.",
    "citations": 95
  },
  {
    "title": "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity",
    "year": 2017,
    "authors": "Jos\u00e9 Camacho-Collados, Mohammad Taher Pilehvar, Nigel Collier, Roberto Navigli",
    "url": "https://api.semanticscholar.org/CorpusId:7665329",
    "relevance": 3,
    "abstract": "This paper introduces a new task on Multilingual and Cross-lingual SemanticThis paper introduces a new task on Multilingual and Cross-lingual Semantic Word Similarity which measures the semantic similarity of word pairs within and across five languages: English, Farsi, German, Italian and Spanish. High quality datasets were manually curated for the five languages with high inter-annotator agreements (consistently in the 0.9 ballpark). These were used for semi-automatic construction of ten cross-lingual datasets. 17 teams participated in the task, submitting 24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri.org/semeval2017/task2/",
    "citations": 157
  },
  {
    "title": "Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples",
    "year": 2025,
    "authors": "Andrianos Michail, Simon Clematide, Rico Sennrich",
    "url": "https://api.semanticscholar.org/CorpusId:276287513",
    "relevance": 3,
    "abstract": "The evaluation of cross-lingual semantic search models is often limited to existing datasets from tasks such as information retrieval and semantic textual similarity. We introduce Cross-Lingual Semantic Discrimination (CLSD), a lightweight evaluation task that requires only parallel sentences and a Large Language Model (LLM) to generate adversarial distractors. CLSD measures an embedding model's ability to rank the true parallel sentence above semantically misleading but lexically similar alternatives. As a case study, we construct CLSD datasets for German--French in the news domain. Our experiments show that models fine-tuned for retrieval tasks benefit from pivoting through English, whereas bitext mining models perform best in direct cross-lingual settings. A fine-grained similarity analysis further reveals that embedding models differ in their sensitivity to linguistic perturbations. We release our code and datasets under AGPL-3.0: https://github.com/impresso/cross_lingual_semantic_discrimination",
    "citations": 4
  },
  {
    "title": "Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models",
    "year": 2019,
    "authors": "Takashi Wada, Tomoharu Iwata, Yuji Matsumoto",
    "url": "https://api.semanticscholar.org/CorpusId:150149297",
    "relevance": 3,
    "abstract": "Recently, a variety of unsupervised methods have been proposed that map pre-trained word embeddings of different languages into the same space without any parallel data. These methods aim to find a linear transformation based on the assumption that monolingual word embeddings are approximately isomorphic between languages. However, it has been demonstrated that this assumption holds true only on specific conditions, and with limited resources, the performance of these methods decreases drastically. To overcome this problem, we propose a new unsupervised multilingual embedding method that does not rely on such assumption and performs well under resource-poor scenarios, namely when only a small amount of monolingual data (i.e., 50k sentences) are available, or when the domains of monolingual data are different across languages. Our proposed model, which we call \u2018Multilingual Neural Language Models\u2019, shares some of the network parameters among multiple languages, and encodes sentences of multiple languages into the same space. The model jointly learns word embeddings of different languages in the same space, and generates multilingual embeddings without any parallel data or pre-training. Our experiments on word alignment tasks have demonstrated that, on the low-resource condition, our model substantially outperforms existing unsupervised and even supervised methods trained with 500 bilingual pairs of words. Our model also outperforms unsupervised methods given different-domain corpora across languages. Our code is publicly available.",
    "citations": 25
  },
  {
    "title": "Cross-lingual transfer of sentiment classifiers",
    "year": 2021,
    "authors": "M. Robnik-Sikonja, Kristjan Reba, I. Mozeti\u010d",
    "url": "https://api.semanticscholar.org/CorpusId:237838256",
    "relevance": 3,
    "abstract": "Word embeddings represent words in a numeric space so that semantic relations between words are represented as distances and directions in the vector space. Cross-lingual word embeddings transform vector spaces of different languages so that similar words are aligned. This is done by mapping one language\u2019s vector space to the vector space of another language or by construction of a joint vector space for multiple languages. Cross-lingual embeddings can be used to transfer machine learning models between languages, thereby compensating for insufficient data in less-resourced languages. We use cross-lingual word embeddings to transfer machine learning prediction models for Twitter sentiment between 13 languages. We focus on two transfer mechanisms that recently show superior transfer performance. The first mechanism uses the trained models whose input is the joint numerical space for many languages as implemented in the LASER library. The second mechanism uses large pretrained multilingual BERT language models. Our experiments show that the transfer of models between similar languages is sensible, even with no target language data. The performance of cross-lingual models obtained with the multilingual BERT and LASER library is comparable, and the differences are language-dependent. The transfer with CroSloEngual BERT, pretrained on only three languages, is superior on these and some closely related languages.",
    "citations": 9
  },
  {
    "title": "Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment",
    "year": 2024,
    "authors": "Zhongtao Miao, Qiyu Wu, Kaiyan Zhao, Zilong Wu, Yoshimasa Tsuruoka",
    "url": "https://api.semanticscholar.org/CorpusId:268875764",
    "relevance": 3,
    "abstract": "The field of cross-lingual sentence embeddings has recently experienced significant advancements, but research concerning low-resource languages has lagged due to the scarcity of parallel corpora. This paper shows that cross-lingual word representation in low-resource languages is notably under-aligned with that in high-resource languages in current models. To address this, we introduce a novel framework that explicitly aligns words between English and eight low-resource languages, utilizing off-the-shelf word alignment models. This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking. We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on sentence embeddings in low-resource languages. In addition, the competitive performance of the proposed model across a broader range of tasks in high-resource languages underscores its practicality.",
    "citations": 20
  },
  {
    "title": "Learning Unsupervised Multilingual Word Embeddings with Incremental Multilingual Hubs",
    "year": 2019,
    "authors": "Geert Heyman, B. Verreet, Ivan Vulic, Marie-Francine Moens",
    "url": "https://api.semanticscholar.org/CorpusId:174800783",
    "relevance": 3,
    "abstract": "Recent research has discovered that a shared bilingual word embedding space can be induced by projecting monolingual word embedding spaces from two languages using a self-learning paradigm without any bilingual supervision. However, it has also been shown that for distant language pairs such fully unsupervised self-learning methods are unstable and often get stuck in poor local optima due to reduced isomorphism between starting monolingual spaces. In this work, we propose a new robust framework for learning unsupervised multilingual word embeddings that mitigates the instability issues. We learn a shared multilingual embedding space for a variable number of languages by incrementally adding new languages one by one to the current multilingual space. Through the gradual language addition the method can leverage the interdependencies between the new language and all other languages in the current multilingual space. We find that it is beneficial to project more distant languages later in the iterative process. Our fully unsupervised multilingual embedding spaces yield results that are on par with the state-of-the-art methods in the bilingual lexicon induction (BLI) task, and simultaneously obtain state-of-the-art scores on two downstream tasks: multilingual document classification and multilingual dependency parsing, outperforming even supervised baselines. This finding also accentuates the need to establish evaluation protocols for cross-lingual word embeddings beyond the omnipresent intrinsic BLI task in future work.",
    "citations": 23
  },
  {
    "title": "Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language Models",
    "year": 2018,
    "authors": "Takashi Wada, Tomoharu Iwata",
    "url": "https://api.semanticscholar.org/CorpusId:52179115",
    "relevance": 3,
    "abstract": "We propose an unsupervised method to obtain cross-lingual embeddings without any parallel data or pre-trained word embeddings. The proposed model, which we call multilingual neural language models, takes sentences of multiple languages as an input. The proposed model contains bidirectional LSTMs that perform as forward and backward language models, and these networks are shared among all the languages. The other parameters, i.e. word embeddings and linear transformation between hidden states and outputs, are specific to each language. The shared LSTMs can capture the common sentence structure among all languages. Accordingly, word embeddings of each language are mapped into a common latent space, making it possible to measure the similarity of words across multiple languages. We evaluate the quality of the cross-lingual word embeddings on a word alignment task. Our experiments demonstrate that our model can obtain cross-lingual embeddings of much higher quality than existing unsupervised models when only a small amount of monolingual data (i.e. 50k sentences) are available, or the domains of monolingual data are different across languages.",
    "citations": 28
  },
  {
    "title": "Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation",
    "year": 2023,
    "authors": "Di Wu, C. Monz",
    "url": "https://api.semanticscholar.org/CorpusId:258841499",
    "relevance": 3,
    "abstract": "Using a vocabulary that is shared across languages is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, assuming that shared tokens refer to similar meanings across languages. However, when word overlap is small, especially due to different writing systems, transfer is inhibited. In this paper, we define word-level information transfer pathways via word equivalence classes and rely on graph networks to fuse word embeddings across languages. Our experiments demonstrate the advantages of our approach: 1) embeddings of words with similar meanings are better aligned across languages, 2) our method achieves consistent BLEU improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less than 1.0\\% additional trainable parameters are required with a limited increase in computational costs, while inference time remains identical to the baseline. We release the codebase to the community.",
    "citations": 10
  },
  {
    "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
    "year": 2024,
    "authors": "Xinyu Crystina Zhang, Jing Lu, Vinh Q. Tran, Tal Schuster, Donald Metzler, Jimmy Lin",
    "url": "https://api.semanticscholar.org/CorpusId:273877383",
    "relevance": 3,
    "abstract": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form\"semantic tokens\"by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
    "citations": 2
  },
  {
    "title": "Cross-lingual Transfer of Twitter Sentiment Models Using a Common Vector Space",
    "year": 2020,
    "authors": "M. Robnik-Sikonja, Kristjan Reba, I. Mozeti\u010d",
    "url": "https://api.semanticscholar.org/CorpusId:218665268",
    "relevance": 3,
    "abstract": "Word embeddings represent words in a numeric space in such a way that semantic relations between words are encoded as distances and directions in the vector space. Cross-lingual word embeddings map words from one language to the vector space of another language, or words from multiple languages to the same vector space where similar words are aligned. Cross-lingual embeddings can be used to transfer machine learning models between languages and thereby compensate for insufficient data in less-resourced languages. We use cross-lingual word embeddings to transfer machine learning prediction models for Twitter sentiment between 13 languages. We focus on two transfer mechanisms using the joint numerical space for many languages as implemented in the LASER library: the transfer of trained models, and expansion of training sets with instances from other languages. Our experiments show that the transfer of models between similar languages is sensible, while dataset expansion did not increase the predictive performance.",
    "citations": 6
  },
  {
    "title": "Cross-Lingual BERT Contextual Embedding Space Mapping with Isotropic and Isometric Conditions",
    "year": 2021,
    "authors": "Haoran Xu, Philipp Koehn",
    "url": "https://api.semanticscholar.org/CorpusId:236133964",
    "relevance": 3,
    "abstract": "Typically, a linearly orthogonal transformation mapping is learned by aligning static type-level embeddings to build a shared semantic space. In view of the analysis that contextual embeddings contain richer semantic features, we investigate a context-aware and dictionary-free mapping approach by leveraging parallel corpora. We illustrate that our contextual embedding space mapping significantly outperforms previous multilingual word embedding methods on the bilingual dictionary induction (BDI) task by providing a higher degree of isomorphism. To improve the quality of mapping, we also explore sense-level embeddings that are split from type-level representations, which can align spaces in a finer resolution and yield more precise mapping. Moreover, we reveal that contextual embedding spaces suffer from their natural properties -- anisotropy and anisometry. To mitigate these two problems, we introduce the iterative normalization algorithm as an imperative preprocessing step. Our findings unfold the tight relationship between isotropy, isometry, and isomorphism in normalized contextual embedding spaces.",
    "citations": 13
  },
  {
    "title": "A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings",
    "year": 2019,
    "authors": "Niels van der Heijden, Samira Abnar, Ekaterina Shutova",
    "url": "https://api.semanticscholar.org/CorpusId:209444184",
    "relevance": 3,
    "abstract": "The lack of annotated data in many languages is a well-known challenge within the field of multilingual natural language processing (NLP). Therefore, many recent studies focus on zero-shot transfer learning and joint training across languages to overcome data scarcity for low-resource languages. In this work we (i) perform a comprehensive comparison of state-of-the-art multilingual word and sentence encoders on the tasks of named entity recognition (NER) and part of speech (POS) tagging; and (ii) propose a new method for creating multilingual contextualized word embeddings, compare it to multiple baselines and show that it performs at or above state-of-the-art level in zero-shot transfer settings. Finally, we show that our method allows for better knowledge sharing across languages in a joint training setting.",
    "citations": 16
  },
  {
    "title": "When Word Embeddings Become Endangered",
    "year": 2021,
    "authors": "Khalid Alnajjar",
    "url": "https://api.semanticscholar.org/CorpusId:232335507",
    "relevance": 3,
    "abstract": "Big languages such as English and Finnish have many natural language processing (NLP) resources and models, but this is not the case for low-resourced and endangered languages as such resources are so scarce despite the great advantages they would provide for the language communities. The most common types of resources available for low-resourced and endangered languages are translation dictionaries and universal dependencies. In this paper, we present a method for constructing word embeddings for endangered languages using existing word embeddings of different resource-rich languages and the translation dictionaries of resource-poor languages. Thereafter, the embeddings are fine-tuned using the sentences in the universal dependencies and aligned to match the semantic spaces of the big languages; resulting in cross-lingual embeddings. The endangered languages we work with here are Erzya, Moksha, Komi-Zyrian and Skolt Sami. Furthermore, we build a universal sentiment analysis model for all the languages that are part of this study, whether endangered or not, by utilizing cross-lingual word embeddings. The evaluation conducted shows that our word embeddings for endangered languages are well-aligned with the resource-rich languages, and they are suitable for training task-specific models as demonstrated by our sentiment analysis models which achieved high accuracies. All our cross-lingual word embeddings and sentiment analysis models will be released openly via an easy-to-use Python library.",
    "citations": 11
  },
  {
    "title": "Learning Multilingual Word Embeddings Using Image-Text Data",
    "year": 2019,
    "authors": "K. Singhal, K. Raman, B. T. Cate",
    "url": "https://api.semanticscholar.org/CorpusId:168170148",
    "relevance": 3,
    "abstract": "There has been significant interest recently in learning multilingual word embeddings \u2013 in which semantically similar words across languages have similar embeddings. State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings. In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data. In particular, we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text. Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.",
    "citations": 10
  },
  {
    "title": "Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings",
    "year": 2021,
    "authors": "Linlin Liu, Thien Hai Nguyen, Shafiq R. Joty, Lidong Bing, Luo Si",
    "url": "https://api.semanticscholar.org/CorpusId:232185390",
    "relevance": 3,
    "abstract": "Cross-lingual word embeddings (CLWE) have been proven useful in many cross-lingual tasks. However, most existing approaches to learn CLWE including the ones with contextual embeddings are sense agnostic. In this work, we propose a novel framework to align contextual embeddings at the sense level by leveraging cross-lingual signal from bilingual dictionaries only. We operationalize our framework by first proposing a novel sense-aware cross entropy loss to model word senses explicitly. The monolingual ELMo and BERT models pretrained with our sense-aware cross entropy loss demonstrate significant performance improvement for word sense disambiguation tasks. We then propose a sense alignment objective on top of the sense-aware cross entropy loss for cross-lingual model pretraining, and pretrain cross-lingual models for several language pairs (English to German/Spanish/Japanese/Chinese). Compared with the best baseline results, our cross-lingual models achieve 0.52%, 2.09% and 1.29% average performance improvements on zero-shot cross-lingual NER, sentiment classification and XNLI tasks, respectively.",
    "citations": 6
  },
  {
    "title": "Meemi: A Simple Method for Post-processing Cross-lingual Word Embeddings",
    "year": 2019,
    "authors": "Yerai Doval, Jos\u00e9 Camacho-Collados, Luis Espinosa Anke, S. Schockaert",
    "url": "https://api.semanticscholar.org/CorpusId:204824077",
    "relevance": 3,
    "abstract": "Word embeddings have become a standard resource in the toolset of any Natural Language Processing practitioner. While monolingual word embeddings encode information about words in the context of a particular language, cross-lingual embeddings define a multilingual space where word embeddings from two or more languages are integrated together. Current state-of-the-art approaches learn these embeddings by aligning two disjoint monolingual vector spaces through an orthogonal transformation which preserves the structure of the monolingual counterparts. In this work, we propose to apply an additional transformation after this initial alignment step, which aims to bring the vector representations of a given word and its translations closer to their average. Since this additional transformation is non-orthogonal, it also affects the structure of the monolingual spaces. We show that our approach both improves the integration of the monolingual spaces as well as the quality of the monolingual spaces themselves. Furthermore, because our transformation can be applied to an arbitrary number of languages, we are able to effectively obtain a truly multilingual space. The resulting (monolingual and multilingual) spaces show consistent gains over the current state-of-the-art in standard intrinsic tasks, namely dictionary induction and word similarity, as well as in extrinsic tasks such as cross-lingual hypernym discovery and cross-lingual natural language inference.",
    "citations": 8
  },
  {
    "title": "Multilingual Model Using Cross-Task Embedding Projection",
    "year": 2019,
    "authors": "Jin Sakuma, Naoki Yoshinaga",
    "url": "https://api.semanticscholar.org/CorpusId:196172425",
    "relevance": 3,
    "abstract": "We present a method for applying a neural network trained on one (resource-rich) language for a given task to other (resource-poor) languages. We accomplish this by inducing a mapping from pre-trained cross-lingual word embeddings to the embedding layer of the neural network trained on the resource-rich language. To perform element-wise cross-task embedding projection, we invent locally linear mapping which assumes and preserves the local topology across the semantic spaces before and after the projection. Experimental results on topic classification task and sentiment analysis task showed that the fully task-specific multilingual model obtained using our method outperformed the existing multilingual models with embedding layers fixed to pre-trained cross-lingual word embeddings.",
    "citations": 2
  },
  {
    "title": "A Multi-task Approach to Learning Multilingual Representations",
    "year": 2018,
    "authors": "Karan Singla, Dogan Can, Shrikanth S. Narayanan",
    "url": "https://api.semanticscholar.org/CorpusId:51873570",
    "relevance": 3,
    "abstract": "We present a novel multi-task modeling approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a limited resource scenario.",
    "citations": 21
  },
  {
    "title": "Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora",
    "year": 2020,
    "authors": "Takashi Wada, Tomoharu Iwata, Yuji Matsumoto, Timothy Baldwin, Jey Han Lau",
    "url": "https://api.semanticscholar.org/CorpusId:225094299",
    "relevance": 3,
    "abstract": "We propose a new approach for learning contextualised cross-lingual word embeddings based on a small parallel corpus (e.g. a few hundred sentence pairs). Our method obtains word embeddings via an LSTM encoder-decoder model that simultaneously translates and reconstructs an input sentence. Through sharing model parameters among different languages, our model jointly trains the word embeddings in a common cross-lingual space. We also propose to combine word and subword embeddings to make use of orthographic similarities across different languages. We base our experiments on real-world data from endangered languages, namely Yongning Na, Shipibo-Konibo, and Griko. Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs. These results demonstrate that, contrary to common belief, an encoder-decoder translation model is beneficial for learning cross-lingual representations even in extremely low-resource conditions. Furthermore, our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.",
    "citations": 8
  },
  {
    "title": "Cross-Lingual Word Embeddings and the Structure of the Human Bilingual Lexicon",
    "year": 2019,
    "authors": "Paola Merlo, M. A. Rodriguez",
    "url": "https://api.semanticscholar.org/CorpusId:208253752",
    "relevance": 3,
    "abstract": "Research on the bilingual lexicon has uncovered fascinating interactions between the lexicons of the native language and of the second language in bilingual speakers. In particular, it has been found that the lexicon of the underlying native language affects the organisation of the second language. In the spirit of interpreting current distributed representations, this paper investigates two models of cross-lingual word embeddings, comparing them to the shared-translation effect and the cross-lingual coactivation effects of false and true friends (cognates) found in humans. We find that the similarity structure of the cross-lingual word embeddings space yields the same effects as the human bilingual lexicon.",
    "citations": 7
  },
  {
    "title": "Multilingual Factor Analysis",
    "year": 2019,
    "authors": "Francisco Vargas, Kamen Brestnichki, Alexandros Papadopoulos-Korfiatis, Nils Y. Hammerla",
    "url": "https://api.semanticscholar.org/CorpusId:153311735",
    "relevance": 3,
    "abstract": "In this work we approach the task of learning multilingual word representations in an offline manner by fitting a generative latent variable model to a multilingual dictionary. We model equivalent words in different languages as different views of the same word generated by a common latent variable representing their latent lexical meaning. We explore the task of alignment by querying the fitted model for multilingual embeddings achieving competitive results across a variety of tasks. The proposed model is robust to noise in the embedding space making it a suitable method for distributed representations learned from noisy corpora.",
    "citations": 1
  },
  {
    "title": "Lost in Alignment: A Survey on Cross-Lingual Alignment Methods for Contextualized Representation",
    "year": 2025,
    "authors": "Filippo Pallucchini, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica",
    "url": "https://www.semanticscholar.org/paper/13955b06b63f23837fe5963b9b28beb0660d2cb1",
    "relevance": 3,
    "abstract": "Cross-lingual word representations allow us to analyse word meanings across diverse language settings. It is crucial in aiding cross-lingual knowledge transfer when constructing natural language processing (NLP) models for languages with limited resources. This survey presents a comprehensive classification of cross-lingual contextual embedding models. We assess their data requirements and objective functions, and we introduce a taxonomy for categorising these approaches. Then, we present a comprehensive table containing a set of hierarchical criteria to compare them better, along with information regarding the availability of code and data to enable replication of the research. Furthermore, we delve into the evaluation methodologies employed for cross-lingual embeddings, exploring their practical applications and addressing their current associated challenges.",
    "citations": 2
  },
  {
    "title": "GlobalTrait: Personality Alignment of Multilingual Word Embeddings",
    "year": 2018,
    "authors": "Farhad Bin Siddique, D. Bertero, Pascale Fung",
    "url": "https://api.semanticscholar.org/CorpusId:53165920",
    "relevance": 3,
    "abstract": "We propose a multilingual model to recognize Big Five Personality traits from text data in four different languages: English, Spanish, Dutch and Italian. Our analysis shows that words having a similar semantic meaning in different languages do not necessarily correspond to the same personality traits. Therefore, we propose a personality alignment method, GlobalTrait, which has a mapping for each trait from the source language to the target language (English), such that words that correlate positively to each trait are close together in the multilingual vector space. Using these aligned embeddings for training, we can transfer personality related training features from high-resource languages such as English to other low-resource languages, and get better multilingual results, when compared to using simple monolingual and unaligned multilingual embeddings. We achieve an average F-score increase (across all three languages except English) from 65 to 73.4 (+8.4), when comparing our monolingual model to multilingual using CNN with personality aligned embeddings. We also show relatively good performance in the regression tasks, and better classification results when evaluating our model on a separate Chinese dataset.",
    "citations": 5
  },
  {
    "title": "WEWD: A Combined Approach for Measuring Cross-lingual Semantic Word Similarity Based on Word Embeddings and Word Definitions",
    "year": 2021,
    "authors": "Van-Tan Bui, Phuong-Thai Nguyen",
    "url": "https://www.semanticscholar.org/paper/4eac90c00659bd46355f77d141428b72636da74d",
    "relevance": 3,
    "abstract": "Cross-lingual semantic word similarity (CLSW) ad- dresses the task of estimating the semantic distance between two words across languages. This task is an important component in many natural language processing applications. Recent studies have proposed several effective CLSW models for resource- rich language pairs such as English-German, English-French. However, This task has not been effectively addressed for language pairs consisting of Vietnamese and another one. In this paper, we propose a neural network model that exploits cross- lingual lexical resources to learn high-quality cross-lingual word embedding models. Since our neural network model is language- independent, it can learn a truly multilingual space. Furthermore, we introduce a novel cross-lingual semantic word similarity measurement method based on Word Embeddings and Word Definitions (WEWD). Last but not least, we introduce a standard Vietnamese-English dataset for the cross-lingual semantic word similarity measurement task (VESim-1000). The experimental results show that our proposed method is more robust and outperforms current state-of-the-art methods that are only based on word embeddings or lexical resources.",
    "citations": 0
  },
  {
    "title": "Detecting Cross-Lingual Plagiarism Using Simulated Word Embeddings",
    "year": 2017,
    "authors": "V. Thompson, C. Bowerman",
    "url": "https://api.semanticscholar.org/CorpusId:25834866",
    "relevance": 3,
    "abstract": "Cross-lingual plagiarism (CLP) occurs when texts written in one language are translated into a different language and used without acknowledgement. One of the most common methods used for detecting CLP requires online machine translators (such as Google or Microsoft translate) which are not always available, and given that plagiarism detection typically involves large document comparison, the amount of translations would overwhelm an online machine translator, especially when detecting plagiarism over the web. In addition, when translated texts are replaced with their synonyms, using online machine translators to detect CLP would result in poor performance. This paper addresses the problem of cross-lingual plagiarism detection (CLPD) by proposing a model that uses simulated word embeddings to reproduce the predictions of an online machine translator (Google translate) when detecting CLP without relying on online translators. The simulated embeddings comprise of translated words in different languages mapped in a common space, and replicated to increase the prediction probability of retrieving the translations of a word (and their synonyms) from the model. Unlike most existing models, the proposed model does not require parallel corpora, and accommodates multiple languages (multi-lingual). We demonstrated the effectiveness of the proposed model in detecting CLP in standard datasets that contain CLP cases, and evaluated its performance against a state-of-the-art baseline that relies on online machine translator (T+MA model). Evaluation results revealed that the proposed model is not only effective in detecting CLP, it outperformed the baseline. The results indicate that CLP could be detected with state-of-the-art performances by leveraging the prediction accuracy of an internet translator with word embeddings without relying on internet translators.",
    "citations": 3
  },
  {
    "title": "How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions",
    "year": 2019,
    "authors": "Goran Glavas, Robert Litschko, Sebastian Ruder, Ivan Vulic",
    "url": "https://api.semanticscholar.org/CorpusId:59553563",
    "relevance": 2,
    "abstract": "Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models: we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis.",
    "citations": 188
  },
  {
    "title": "Cross-lingual alignments of ELMo contextual embeddings",
    "year": 2021,
    "authors": "Matej Ul\u010dar, Marko Robnik-Sikonja",
    "url": "https://api.semanticscholar.org/CorpusId:235683108",
    "relevance": 2,
    "abstract": "Building machine learning prediction models for a specific natural language processing (NLP) task requires sufficient training data, which can be difficult to obtain for less-resourced languages. Cross-lingual embeddings map word embeddings from a less-resourced language to a resource-rich language so that a prediction model trained on data from the resource-rich language can also be used in the less-resourced language. To produce cross-lingual mappings of recent contextual embeddings, anchor points between the embedding spaces have to be words in the same context. We address this issue with a novel method for creating cross-lingual contextual alignment datasets. Based on that, we propose several cross-lingual mapping methods for ELMo embeddings. The proposed linear mapping methods use existing Vecmap and MUSE alignments on contextual ELMo embeddings. Novel nonlinear ELMoGAN mapping methods are based on generative adversarial networks (GANs) and do not assume isomorphic embedding spaces. We evaluate the proposed mapping methods on nine languages, using four downstream tasks: named entity recognition (NER), dependency parsing (DP), terminology alignment, and sentiment analysis. The ELMoGAN methods perform very well on the NER and terminology alignment tasks, with a lower cross-lingual loss for NER compared to the direct training on some languages. In DP and sentiment analysis, linear contextual alignment variants are more successful.",
    "citations": 22
  },
  {
    "title": "Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?",
    "year": 2019,
    "authors": "Ivan Vulic, Goran Glavas, Roi Reichart, A. Korhonen",
    "url": "https://api.semanticscholar.org/CorpusId:202539655",
    "relevance": 2,
    "abstract": "Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any supervision makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resource-poor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.",
    "citations": 92
  },
  {
    "title": "ArbEngVec : Arabic-English Cross-Lingual Word Embedding Model",
    "year": 2019,
    "authors": "Raki Lachraf, El Moatez Billah Nagoudi, Youcef Ayachi, Ahmed Abdelali, D. Schwab",
    "url": "https://api.semanticscholar.org/CorpusId:196174722",
    "relevance": 2,
    "abstract": "Word Embeddings (WE) are getting increasingly popular and widely applied in many Natural Language Processing (NLP) applications due to their effectiveness in capturing semantic properties of words; Machine Translation (MT), Information Retrieval (IR) and Information Extraction (IE) are among such areas. In this paper, we propose an open source ArbEngVec which provides several Arabic-English cross-lingual word embedding models. To train our bilingual models, we use a large dataset with more than 93 million pairs of Arabic-English parallel sentences. In addition, we perform both extrinsic and intrinsic evaluations for the different word embedding model variants. The extrinsic evaluation assesses the performance of models on the cross-language Semantic Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task.",
    "citations": 18
  },
  {
    "title": "Robust Unsupervised Cross-Lingual Word Embedding using Domain Flow Interpolation",
    "year": 2022,
    "authors": "Liping Tang, Zhen Li, Zhiquan Luo, H. Meng",
    "url": "https://www.semanticscholar.org/paper/db9cb3749585d7a735540f7b2c15130cd12368bd",
    "relevance": 2,
    "abstract": "This paper investigates an unsupervised approach towards deriving a universal, cross-lingual word embedding space, where words with similar semantics from different languages are close to one another. Previous adversarial approaches have shown promising results in inducing cross-lingual word embedding without parallel data. However, the training stage shows instability for distant language pairs. Instead of mapping the source language space directly to the target language space, we propose to make use of a sequence of intermediate spaces for smooth bridging. Each intermediate space may be conceived as a pseudo-language space and is introduced via simple linear interpolation. This approach is modeled after domain flow in computer vision, but with a modified objective function. Experiments on intrinsic Bilingual Dictionary Induction tasks show that the proposed approach can improve the robustness of adversarial models with comparable and even better precision. Further experiments on the downstream task of Cross-Lingual Natural Language Inference show that the proposed model achieves significant performance improvement for distant language pairs in downstream tasks compared to state-of-the-art adversarial and non-adversarial models.",
    "citations": 0
  },
  {
    "title": "CLUSE: Cross-Lingual Unsupervised Sense Embeddings",
    "year": 2018,
    "authors": "Ta-Chung Chi, Yun-Nung (Vivian) Chen",
    "url": "https://api.semanticscholar.org/CorpusId:52289318",
    "relevance": 3,
    "abstract": "This paper proposes a modularized sense induction and representation learning model that jointly learns bilingual sense embeddings that align well in the vector space, where the cross-lingual signal in the English-Chinese parallel corpus is exploited to capture the collocation and distributed characteristics in the language pair. The model is evaluated on the Stanford Contextual Word Similarity (SCWS) dataset to ensure the quality of monolingual sense embeddings. In addition, we introduce Bilingual Contextual Word Similarity (BCWS), a large and high-quality dataset for evaluating cross-lingual sense embeddings, which is the first attempt of measuring whether the learned embeddings are indeed aligned well in the vector space. The proposed approach shows the superior quality of sense embeddings evaluated in both monolingual and bilingual spaces.",
    "citations": 6
  },
  {
    "title": "Cross-Lingual Word Embeddings for Morphologically Rich Languages",
    "year": 2019,
    "authors": "A. Ustun, G. Bouma, Gertjan van Noord",
    "url": "https://www.semanticscholar.org/paper/9579d953d3eced070210870b52549ddc950cccd2",
    "relevance": 2,
    "abstract": "Cross-lingual word embedding models learn a shared vector space for two or more languages so that words with similar meaning are represented by similar vectors regardless of their language. Although the existing models achieve high performance on pairs of morphologically simple languages, they perform very poorly on morphologically rich languages such as Turkish and Finnish. In this paper, we propose a morpheme-based model in order to increase the performance of cross-lingual word embeddings on morphologically rich languages. Our model includes a simple extension which enables us to exploit morphemes for cross-lingual mapping. We applied our model for the Turkish-Finnish language pair on the bilingual word translation task. Results show that our model outperforms the baseline models by 2% in the nearest neighbour ranking.",
    "citations": 2
  },
  {
    "title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison",
    "year": 2016,
    "authors": "Shyam Upadhyay, Manaal Faruqui, Chris Dyer, Dan Roth",
    "url": "https://api.semanticscholar.org/CorpusId:5357629",
    "relevance": 2,
    "abstract": "Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.",
    "citations": 189
  },
  {
    "title": "Bilingual Word Embeddings from Non-Parallel Document-Aligned Data Applied to Bilingual Lexicon Induction",
    "year": 2015,
    "authors": "Ivan Vulic, Marie-Francine Moens",
    "url": "https://api.semanticscholar.org/CorpusId:14183678",
    "relevance": 2,
    "abstract": "We propose a simple yet effective approach to learning bilingual word embeddings (BWEs) from non-parallel document-aligned data (based on the omnipresent skip-gram model), and its application to bilingual lexicon induction (BLI). We demonstrate the utility of the induced BWEs in the BLI task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models significantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training.",
    "citations": 136
  },
  {
    "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
    "year": 2020,
    "authors": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka, Eneko Agirre",
    "url": "https://api.semanticscholar.org/CorpusId:216914383",
    "relevance": 2,
    "abstract": "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.",
    "citations": 74
  },
  {
    "title": "Bilingual Distributed Word Representations from Document-Aligned Comparable Data",
    "year": 2015,
    "authors": "Ivan Vulic, Marie-Francine Moens",
    "url": "https://api.semanticscholar.org/CorpusId:16452496",
    "relevance": 2,
    "abstract": "We propose a new model for learning bilingual word representations from nonparallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.",
    "citations": 106
  },
  {
    "title": "Learning Bilingual Word Embedding Mappings with Similar Words in Related Languages Using GAN",
    "year": 2022,
    "authors": "Ghafour Alipour, J. B. Mohasefi, M. Feizi-Derakhshi",
    "url": "https://api.semanticscholar.org/CorpusId:246688573",
    "relevance": 2,
    "abstract": "ABSTRACT Cross-lingual word embeddings display words from different languages in the same vector space. They provide reasoning about semantics, compare the meaning of words across languages and word meaning in multilingual contexts, necessary to bilingual lexicon induction, machine translation, and cross-lingual information retrieval. This paper proposes an efficient approach to learn bilingual transform mapping between monolingual word embeddings in language pairs. We choose ten different languages from three different language families and downloaded their last update Wikipedia dumps1 1. https://dumps.wikimedia.org. Then, with some pre-processing steps and using word2vec, we produce word embeddings for them. We select seven language pairs from chosen languages. Since the selected languages are relative, they have thousands of identical words with similar meanings. With these identical dictation words and word embedding models of each language, we create training, validation and, test sets for the language pairs. We then use a generative adversarial network (GAN) to learn the transform mapping between word embeddings of source and target languages. The average accuracy of our proposed method in all language pairs is 71.34%. The highest accuracy is achieved for the Turkish-Azerbaijani language pair with the accuracy 78.32%., which is noticeably higher than prior methods.",
    "citations": 11
  },
  {
    "title": "Improving Cross-Lingual Word Embeddings by Meeting in the Middle",
    "year": 2018,
    "authors": "Yerai Doval, Jos\u00e9 Camacho-Collados, Luis Espinosa Anke, S. Schockaert",
    "url": "https://api.semanticscholar.org/CorpusId:52094910",
    "relevance": 2,
    "abstract": "Cross-lingual word embeddings are becoming increasingly important in multilingual NLP. Recently, it has been shown that these embeddings can be effectively learned by aligning two disjoint monolingual vector spaces through linear transformations, using no more than a small bilingual dictionary as supervision. In this work, we propose to apply an additional transformation after the initial alignment step, which moves cross-lingual synonyms towards a middle point between them. By applying this transformation our aim is to obtain a better cross-lingual integration of the vector spaces. In addition, and perhaps surprisingly, the monolingual spaces also improve by this transformation. This is in contrast to the original alignment, which is typically learned such that the structure of the monolingual spaces is preserved. Our experiments confirm that the resulting cross-lingual embeddings outperform state-of-the-art models in both monolingual and cross-lingual evaluation tasks.",
    "citations": 62
  },
  {
    "title": "LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction through Non-Linear Mapping in Latent Space",
    "year": 2020,
    "authors": "Tasnim Mohiuddin, M Saiful Bari, Shafiq R. Joty",
    "url": "https://api.semanticscholar.org/CorpusId:216641774",
    "relevance": 2,
    "abstract": "Most of the successful and predominant methods for bilingual lexicon induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e., approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages. In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses nonlinear mapping in the latent space of two independently trained auto-encoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping.",
    "citations": 52
  },
  {
    "title": "On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning",
    "year": 2019,
    "authors": "Yerai Doval, Jos\u00e9 Camacho-Collados, Luis Espinosa Anke, S. Schockaert",
    "url": "https://api.semanticscholar.org/CorpusId:201124803",
    "relevance": 2,
    "abstract": "Cross-lingual word embeddings are vector representations of words in different languages where words with similar meaning are represented by similar vectors, regardless of the language. Recent developments which construct these embeddings by aligning monolingual spaces have shown that accurate alignments can be obtained with little or no supervision, which usually comes in the form of bilingual dictionaries. However, the focus has been on a particular controlled scenario for evaluation, and there is no strong evidence on how current state-of-the-art systems would fare with noisy text or for language pairs with major linguistic differences. In this paper we present an extensive evaluation over multiple cross-lingual embedding models, analyzing their strengths and limitations with respect to different variables such as target language, training corpora and amount of supervision. Our conclusions put in doubt the view that high-quality cross-lingual embeddings can always be learned without much supervision.",
    "citations": 21
  },
  {
    "title": "A survey of neural-network-based methods utilising comparable data for finding translation equivalents",
    "year": 2024,
    "authors": "Michaela Denisov'a, Pavel Rychl'y",
    "url": "https://api.semanticscholar.org/CorpusId:273502039",
    "relevance": 2,
    "abstract": "The importance of inducing bilingual dictionary components in many natural language processing (NLP) applications is indisputable. However, the dictionary compilation process requires extensive work and combines two disciplines, NLP and lexicography, while the former often omits the latter. In this paper, we present the most common approaches from NLP that endeavour to automatically induce one of the essential dictionary components, translation equivalents and focus on the neural-network-based methods using comparable data. We analyse them from a lexicographic perspective since their viewpoints are crucial for improving the described methods. Moreover, we identify the methods that integrate these viewpoints and can be further exploited in various applications that require them. This survey encourages a connection between the NLP and lexicography fields as the NLP field can benefit from lexicographic insights, and it serves as a helping and inspiring material for further research in the context of neural-network-based methods utilising comparable data.",
    "citations": 0
  },
  {
    "title": "Quantized Wasserstein Procrustes Alignment of Word Embedding Spaces",
    "year": 2022,
    "authors": "P. Aboagye, Yan-luan Zheng, Michael Yeh, Junpeng Wang, Zhongfang Zhuang, Huiyuan Chen, Liang Wang, Wei Zhang, J. M. Phillips",
    "url": "https://api.semanticscholar.org/CorpusId:252186399",
    "relevance": 2,
    "abstract": "Motivated by the widespread interest in the cross-lingual transfer of NLP models from high resource to low resource languages, research on Cross-lingual word embeddings (CLWEs) has gained much popularity over the years. Among the most successful and attractive CLWE models are the unsupervised CLWE models. These unsupervised CLWE models pose the alignment task as a Wasserstein-Procrustes problem aiming to estimate a permutation matrix and an orthogonal matrix jointly. Most existing unsupervised CLWE models resort to Optimal Transport (OT) based methods to estimate the permutation matrix. However, linear programming algorithms and approximate OT solvers via Sinkhorn for computing the permutation matrix scale cubically and quadratically, respectively, in the input size. This makes it impractical and infeasible to compute OT distances exactly for larger sample size, resulting in a poor approximation quality of the permutation matrix and subsequently a less robust learned transfer function or mapper. This paper proposes an unsupervised projection-based CLWE model called quantized Wasserstein Procrustes (qWP) that jointly estimates a permutation matrix and an orthogonal matrix. qWP relies on a quantization step to estimate the permutation matrix between two probability distributions or measures. This approach substantially improves the approximation quality of empirical OT solvers given fixed computational cost. We demonstrate that qWP achieves state-of-the-art results on the Bilingual lexicon Induction (BLI) task.",
    "citations": 6
  },
  {
    "title": "Density Matching for Bilingual Word Embedding",
    "year": 2019,
    "authors": "Chunting Zhou, Xuezhe Ma, Di Wang, Graham Neubig",
    "url": "https://api.semanticscholar.org/CorpusId:102354931",
    "relevance": 2,
    "abstract": "Recent approaches to cross-lingual word embedding have generally been based on linear transformations between the sets of embedding vectors in the two languages. In this paper, we propose an approach that instead expresses the two monolingual embedding spaces as probability densities defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow. The method requires no explicit supervision, and can be learned with only a seed dictionary of words that have identical strings. We argue that this formulation has several intuitively attractive properties, particularly with the respect to improving robustness and generalization to mappings between difficult language pairs or word pairs. On a benchmark data set of bilingual lexicon induction and cross-lingual word similarity, our approach can achieve competitive or superior performance compared to state-of-the-art published results, with particularly strong results being found on etymologically distant and/or morphologically rich languages.",
    "citations": 40
  },
  {
    "title": "English-Malay Cross-Lingual Embedding Alignment using Bilingual Lexicon Augmentation",
    "year": 2022,
    "authors": "Ying Hao Lim, Jasy Suet Yan Liew",
    "url": "https://api.semanticscholar.org/CorpusId:248780179",
    "relevance": 2,
    "abstract": "As high-quality Malay language resources are still a scarcity, cross lingual word embeddings make it possible for richer English resources to be leveraged for downstream Malay text classification tasks. This paper focuses on creating an English-Malay cross-lingual word embeddings using embedding alignment by exploiting existing language resources. We augmented the training bilingual lexicons using machine translation with the goal to improve the alignment precision of our cross-lingual word embeddings. We investigated the quality of the current state-of-the-art English-Malay bilingual lexicon and worked on improving its quality using Google Translate. We also examined the effect of Malay word coverage on the quality of cross-lingual word embeddings. Experimental results with a precision up till 28.17% show that the alignment precision of the cross-lingual word embeddings would inevitably degrade after 1-NN but a better seed lexicon and cleaner nearest neighbours can reduce the number of word pairs required to achieve satisfactory performance. As the English and Malay monolingual embeddings are pre-trained on informal language corpora, our proposed English-Malay embeddings alignment approach is also able to map non-standard Malay translations in the English nearest neighbours.",
    "citations": 0
  },
  {
    "title": "Semantic Recommendation System for Bilingual Corpus of Academic Papers",
    "year": 2021,
    "authors": "Anna Safaryan, Petr Filchenkov, Weijia Yan, Andrey Kutuzov, Irina Nikishina",
    "url": "https://api.semanticscholar.org/CorpusId:232336430",
    "relevance": 2,
    "abstract": "We tested four methods of making document representations cross-lingual for the task of semantic search for the similar papers based on the corpus of papers from three Russian conferences on NLP: Dialogue, AIST and AINL. The pipeline consisted of three stages: preprocessing, word-by-word vectorisation using models obtained with various methods to map vectors from two independent vector spaces to a common one, and search for the most similar papers based on the cosine similarity of text vectors. The four methods used can be grouped into two approaches: 1) aligning two pretrained monolingual word embedding models with a bilingual dictionary on our own (for example, with the VecMap algorithm) and 2) using pre-aligned cross-lingual word embedding models (MUSE). To find out, which approach brings more benefit to the task, we conducted a manual evaluation of the results and calculated the average precision of recommendations for all the methods mentioned above. MUSE turned out to have the highest search relevance, but the other methods produced more recommendations in a language other than the one of the target paper.",
    "citations": 2
  },
  {
    "title": "Unsupervised Evaluation of Human Translation Quality",
    "year": 2019,
    "authors": "Yi Zhou, Danushka Bollegala",
    "url": "https://api.semanticscholar.org/CorpusId:204754744",
    "relevance": 2,
    "abstract": "Even though machine translation (MT) systems have reached impressive performances in cross-lingual translation tasks, the quality of MT is still far behind professional human translations (HTs) due to the complexity in natural languages, especially for terminologies in different domains. Therefore, HTs are still widely demanded in practice. However, the quality of HT is also imperfect and vary significantly depending on the experience and knowledge of the translators. Evaluating the quality of HT in an automatic manner has faced many challenges. Although bilingual speakers are able to assess the translation quality, manually checking the accuracy of translations is expensive and time-consuming. In this paper, we propose an unsupervised method to evaluate the quality of HT without requiring any labelled data. We compare a range of methods for automatically grading HTs and observe the Bidirectional Minimum Word Mover\u2019s distance (BiMWMD) to produce gradings that correlate well with humans.",
    "citations": 5
  },
  {
    "title": "A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity",
    "year": 2019,
    "authors": "Yoshinari Fujinuma, Jordan L. Boyd-Graber, Michael J. Paul",
    "url": "https://api.semanticscholar.org/CorpusId:174799386",
    "relevance": 2,
    "abstract": "Cross-lingual word embeddings encode the meaning of words from different languages into a shared low-dimensional space. An important requirement for many downstream tasks is that word similarity should be independent of language\u2014i.e., word vectors within one language should not be more similar to each other than to words in another language. We measure this characteristic using modularity, a network measurement that measures the strength of clusters in a graph. Modularity has a moderate to strong correlation with three downstream tasks, even though modularity is based only on the structure of embeddings and does not require any external resources. We show through experiments that modularity can serve as an intrinsic validation metric to improve unsupervised cross-lingual word embeddings, particularly on distant language pairs in low-resource settings.",
    "citations": 28
  },
  {
    "title": "BCWS: Bilingual Contextual Word Similarity",
    "year": 2018,
    "authors": "Ta-Chung Chi, Ching-Yen Shih, Yun-Nung (Vivian) Chen",
    "url": "https://api.semanticscholar.org/CorpusId:53047117",
    "relevance": 2,
    "abstract": "This paper introduces the first dataset for evaluating English-Chinese Bilingual Contextual Word Similarity, namely BCWS (this https URL). The dataset consists of 2,091 English-Chinese word pairs with the corresponding sentential contexts and their similarity scores annotated by the human. Our annotated dataset has higher consistency compared to other similar datasets. We establish several baselines for the bilingual embedding task to benchmark the experiments. Modeling cross-lingual sense representations as provided in this dataset has the potential of moving artificial intelligence from monolingual understanding towards multilingual understanding.",
    "citations": 3
  },
  {
    "title": "Learning to Represent Bilingual Dictionaries",
    "year": 2018,
    "authors": "Muhao Chen, Yingtao Tian, Haochen Chen, Kai-Wei Chang, S. Skiena, C. Zaniolo",
    "url": "https://api.semanticscholar.org/CorpusId:51984213",
    "relevance": 2,
    "abstract": "Bilingual word embeddings have been widely used to capture the correspondence of lexical semantics in different human languages. However, the cross-lingual correspondence between sentences and words is less studied, despite that this correspondence can significantly benefit many applications such as crosslingual semantic search and textual inference. To bridge this gap, we propose a neural embedding model that leverages bilingual dictionaries. The proposed model is trained to map the lexical definitions to the cross-lingual target words, for which we explore with different sentence encoding techniques. To enhance the learning process on limited resources, our model adopts several critical learning strategies, including multi-task learning on different bridges of languages, and joint learning of the dictionary model with a bilingual word embedding model. We conduct experiments on two new tasks. In the cross-lingual reverse dictionary retrieval task, we demonstrate that our model is capable of comprehending bilingual concepts based on descriptions, and the proposed learning strategies are effective. In the bilingual paraphrase identification task, we show that our model effectively associates sentences in different languages via a shared embedding space, and outperforms existing approaches in identifying bilingual paraphrases.",
    "citations": 13
  },
  {
    "title": "Cross-Lingual Contextual Word Embeddings Mapping With Multi-Sense Words In Mind",
    "year": 2019,
    "authors": "Zheng Zhang, Ruiqing Yin, Jun Zhu, Pierre Zweigenbaum",
    "url": "https://api.semanticscholar.org/CorpusId:202677272",
    "relevance": 2,
    "abstract": "Recent work in cross-lingual contextual word embedding learning cannot handle multi-sense words well. In this work, we explore the characteristics of contextual word embeddings and show the link between contextual word embeddings and word senses. We propose two improving solutions by considering contextual multi-sense word embeddings as noise (removal) and by generating cluster level average anchor embeddings for contextual multi-sense word embeddings (replacement). Experiments show that our solutions can improve the supervised contextual word embeddings alignment for multi-sense words in a microscopic perspective without hurting the macroscopic performance on the bilingual lexicon induction task. For unsupervised alignment, our methods significantly improve the performance on the bilingual lexicon induction task for more than 10 points.",
    "citations": 4
  },
  {
    "title": "Expanding the Text Classification Toolbox with Cross-Lingual Embeddings",
    "year": 2019,
    "authors": "Meryem M'hamdi, Robert West, Andreea Hossmann, Michael Baeriswyl, C. Musat",
    "url": "https://api.semanticscholar.org/CorpusId:85502789",
    "relevance": 2,
    "abstract": "Most work in text classification and Natural Language Processing (NLP) focuses on English or a handful of other languages that have text corpora of hundreds of millions of words. This is creating a new version of the digital divide: the artificial intelligence (AI) divide. Transfer-based approaches, such as Cross-Lingual Text Classification (CLTC) - the task of categorizing texts written in different languages into a common taxonomy, are a promising solution to the emerging AI divide. Recent work on CLTC has focused on demonstrating the benefits of using bilingual word embeddings as features, relegating the CLTC problem to a mere benchmark based on a simple averaged perceptron. \nIn this paper, we explore more extensively and systematically two flavors of the CLTC problem: news topic classification and textual churn intent detection (TCID) in social media. In particular, we test the hypothesis that embeddings with context are more effective, by multi-tasking the learning of multilingual word embeddings and text classification; we explore neural architectures for CLTC; and we move from bi- to multi-lingual word embeddings. For all architectures, types of word embeddings and datasets, we notice a consistent gain trend in favor of multilingual joint training, especially for low-resourced languages.",
    "citations": 1
  },
  {
    "title": "Cross-Lingual Word Embedding Refinement by \\ell_{1} Norm Optimisation",
    "year": 2021,
    "authors": "Xu Lin, Mark Stevenson",
    "url": "https://www.semanticscholar.org/paper/17fa1a5a4b12eb312a6f3f069e013d644708e437",
    "relevance": 2,
    "abstract": "Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the \u21132 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. \u21131 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the \u21131 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this strategy be adopted as a standard for CLWE methods.",
    "citations": 14
  },
  {
    "title": "Refinement of Unsupervised Cross-Lingual Word Embeddings",
    "year": 2020,
    "authors": "Magdalena Biesialska, M. Costa-juss\u00e0",
    "url": "https://www.semanticscholar.org/paper/3356b149591fc6f0d206f94cf585236f8e1d3736",
    "relevance": 2,
    "abstract": "Cross-lingual word embeddings aim to bridge the gap between high-resource and low-resource languages by allowing to learn multilingual word representations even without using any direct bilingual signal. The lion's share of the methods are projection-based approaches that map pre-trained embeddings into a shared latent space. These methods are mostly based on the orthogonal transformation, which assumes language vector spaces to be isomorphic. However, this criterion does not necessarily hold, especially for morphologically-rich languages. In this paper, we propose a self-supervised method to refine the alignment of unsupervised bilingual word embeddings. The proposed model moves vectors of words and their corresponding translations closer to each other as well as enforces length- and center-invariance, thus allowing to better align cross-lingual embeddings. The experimental results demonstrate the effectiveness of our approach, as in most cases it outperforms state-of-the-art methods in a bilingual lexicon induction task.",
    "citations": 4
  },
  {
    "title": "GLUECoS: An Evaluation Benchmark for Code-Switched NLP",
    "year": 2020,
    "authors": "Simran Khanuja, Sandipan Dandapat, A. Srinivasan, Sunayana Sitaram, M. Choudhury",
    "url": "https://api.semanticscholar.org/CorpusId:216553264",
    "relevance": 2,
    "abstract": "Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference. We present results on all these tasks using cross-lingual word embedding models and multilingual models. In addition, we fine-tune multilingual models on artificially generated code-switched data. Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.",
    "citations": 178
  },
  {
    "title": "Multilingual Training of Crosslingual Word Embeddings",
    "year": 2017,
    "authors": "H. Kanayama, Trevor Cohn, Tengfei Ma, Steven Bird, Long Duong",
    "url": "https://api.semanticscholar.org/CorpusId:17908320",
    "relevance": 2,
    "abstract": "Crosslingual word embeddings represent lexical items from different languages using the same vector space, enabling crosslingual transfer. Most prior work constructs embeddings for a pair of languages, with English on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space.In this way, we can exploit and combine strength of many languages. We obtained high performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks.",
    "citations": 48
  },
  {
    "title": "Cross-lingual Word Embeddings in Hyperbolic Space",
    "year": 2022,
    "authors": "Chandni Saxena, Mudit Chaudhary, H. Meng",
    "url": "https://api.semanticscholar.org/CorpusId:248512913",
    "relevance": 2,
    "abstract": "Cross-lingual word embeddings can be applied to several natural language processing applications across multiple languages. Unlike prior works that use word embeddings based on the Euclidean space, this short paper presents a simple and effective cross-lingual Word2Vec model that adapts to the Poincar\\'e ball model of hyperbolic space to learn unsupervised cross-lingual word representations from a German-English parallel corpus. It has been shown that hyperbolic embeddings can capture and preserve hierarchical relationships. We evaluate the model on both hypernymy and analogy tasks. The proposed model achieves comparable performance with the vanilla Word2Vec model on the cross-lingual analogy task, the hypernymy task shows that the cross-lingual Poincar\\'e Word2Vec model can capture latent hierarchical structure from free text across languages, which are absent from the Euclidean-based Word2Vec representations. Our results show that by preserving the latent hierarchical information, hyperbolic spaces can offer better representations for cross-lingual embeddings.",
    "citations": 4
  },
  {
    "title": "Bilingual Topic Models for Comparable Corpora",
    "year": 2021,
    "authors": "Georgios Balikas, Massih-Reza Amini, M. Clausel",
    "url": "https://api.semanticscholar.org/CorpusId:244729087",
    "relevance": 2,
    "abstract": "Probabilistic topic models like Latent Dirichlet Allocation (LDA) have been previously extended to the bilingual setting. A fundamental modeling assumption in several of these extensions is that the input corpora are in the form of document pairs whose constituent documents share a single topic distribution. However, this assumption is strong for comparable corpora that consist of documents thematically similar to an extent only, which are, in turn, the most commonly available or easy to obtain. In this paper we relax this assumption by proposing for the paired documents to have separate, yet bound topic distributions. % a binding mechanism between the distributions of the paired documents. We suggest that the strength of the bound should depend on each pair's semantic similarity. To estimate the similarity of documents that are written in different languages we use cross-lingual word embeddings that are learned with shallow neural networks. We evaluate the proposed binding mechanism by extending two topic models: a bilingual adaptation of LDA that assumes bag-of-words inputs and a model that incorporates part of the text structure in the form of boundaries of semantically coherent segments. To assess the performance of the novel topic models we conduct intrinsic and extrinsic experiments on five bilingual, comparable corpora of English documents with French, German, Italian, Spanish and Portuguese documents. The results demonstrate the efficiency of our approach in terms of both topic coherence measured by the normalized point-wise mutual information, and generalization performance measured by perplexity and in terms of Mean Reciprocal Rank in a cross-lingual document retrieval task for each of the language pairs.",
    "citations": 0
  },
  {
    "title": "What makes multilingual BERT multilingual?",
    "year": 2020,
    "authors": "Chi-Liang Liu, Tsung-Yuan Hsu, Yung-Sung Chuang, Hung-yi Lee",
    "url": "https://api.semanticscholar.org/CorpusId:224819080",
    "relevance": 2,
    "abstract": "Recently, multilingual BERT works remarkably well on cross-lingual transfer tasks, superior to static non-contextualized word embeddings. In this work, we provide an in-depth experimental study to supplement the existing literature of cross-lingual ability. We compare the cross-lingual ability of non-contextualized and contextualized representation model with the same data. We found that datasize and context window size are crucial factors to the transferability.",
    "citations": 6
  },
  {
    "title": "Inducing Crosslingual Distributed Representations of Words",
    "year": 2012,
    "authors": "A. Klementiev, Ivan Titov, Binod Bhattarai",
    "url": "https://www.semanticscholar.org/paper/d1f37d9cab68eb8cda669cc949394732f33264b4",
    "relevance": 1,
    "abstract": "",
    "citations": 390
  },
  {
    "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora",
    "year": 2016,
    "authors": "Long Duong, H. Kanayama, Tengfei Ma, Steven Bird, Trevor Cohn",
    "url": "https://api.semanticscholar.org/CorpusId:13888952",
    "relevance": 1,
    "abstract": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.",
    "citations": 116
  },
  {
    "title": "Data-Augmentation for Bangla-English Code-Mixed Sentiment Analysis: Enhancing Cross Linguistic Contextual Understanding",
    "year": 2023,
    "authors": "Mohammad Tareq, Md. Fokhrul Islam, Swakshar Deb, Sejuti Rahman, A. Mahmud",
    "url": "https://api.semanticscholar.org/CorpusId:258797890",
    "relevance": 1,
    "abstract": "In today\u2019s digital world, automated sentiment analysis from online reviews can contribute to a wide variety of decision-making processes. One example is examining typical perceptions of a product based on customer feedbacks to have a better understanding of consumer expectations, which can help enhance everything from customer service to product offerings. Online review comments, on the other hand, frequently mix different languages, use non-native scripts and do not adhere to strict grammar norms. For a low-resource language like Bangla, the lack of annotated code-mixed data makes automated sentiment analysis more challenging. To address this, we collect online reviews of different products and construct an annotated Bangla-English code mix (BE-CM) dataset (Dataset and other resources are available at https://github.com/fokhruli/CM-seti-anlysis). On our sentiment corpus, we also compare several alternative models from the existing literature. We present a simple but effective data augmentation method that can be utilized with existing word embedding algorithms without the need for a parallel corpus to improve cross-lingual contextual understanding. Our experimental results suggest that training word embedding models (e.g., Word2vec, FastText) with our data augmentation strategy can help the model in capturing the cross-lingual relationship for code-mixed sentences, thereby improving the overall performance of existing classifiers in both supervised learning and zero-shot cross-lingual adaptability. With extensive experimentations, we found that XGBoost with Fasttext embedding trained on our proposed data augmentation method outperforms other alternative models in automated sentiment analysis on code-mixed Bangla-English dataset, with a weighted F1 score of 87%.",
    "citations": 27
  },
  {
    "title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments",
    "year": 2016,
    "authors": "Anders S\u00f8gaard, Yoav Goldberg, Omer Levy",
    "url": "https://api.semanticscholar.org/CorpusId:16479424",
    "relevance": 1,
    "abstract": "While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account.",
    "citations": 66
  },
  {
    "title": "Word Embeddings for Code-Mixed Language Processing",
    "year": 2018,
    "authors": "Adithya Pratapa, M. Choudhury, Sunayana Sitaram",
    "url": "https://api.semanticscholar.org/CorpusId:53080549",
    "relevance": 1,
    "abstract": "We compare three existing bilingual word embedding approaches, and a novel approach of training skip-grams on synthetic code-mixed text generated through linguistic models of code-mixing, on two tasks - sentiment analysis and POS tagging for code-mixed text. Our results show that while CVM and CCA based embeddings perform as well as the proposed embedding technique on semantic and syntactic tasks respectively, the proposed approach provides the best performance for both tasks overall. Thus, this study demonstrates that existing bilingual embedding techniques are not ideal for code-mixed text processing and there is a need for learning multilingual word embedding from the code-mixed text.",
    "citations": 62
  },
  {
    "title": "Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping",
    "year": 2019,
    "authors": "Jian Ni, Radu Florian",
    "url": "https://api.semanticscholar.org/CorpusId:202769250",
    "relevance": 1,
    "abstract": "Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-of-the-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a resource-poor language. In this paper, we propose a new approach for cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language, so that a well-trained source-language neural network RE model can be directly applied to the target language. Experiment results show that the proposed approach achieves very good performance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs.",
    "citations": 27
  },
  {
    "title": "En-Ar Bilingual Word Embeddings without Word Alignment: Factors Effects",
    "year": 2019,
    "authors": "T. Alqaisi, Simon E. M. O'Keefe",
    "url": "https://api.semanticscholar.org/CorpusId:199379338",
    "relevance": 1,
    "abstract": "This paper introduces the first attempt to investigate morphological segmentation on En-Ar bilingual word embeddings using bilingual word embeddings model without word alignment (BilBOWA). We investigate the effect of sentence length and embedding size on the learning process. Our experiment shows that using the D3 segmentation scheme improves the accuracy of learning bilingual word embeddings up to 10 percentage points compared to the ATB and D0 schemes in all different training settings.",
    "citations": 8
  },
  {
    "title": "Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder",
    "year": 2019,
    "authors": "Yunsu Kim, Jiahui Geng, H. Ney",
    "url": "https://api.semanticscholar.org/CorpusId:52237660",
    "relevance": 1,
    "abstract": "Unsupervised learning of cross-lingual word embedding offers elegant matching of words across languages, but has fundamental limitations in translating sentences. In this paper, we propose simple yet effective methods to improve word-by-word translation of cross-lingual embeddings, using only monolingual corpora but without any back-translation. We integrate a language model for context-aware search, and use a novel denoising autoencoder to handle reordering. Our system surpasses state-of-the-art unsupervised translation systems without costly iterative training. We also analyze the effect of vocabulary size and denoising type on the translation performance, which provides better understanding of learning the cross-lingual word embedding and its usage in translation.",
    "citations": 43
  },
  {
    "title": "Unsupervised Cross-lingual Representation Learning at Scale",
    "year": 2019,
    "authors": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco (Paco) Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov",
    "url": "https://www.semanticscholar.org/paper/6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
    "relevance": 1,
    "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
    "citations": 7836
  },
  {
    "title": "Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability",
    "year": 2022,
    "authors": "Yoshinari Fujinuma, Jordan L. Boyd-Graber, Katharina Kann",
    "url": "https://api.semanticscholar.org/CorpusId:247594723",
    "relevance": 1,
    "abstract": "Pretrained multilingual models enable zero-shot learning even for unseen languages, and that performance can be further improved via adaptation prior to finetuning. However, it is unclear how the number of pretraining languages influences a model\u2019s zero-shot learning for languages unseen during pretraining. To fill this gap, we ask the following research questions: (1) How does the number of pretraining languages influence zero-shot performance on unseen target languages? (2) Does the answer to that question change with model adaptation? (3) Do the findings for our first question change if the languages used for pretraining are all related? Our experiments on pretraining with related languages indicate that choosing a diverse set of languages is crucial. Without model adaptation, surprisingly, increasing the number of pretraining languages yields better results up to adding related languages, after which performance plateaus.In contrast, with model adaptation via continued pretraining, pretraining on a larger number of languages often gives further improvement, suggesting that model adaptation is crucial to exploit additional pretraining languages.",
    "citations": 29
  },
  {
    "title": "An Empirical Study on Crosslingual Transfer in Probabilistic Topic Models",
    "year": 2018,
    "authors": "Shudong Hao, Michael J. Paul",
    "url": "https://api.semanticscholar.org/CorpusId:182953256",
    "relevance": 1,
    "abstract": "Probabilistic topic modeling is a common first step in crosslingual tasks to enable knowledge transfer and extract multilingual features. Although many multilingual topic models have been developed, their assumptions about the training corpus are quite varied, and it is not clear how well the different models can be utilized under various training conditions. In this article, the knowledge transfer mechanisms behind different multilingual topic models are systematically studied, and through a broad set of experiments with four models on ten languages, we provide empirical insights that can inform the selection and future development of multilingual topic models.",
    "citations": 8
  },
  {
    "title": "SensEmBERT: Context-Enhanced Sense Embeddings for Multilingual Word Sense Disambiguation",
    "year": 2020,
    "authors": "Bianca Scarlini, Tommaso Pasini, Roberto Navigli",
    "url": "https://api.semanticscholar.org/CorpusId:211628120",
    "relevance": 3,
    "abstract": "Contextual representations of words derived by neural language models have proven to effectively encode the subtle distinctions that might occur between different meanings of the same word. However, these representations are not tied to a semantic network, hence they leave the word meanings implicit and thereby neglect the information that can be derived from the knowledge base itself. In this paper, we propose SensEmBERT, a knowledge-based approach that brings together the expressive power of language modelling and the vast amount of knowledge contained in a semantic network to produce high-quality latent semantic representations of word meanings in multiple languages. Our vectors lie in a space comparable with that of contextualized word embeddings, thus allowing a word occurrence to be easily linked to its meaning by applying a simple nearest neighbour approach.We show that, whilst not relying on manual semantic annotations, SensEmBERT is able to either achieve or surpass state-of-the-art results attained by most of the supervised neural approaches on the English Word Sense Disambiguation task. When scaling to other languages, our representations prove to be equally effective as their English counterpart and outperform the existing state of the art on all the Word Sense Disambiguation multilingual datasets. The embeddings are released in five different languages at http://sensembert.org.",
    "citations": 121
  },
  {
    "title": "SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC)",
    "year": 2021,
    "authors": "Federico Martelli, N. Kalach, Gabriele Tola, Roberto Navigli",
    "url": "https://api.semanticscholar.org/CorpusId:234756243",
    "relevance": 3,
    "abstract": "In this paper, we introduce the first SemEval task on Multilingual and Cross-Lingual Word-in-Context disambiguation (MCL-WiC). This task allows the largely under-investigated inherent ability of systems to discriminate between word senses within and across languages to be evaluated, dropping the requirement of a fixed sense inventory. Framed as a binary classification, our task is divided into two parts. In the multilingual sub-task, participating systems are required to determine whether two target words, each occurring in a different context within the same language, express the same meaning or not. Instead, in the cross-lingual part, systems are asked to perform the task in a cross-lingual scenario, in which the two target words and their corresponding contexts are provided in two different languages. We illustrate our task, as well as the construction of our manually-created dataset including five languages, namely Arabic, Chinese, English, French and Russian, and the results of the participating systems. Datasets and results are available at: https://github.com/SapienzaNLP/mcl-wic.",
    "citations": 64
  },
  {
    "title": "Multilingual Word Sense Disambiguation with Unified Sense Representation",
    "year": 2022,
    "authors": "Ying Su, Hongming Zhang, Yangqiu Song, Tong Zhang",
    "url": "https://api.semanticscholar.org/CorpusId:252818944",
    "relevance": 3,
    "abstract": "As a key natural language processing (NLP) task, word sense disambiguation (WSD) evaluates how well NLP models can understand the fine-grained semantics of words under specific contexts. Benefited from the large-scale annotation, current WSD systems have achieved impressive performances in English by combining supervised learning with lexical knowledge. However, such success is hard to be replicated in other languages, where we only have very limited annotations. In this paper, based on that the multilingual lexicon BabelNet describing the same set of concepts across languages, we propose to build knowledge and supervised based Multilingual Word Sense Disambiguation (MWSD) systems. We build unified sense representations for multiple languages and address the annotation scarcity problem for MWSD by transferring annotations from rich sourced languages. With the unified sense representations, annotations from multiple languages can be jointly trained to benefit the MWSD tasks. Evaluations of SemEval-13 and SemEval-15 datasets demonstrate the effectiveness of our methodology.",
    "citations": 10
  },
  {
    "title": "Word Sense Disambiguation for 158 Languages using Word Embeddings Only",
    "year": 2020,
    "authors": "V. Logacheva, Denis Teslenko, Artem Shelmanov, Steffen Remus, Dmitry Ustalov, Andrey Kutuzov, E. Artemova, Christian Biemann, Simone Paolo Ponzetto, Alexander Panchenko",
    "url": "https://api.semanticscholar.org/CorpusId:212725472",
    "relevance": 3,
    "abstract": "Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this task. However, (i) the inherent Zipfian distribution of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al., (2018), enabling WSD in these languages. Models and system are available online.",
    "citations": 10
  },
  {
    "title": "Cross-lingual Word Sense Disambiguation using mBERT Embeddings with Syntactic Dependencies",
    "year": 2020,
    "authors": "Xingran Zhu",
    "url": "https://api.semanticscholar.org/CorpusId:228083808",
    "relevance": 3,
    "abstract": "Cross-lingual word sense disambiguation (WSD) tackles the challenge of disambiguating ambiguous words across languages given context. The pre-trained BERT embedding model has been proven to be effective in extracting contextual information of words, and have been incorporated as features into many state-of-the-art WSD systems. In order to investigate how syntactic information can be added into the BERT embeddings to result in both semantics- and syntax-incorporated word embeddings, this project proposes the concatenated embeddings by producing dependency parse tress and encoding the relative relationships of words into the input embeddings. Two methods are also proposed to reduce the size of the concatenated embeddings. The experimental results show that the high dimensionality of the syntax-incorporated embeddings constitute an obstacle for the classification task, which needs to be further addressed in future studies.",
    "citations": 6
  },
  {
    "title": "Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks",
    "year": 2025,
    "authors": "Roksana Goworek, Haim Dubossarsky",
    "url": "https://api.semanticscholar.org/CorpusId:279070911",
    "relevance": 3,
    "abstract": "Cross-lingual transfer is central to modern NLP, enabling models to perform tasks in languages different from those they were trained on. A common assumption is that training on more languages improves zero-shot transfer. We test this on sense-aware tasks-polysemy and lexical semantic change-and find that multilinguality is not necessary for effective transfer. Our large-scale analysis across 28 languages reveals that other factors, such as differences in pretraining and fine-tuning data and evaluation artifacts, better explain the perceived benefits of multilinguality. We also release fine-tuned models and provide empirical baselines to support future research. While focused on two sense-aware tasks, our findings offer broader insights into cross-lingual transfer, especially for low-resource languages.",
    "citations": 0
  },
  {
    "title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context",
    "year": 2017,
    "authors": "Shyam Upadhyay, Kai-Wei Chang, Matt Taddy, A. Kalai, James Y. Zou",
    "url": "https://api.semanticscholar.org/CorpusId:10211692",
    "relevance": 3,
    "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense wor d embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields comparable performance to a state of the art monolingual model trained on five times more training data.",
    "citations": 27
  },
  {
    "title": "Evaluating Distributed Representations for Multi-Level Lexical Semantics: A Research Proposal",
    "year": 2024,
    "authors": "Zhu Liu",
    "url": "https://api.semanticscholar.org/CorpusId:270211581",
    "relevance": 3,
    "abstract": "Modern neural networks (NNs), trained on extensive raw sentence data, construct distributed representations by compressing individual words into dense, continuous, high-dimensional vectors. These representations are expected to capture multi-level lexical meaning. In this thesis, our objective is to examine the efficacy of distributed representations from NNs in encoding lexical meaning. Initially, we identify and formalize three levels of lexical semantics: \\textit{local}, \\textit{global}, and \\textit{mixed} levels. Then, for each level, we evaluate language models by collecting or constructing multilingual datasets, leveraging various language models, and employing linguistic analysis theories. This thesis builds a bridge between computational models and lexical semantics, aiming to complement each other.",
    "citations": 0
  },
  {
    "title": "SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods",
    "year": 2025,
    "authors": "Roksana Goworek, Harpal Karlcut, M. Shezad, Nijaguna Darshana, Abhishek Mane, Syam Bondada, R. Sikka, Ulvi Mammadov, Rauf Allahverdiyev, Sriram Purighella, Paridhi Gupta, M. Ndegwa, Haim Dubossarsky",
    "url": "https://api.semanticscholar.org/CorpusId:278997247",
    "relevance": 2,
    "abstract": "This paper addresses the critical need for high-quality evaluation datasets in low-resource languages to advance cross-lingual transfer. While cross-lingual transfer offers a key strategy for leveraging multilingual pretraining to expand language technologies to understudied and typologically diverse languages, its effectiveness is dependent on quality and suitable benchmarks. We release new sense-annotated datasets of sentences containing polysemous words, spanning ten low-resource languages across diverse language families and scripts. To facilitate dataset creation, the paper presents a demonstrably beneficial semi-automatic annotation method. The utility of the datasets is demonstrated through Word-in-Context (WiC) formatted experiments that evaluate transfer on these low-resource languages. Results highlight the importance of targeted dataset creation and evaluation for effective polysemy disambiguation in low-resource settings and transfer studies. The released datasets and code aim to support further research into fair, robust, and truly multilingual NLP.",
    "citations": 2
  },
  {
    "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders",
    "year": 2016,
    "authors": "Simon Suster, Ivan Titov, Gertjan van Noord",
    "url": "https://api.semanticscholar.org/CorpusId:459717",
    "relevance": 2,
    "abstract": "We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time.",
    "citations": 43
  },
  {
    "title": "MuLaN: Multilingual Label propagatioN for Word Sense Disambiguation",
    "year": 2020,
    "authors": "Edoardo Barba, Luigi Procopio, Niccol\u00f2 Campolungo, Tommaso Pasini, Roberto Navigli",
    "url": "https://www.semanticscholar.org/paper/63d499702f5f8e4777149d7886b61ff2310e1dac",
    "relevance": 1,
    "abstract": "The knowledge acquisition bottleneck strongly affects the creation of multilingual sense-annotated data, hence limiting the power of supervised systems when applied to multilingual Word Sense Disambiguation. In this paper, we propose a semi-supervised approach based upon a novel label propagation scheme, which, by jointly leveraging contextualized word embeddings and the multilingual information enclosed in a knowledge base, projects sense labels from a high-resource language, i.e., English, to lower-resourced ones. Backed by several experiments, we provide empirical evidence that our automatically created datasets are of a higher quality than those generated by other competitors and lead a supervised model to achieve state-of-the-art performances in all multilingual Word Sense Disambiguation tasks. We make our datasets available for research purposes at https://github.com/SapienzaNLP/mulan.",
    "citations": 28
  },
  {
    "title": "Multilingual Wordnet sense Ranking using nearest context",
    "year": 2018,
    "authors": "E. U. Vasanthakumar, Francis Bond",
    "url": "https://api.semanticscholar.org/CorpusId:34312692",
    "relevance": 1,
    "abstract": "In this paper, we combine methods to estimate sense rankings from raw text with recent work on word embeddings to provide sense ranking estimates for the entries in the Open Multilingual WordNet (OMW). The existing Word2Vec pre-trained models from Polygot2 are only built for single word entries, we, therefore, re-train them with multiword expressions from the wordnets, so that multiword expressions can also be ranked. Thus this trained model gives embeddings for both single words and multiwords. The resulting lexicon gives a WSD baseline for five languages. The results are evaluated for Semcor sense corpora for 5 languages using Word2Vec and Glove models. The Glove model achieves an average accuracy of 0.47 and Word2Vec achieves 0.31 for languages such as English, Italian, Indonesian, Chinese and Japanese. The experimentation on OMW sense ranking proves that the rank correlation is generally similar to the human ranking. Hence distributional semantics can aid in Wordnet Sense Ranking.",
    "citations": 2
  },
  {
    "title": "A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation",
    "year": 2016,
    "authors": "Hong Jin Kang, Tao Chen, Muthu Kumar Chandrasekaran, Min-Yen Kan",
    "url": "https://api.semanticscholar.org/CorpusId:2640922",
    "relevance": 1,
    "abstract": "Word embeddings are now ubiquitous forms of word representation in natural language processing. There have been applications of word embeddings for monolingual word sense disambiguation (WSD) in English, but few comparisons have been done. This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD. Our simplified method leads to comparable state-of-the-art performance without expensive retraining. Cross-Lingual WSD \u2013 where the word senses of a word in a source language come from a separate target translation language \u2013 can also assist in language learning; for example, when providing translations of target vocabulary for learners. Thus we have also applied word embeddings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking. We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does not work well. We discuss the ramifications of this outcome.",
    "citations": 4
  },
  {
    "title": "Let\u2019s Play Mono-Poly: BERT Can Reveal Words\u2019 Polysemy Level and Partitionability into Senses",
    "year": 2021,
    "authors": "Aina Gar\u00ed Soler, Marianna Apidianaki",
    "url": "https://www.semanticscholar.org/paper/befbef9f4e4c6269fa712294430ff916cf2fd51c",
    "relevance": 1,
    "abstract": "Pre-trained language models (LMs) encode rich information about linguistic structure but their knowledge about lexical polysemy remains unclear. We propose a novel experimental setup for analyzing this knowledge in LMs specifically trained for different languages (English, French, Spanish, and Greek) and in multilingual BERT. We perform our analysis on datasets carefully designed to reflect different sense distributions, and control for parameters that are highly correlated with polysemy such as frequency and grammatical category. We demonstrate that BERT-derived representations reflect words\u2019 polysemy level and their partitionability into senses. Polysemy-related information is more clearly present in English BERT embeddings, but models in other languages also manage to establish relevant distinctions between words at different polysemy levels. Our results contribute to a better understanding of the knowledge encoded in contextualized representations and open up new avenues for multilingual lexical semantics research.",
    "citations": 76
  },
  {
    "title": "With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation",
    "year": 2020,
    "authors": "Bianca Scarlini, Tommaso Pasini, Roberto Navigli",
    "url": "https://api.semanticscholar.org/CorpusId:226262325",
    "relevance": 1,
    "abstract": "Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors. ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only. We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures. ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.",
    "citations": 111
  },
  {
    "title": "Chinese Word Sense Embedding with SememeWSD and Synonym Set",
    "year": 2022,
    "authors": "Yangxi Zhou, Junping Du, Zhe Xue, Ang Li, Zeli Guan",
    "url": "https://api.semanticscholar.org/CorpusId:250113692",
    "relevance": 1,
    "abstract": "Word embedding is a fundamental natural language processing task which can learn feature of words. However, most word embedding methods assign only one vector to a word, even if polysemous words have multi-senses. To address this limitation, we propose SememeWSD Synonym (SWSDS) model to assign a different vector to every sense of polysemous words with the help of word sense disambiguation (WSD) and synonym set in OpenHowNet. We use the SememeWSD model, an unsupervised word sense disambiguation model based on OpenHowNet, to do word sense disambiguation and annotate the polysemous word with sense id. Then, we obtain top 10 synonyms of the word sense from OpenHowNet and calculate the average vector of synonyms as the vector of the word sense. In experiments, We evaluate the SWSDS model on semantic similarity calculation with Gensim's wmdistance method. It achieves improvement of accuracy. We also examine the SememeWSD model on different BERT models to find the more effective model.",
    "citations": 5
  },
  {
    "title": "Improving Word Sense Disambiguation in Neural Machine Translation with Sense Embeddings",
    "year": 2017,
    "authors": "Annette Rios Gonzales, Laura Mascarell, Rico Sennrich",
    "url": "https://api.semanticscholar.org/CorpusId:529114",
    "relevance": 1,
    "abstract": "Word sense disambiguation is necessary in translation because different word senses often have different translations. Neural machine translation models learn different senses of words as part of an end-to-end translation task, and their capability to perform word sense disambiguation has so far not been quantified. We exploit the fact that neural translation models can score arbitrary translations to design a novel cross-lingual word sense disambiguation task that is tailored towards evaluating neural machine translation models. We present a test set of 7,200 lexical ambiguities for German \u2192 English, and 6,700 for German \u2192 French, and report baseline results. With 70% of lexical ambiguities correctly disambiguated, we find that word sense disambiguation remains a challenging problem for neural machine translation, especially for rare word senses. To improve word sense disambiguation in neural machine translation, we experiment with two methods to integrate sense embeddings. In a first approach we pass sense embeddings as additional input to the neural machine translation system. For the second experiment, we extract lexical chains based on sense embeddings from the document and integrate this information into the NMT model. While a baseline NMT system disambiguates frequent word senses quite reliably, the annotation with both sense labels and lexical chains improves the neural models\u2019 performance on rare word senses.",
    "citations": 133
  },
  {
    "title": "Rutgers Multimedia Image Processing Lab at SemEval-2023 Task-1: Text-Augmentation-based Approach for Visual Word Sense Disambiguation",
    "year": 2023,
    "authors": "Keyi Li, Sen Yang, Chenyu Gao, I. Marsic",
    "url": "https://api.semanticscholar.org/CorpusId:259376584",
    "relevance": 1,
    "abstract": "This paper describes our system used in SemEval-2023 Task-1: Visual Word Sense Disambiguation (VWSD). The VWSD task is to identify the correct image that corresponds to an ambiguous target word given limited textual context. To reduce word ambiguity and enhance image selection, we proposed several text augmentation techniques, such as prompting, WordNet synonyms, and text generation. We experimented with different vision-language pre-trained models to capture the joint features of the augmented text and image. Our approach achieved the best performance using a combination of GPT-3 text generation and the CLIP model. On the multilingual test sets, our system achieved an average hit rate (at top-1) of 51.11 and a mean reciprocal rank of 65.69.",
    "citations": 1
  },
  {
    "title": "Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives",
    "year": 2021,
    "authors": "Ming Wang, Yinglin Wang",
    "url": "https://api.semanticscholar.org/CorpusId:236460274",
    "relevance": 1,
    "abstract": "Lately proposed Word Sense Disambiguation (WSD) systems have approached the estimated upper bound of the task on standard evaluation benchmarks. However, these systems typically implement the disambiguation of words in a document almost independently, underutilizing sense and word dependency in context. In this paper, we convert the nearly isolated decisions into interrelated ones by exposing senses in context when learning sense embeddings in a similarity-based Sense Aware Context Exploitation (SACE) architecture. Meanwhile, we enhance the context embedding learning with selected sentences from the same document, rather than utilizing only the sentence where each ambiguous word appears. Experiments on both English and multilingual WSD datasets have shown the effectiveness of our approach, surpassing previous state-of-the-art by large margins (3.7% and 1.2% respectively), especially on few-shot (14.3%) and zero-shot (35.9%) scenarios.",
    "citations": 17
  },
  {
    "title": "SemEval-2010 Task 3: Cross-lingual Word Sense Disambiguation",
    "year": 2009,
    "authors": "Els Lefever, Veronique Hoste",
    "url": "https://api.semanticscholar.org/CorpusId:2085863",
    "relevance": 1,
    "abstract": "We propose a multilingual unsupervised Word Sense Disambiguation (WSD) task for a sample of English nouns. Instead of providing manually sensetagged examples for each sense of a polysemous noun, our sense inventory is built up on the basis of the Europarl parallel corpus. The multilingual setup involves the translations of a given English polysemous noun in five supported languages, viz. Dutch, French, German, Spanish and Italian. \n \nThe task targets the following goals: (a) the manual creation of a multilingual sense inventory for a lexical sample of English nouns and (b) the evaluation of systems on their ability to disambiguate new occurrences of the selected polysemous nouns. For the creation of the hand-tagged gold standard, all translations of a given polysemous English noun are retrieved in the five languages and clustered by meaning. Systems can participate in 5 bilingual evaluation subtasks (English -- Dutch, English -- German, etc.) and in a multilingual subtask covering all language pairs. \n \nAs WSD from cross-lingual evidence is gaining popularity, we believe it is important to create a multilingual gold standard and run cross-lingual WSD benchmark tests.",
    "citations": 131
  },
  {
    "title": "Generating sense inventories for ambiguous arabic words",
    "year": 2021,
    "authors": "Marwah Alian, A. Awajan",
    "url": "https://api.semanticscholar.org/CorpusId:235427991",
    "relevance": 1,
    "abstract": "The process of selecting the appropriate meaning of an ambigous word according to its context is known as word sense disambiguation. In this research, we generate a number of Arabic sense inventories based on an unsupervised approach and different pre-trained embeddings, such as Aravec, Fast text, and Arabic-News embeddings. The resulted inventories from the pre-trained embeddings are evaluated to investigate their efficiency in Arabic word sense disambiguation and sentence similarity. The sense inventories are generated using an unsupervised approach that is based on a graph-based word sense induction algorithm. Results show that the Aravec-Twitter inventory achieves the best accuracy of 0.47 for 50 neighbors and a close accuracy to the Fast text inventory for 200 neighbors while it provides similar accuracy to the Arabic-News inventory for 100neighbors. The experiment of replacing ambiguous words with their sense vectors is tested for sentence similarity using all sense inventories and the results show that using Aravec-Twitter sense inventory provides a better correlation value",
    "citations": 2
  },
  {
    "title": "EViLBERT: Learning Task-Agnostic Multimodal Sense Embeddings",
    "year": 2020,
    "authors": "Agostina Calabrese, Michele Bevilacqua, Roberto Navigli",
    "url": "https://www.semanticscholar.org/paper/366c3f0c35ddce5e10a7e262f09c1f1518b58e27",
    "relevance": 1,
    "abstract": "The problem of grounding language in vision is increasingly attracting scholarly efforts. As of now, however, most of the approaches have been limited to word embeddings, which are not capable of handling polysemous words. This is mainly due to the limited coverage of the available semantically-annotated datasets, hence forcing research to rely on alternative technologies (i.e., image search engines). To address this issue, we introduce EViLBERT, an approach which is able to perform image classification over an open set of concepts, both concrete and non-concrete. Our approach is based on the recently introduced Vision-Language Pretraining (VLP) model, and builds upon a manually-annotated dataset of concept-image pairs. We use our technique to clean up the image-to-concept mapping that is provided within a multilingual knowledge base, resulting in over 258,000 images associated with 42,500 concepts. \n\nWe show that our VLP-based model can be used to create multimodal sense embeddings starting from our automatically-created dataset. In turn, we also show that these multimodal embeddings improve the performance of a Word Sense Disambiguation architecture over a strong unimodal baseline. We release code, dataset and embeddings at http://babelpic.org.",
    "citations": 9
  },
  {
    "title": "Multilingual Word Sense Disambiguation for Semantic Annotations: Fusing Knowledge Graphs, Lexical Resources, and Large Language Models",
    "year": 2024,
    "authors": "Robert David, Anna Kernerman, Ilan Kernerman, Nicolas Ferranti, Assaf Siani",
    "url": "https://www.semanticscholar.org/paper/1242c00475ae03057d15f5ceb2df640de3f6bf5a",
    "relevance": 1,
    "abstract": "",
    "citations": 1
  },
  {
    "title": "A Hybrid Contextual Embedding and Hierarchical Attention for Improving the Performance of Word Sense Disambiguation",
    "year": 2025,
    "authors": "Robbel Habtamu Yigzaw, Beakal Gizachew Assefa, Elefelious Getachew Belay",
    "url": "https://www.semanticscholar.org/paper/bc3e76b700846e49f253ad147267e57914c7a111",
    "relevance": 1,
    "abstract": "Word Sense Disambiguation is determining the correct sense of an ambiguous word within context. It plays a crucial role in natural language applications such as machine translation, question-answering, chatbots, information retrieval, sentiment analysis, and overall language comprehension. Recent advancements in this area have focused on utilizing deep contextual models to address these challenges. However, despite this positive progress, semantical and syntactical ambiguity remains a challenge, especially when dealing with polysomy words, and it is considered an AI-complete problem. In this work, we propose an approach that integrates hierarchical attention mechanisms and BERT embeddings to enhance WSD performance. Our model, incorporating local and global attention, demonstrates significant improvements in accuracy, particularly in complex sentence structures. To the best of our knowledge, our model is the first to incorporate hierarchical attention mechanisms integrated with contextual embedding. We conducted experiment on publicly available datasets for English and Italian language. Experimental results show that our model achieves state-of-the-art results in WSD, surpassing baseline models up to 2.9% F1 accuracy on English WSD. Additionally, it demonstrates superior performance in Italian WSD, outperforming existing papers up to 0.7% F1 accuracy. We further adapted the model for Amharic word sense disambiguation. Despite the absence of a standard benchmark dataset for Amharic WSD, our model achieved an accuracy of 92.4% on a dataset we prepared ourselves. Our findings underscore the significance of linguistic features in contextual information capture for WSD. While Part-of-Speech (POS) tagging has a limited impact, word embeddings significantly influence performance. Local and global attention further improve results, particularly at the word level. Overall the results emphasize the importance of context in WSD, advancing context-aware natural language processing systems.",
    "citations": 3
  },
  {
    "title": "From Word to Sense Embeddings: A Survey on Vector Representations of Meaning",
    "year": 2018,
    "authors": "Jos\u00e9 Camacho-Collados, Mohammad Taher Pilehvar",
    "url": "https://api.semanticscholar.org/CorpusId:13696533",
    "relevance": 1,
    "abstract": "\n \n \nOver the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality. \n \n \n",
    "citations": 363
  },
  {
    "title": "EDS-MEMBED: Multi-sense embeddings based on enhanced distributional semantic structures via a graph walk over word senses",
    "year": 2021,
    "authors": "E. F. Ayetiran, Petr Sojka, V'it Novotn'y",
    "url": "https://api.semanticscholar.org/CorpusId:232075721",
    "relevance": 1,
    "abstract": "",
    "citations": 10
  },
  {
    "title": "LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and Beyond",
    "year": 2021,
    "authors": "Daniel Loureiro, A. Jorge, Jos\u00e9 Camacho-Collados",
    "url": "https://api.semanticscholar.org/CorpusId:235196103",
    "relevance": 1,
    "abstract": "",
    "citations": 30
  },
  {
    "title": "Sentence Semantic Similarity based Complex Network approach for Word Sense Disambiguation",
    "year": 2023,
    "authors": "Et al. Gopal Mohadikar",
    "url": "https://api.semanticscholar.org/CorpusId:265896546",
    "relevance": 1,
    "abstract": "Word Sense Disambiguation is a branch of Natural Language Processing(NLP) that deals with multi-sense words. The multi-sense words are referred to as the polysemous words. The term lexical ambiguity is introduced by the multi-sense words. The existing sense disambiguation module works effectively for single sentences with available context information. The word embedding plays a vital role in the process of disambiguation. The context-dependent word embedding model is used for disambiguation. The main goal of this research paper is to disambiguate the polysemous words by considering available context information. The main identified challenge of disambiguation is the ambiguous word without context information. The discussed complex network approach is disambiguating ambiguous sentences by considering the semantic similarities. The sentence semantic similarity-based network is constructed for disambiguating ambiguous sentences. The proposed methodology is trained with SemCor, Adaptive-Lex, and OMSTI standard lexical resources. The findings state that the discussed methodology is working fine for disambiguating large documents where the sense of ambiguous sentences is on the adjacent sentences.",
    "citations": 0
  },
  {
    "title": "Integrating techniques of social network analysis and word embedding for word sense disambiguation",
    "year": 2025,
    "authors": "Chihli Hung, Chihli Hung, Hsien-Ming Chou",
    "url": "https://www.semanticscholar.org/paper/ab704eac35f65feafbff01677cb9afd9dafc8793",
    "relevance": 1,
    "abstract": "PurposeThis research addresses the challenge of polysemous words in word embedding techniques, which are commonly used in text mining. It aims to resolve word sense ambiguity by introducing a social network sense disambiguation (SNSD) model based on social network analysis (SNA).Design/methodology/approachThe SNSD model treats words as members of a social network and their co-occurrence relationships as interactions. By analyzing these interactions, the model identifies words with high betweenness centrality, which may act as bridges between different word sense communities, indicating polysemy. This unsupervised method does not rely on pre-tagged resources and is validated using the IMDb dataset.FindingsThe SNSD model effectively resolves word sense ambiguity in word embeddings, proving to be a cost-effective and adaptable solution to this issue. The experimental results demonstrate that the model enhances the accuracy of word embeddings by accurately identifying the correct meanings of polysemous words.Originality/valueThis study is the first to apply SNA to word sense disambiguation (WSD). The SNSD model offers a novel, unsupervised approach that overcomes the limitations of traditional supervised or knowledge-based methods, providing a valuable contribution to the field of text mining.",
    "citations": 0
  },
  {
    "title": "From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation",
    "year": 2022,
    "authors": "Marianna Apidianaki",
    "url": "https://api.semanticscholar.org/CorpusId:254960044",
    "relevance": 1,
    "abstract": "Vector-based word representation paradigms situate lexical meaning at different levels of abstraction. Distributional and static embedding models generate a single vector per word type, which is an aggregate across the instances of the word in a corpus. Contextual language models, on the contrary, directly capture the meaning of individual word instances. The goal of this survey is to provide an overview of word meaning representation methods, and of the strategies that have been proposed for improving the quality of the generated vectors. These often involve injecting external knowledge about lexical semantic relationships, or refining the vectors to describe different senses. The survey also covers recent approaches for obtaining word type-level representations from token-level ones, and for combining static and contextualized representations. Special focus is given to probing and interpretation studies aimed at discovering the lexical semantic knowledge that is encoded in contextualized representations. The challenges posed by this exploration have motivated the interest towards static embedding derivation from contextualized embeddings, and for methods aimed at improving the similarity estimates that can be drawn from the space of contextual language models.",
    "citations": 37
  },
  {
    "title": "Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings",
    "year": 2020,
    "authors": "Christos Xypolopoulos, A. Tixier, M. Vazirgiannis",
    "url": "https://api.semanticscholar.org/CorpusId:214612434",
    "relevance": 1,
    "abstract": "The number of senses of a given word, or polysemy, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate polysemy based on simple geometry in the contextual embedding space. Our approach is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as WordNet, OntoNotes, Oxford, Wikipedia, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes it applicable to any language. Code and data are publicly available https://github.com/ksipos/polysemy-assessment .",
    "citations": 10
  },
  {
    "title": "Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy",
    "year": 2021,
    "authors": "Marcos Garcia",
    "url": "https://api.semanticscholar.org/CorpusId:235652017",
    "relevance": 1,
    "abstract": "This paper presents a multilingual study of word meaning representations in context. We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy. To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context. However, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences. Experiments are performed in Galician, Portuguese, English, and Spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study.",
    "citations": 25
  },
  {
    "title": "NLP Research and Resources at DaSciM, Ecole Polytechnique",
    "year": 2021,
    "authors": "Hadi Abdine, Yanzhu Guo, Moussa Kamal Eddine, Giannis Nikolentzos, Stamatis Outsios, Guokan Shang, Christos Xypolopoulos, M. Vazirgiannis",
    "url": "https://api.semanticscholar.org/CorpusId:244773040",
    "relevance": 1,
    "abstract": "DaSciM (Data Science and Mining) part of LIX at Ecole Polytechnique, established in 2013 and since then producing research results in the area of large scale data analysis via methods of machine and deep learning. The group has been specifically active in the area of NLP and text mining with interesting results at methodological and resources level. Here follow our different contributions of interest to the AFIA community. 1 Graph based representations for NLP and Text Mining In recent years, graphs have become a widely used tool for modeling structured data. To enable the application of graph-based approaches to textual data, members of the DaSciM team developed the graph-of-words approach [RV13] which maps text to a graph where vertices correspond to terms and edges represent co-occurrences between the connected terms within a fixed-size window. Once a document is represented as a graph, traditional, but also modern algorithms designed for graph-structured data can be applied to natural language texts. The researchers of the DaSciM team have explored how different text mining tasks can benefit from graph-based algorithms. One such task is keyword extraction. Capitalizing on the concept of graph degeneracy, the k-core algorithm was applied to the graph representation of text to identify cohesive subgraphs [RV15; TMV16]. The vertices of these subgraphs can be considered as the most important terms (i.e., keywords) of a given textual document. The members of the DaSciM team have also utilized machine learning algorithms that operate on graphs to deal with tasks such as text categorization and sentiment analysis. Both graph kernels [Nik+17] and graph neural networks [NTV20], the two dominant methodologies for performing machine learning on graphs, have been applied to these problems with great success. The effectiveness of the graph-based representations has also been evaluated in the task of detecting sub-events from data collected from Twitter [Mel+15; Mel+18]. The occurrence of a sub-event is usually associated with a change in the content of the messages posted recently by users compared to the content of the messages posted in the past. Such a significant change of content is reflected in the structure of the graph representation of tweets and can be captured by graph-based approaches.",
    "citations": 0
  },
  {
    "title": "Evaluating multi-sense embeddings for semantic resolution monolingually and in word translation",
    "year": 2016,
    "authors": "G\u00e1bor Borb\u00e9ly, M\u00e1rton Makrai, D. Nemeskey, Andr\u00e1s Kornai",
    "url": "https://api.semanticscholar.org/CorpusId:18614158",
    "relevance": 1,
    "abstract": "Multi-sense word embeddings (MSEs) model different meanings of word forms with different vectors. We propose two new methods for evaluating MSEs, one based on monolingual dictionaries, and the other exploiting the principle that words may be ambiguous as far as the postulated senses translate to different words in some other language.",
    "citations": 5
  },
  {
    "title": "An Unsupervised, Geometric and Syntax-aware Quantification of Polysemy",
    "year": 2022,
    "authors": "Anmol Goel, Charu Sharma, P. Kumaraguru",
    "url": "https://api.semanticscholar.org/CorpusId:256461217",
    "relevance": 1,
    "abstract": "Polysemy is the phenomenon where a single word form possesses two or more related senses. It is an extremely ubiquitous part of natural language and analyzing it has sparked rich discussions in the linguistics, psychology and philosophy communities alike. With scarce attention paid to polysemy in computational linguistics, and even scarcer attention toward quantifying polysemy, in this paper, we propose a novel, unsupervised framework to compute and estimate polysemy scores for words in multiple languages. We infuse our proposed quantification with syntactic knowledge in the form of dependency structures. This informs the final polysemy scores of the lexicon motivated by recent linguistic findings that suggest there is an implicit relation between syntax and ambiguity/polysemy. We adopt a graph based approach by computing the discrete Ollivier Ricci curvature on a graph of the contextual nearest neighbors. We test our framework on curated datasets controlling for different sense distributions of words in 3 typologically diverse languages - English, French and Spanish. The effectiveness of our framework is demonstrated by significant correlations of our quantification with expert human annotated language resources like WordNet. We observe a 0.3 point increase in the correlation coefficient as compared to previous quantification studies in English. Our research leverages contextual language models and syntactic structures to empirically support the widely held theoretical linguistic notion that syntax is intricately linked to ambiguity/polysemy.",
    "citations": 5
  },
  {
    "title": "Finding Universal Grammatical Relations in Multilingual BERT",
    "year": 2020,
    "authors": "Ethan A. Chi, John Hewitt, Christopher D. Manning",
    "url": "https://www.semanticscholar.org/paper/1376a8e1b06b7a7b7cacd45f52268e427c3b0135",
    "relevance": 3,
    "abstract": "Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks\u2019 internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.",
    "citations": 167
  },
  {
    "title": "First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT",
    "year": 2021,
    "authors": "Benjamin Muller, Yanai Elazar, B. Sagot, Djam\u00e9 Seddah",
    "url": "https://www.semanticscholar.org/paper/1cfd0f08bb7ab449bbccc3d69a62ffbf43a7eb7f",
    "relevance": 3,
    "abstract": "Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model\u2019s internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis.",
    "citations": 86
  },
  {
    "title": "How Multilingual is Multilingual BERT?",
    "year": 2019,
    "authors": "Telmo Pires, Eva Schlinger, Dan Garrette",
    "url": "https://www.semanticscholar.org/paper/809cc93921e4698bde891475254ad6dfba33d03b",
    "relevance": 3,
    "abstract": "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
    "citations": 1605
  },
  {
    "title": "A Primer in BERTology: What We Know About How BERT Works",
    "year": 2020,
    "authors": "Anna Rogers, Olga Kovaleva, Anna Rumshisky",
    "url": "https://api.semanticscholar.org/CorpusId:211532403",
    "relevance": 3,
    "abstract": "Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.",
    "citations": 1757
  },
  {
    "title": "On the Language Neutrality of Pre-trained Multilingual Representations",
    "year": 2020,
    "authors": "Jind\u0159ich Libovick\u00fd, Rudolf Rosa, Alexander M. Fraser",
    "url": "https://www.semanticscholar.org/paper/75a35576efee34622254f265e4cbeb5e01eea7a1",
    "relevance": 3,
    "abstract": "Multilingual contextual embeddings, such as multilingual BERT and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead investigate the language-neutrality of multilingual contextual embeddings directly and with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and, in general, more informative than aligned static word-type embeddings, which are explicitly trained for language neutrality. Contextual embeddings are still only moderately language-neutral by default, so we propose two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach state-of-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data.",
    "citations": 115
  },
  {
    "title": "Emerging Cross-lingual Structure in Pretrained Language Models",
    "year": 2019,
    "authors": "Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, Veselin Stoyanov",
    "url": "https://api.semanticscholar.org/CorpusId:207853017",
    "relevance": 3,
    "abstract": "We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder. To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process.",
    "citations": 300
  },
  {
    "title": "How Language-Neutral is Multilingual BERT?",
    "year": 2019,
    "authors": "Jind\u0159ich Libovick\u00fd, Rudolf Rosa, Alexander M. Fraser",
    "url": "https://www.semanticscholar.org/paper/5d8beeca1a2e3263b2796e74e2f57ffb579737ee",
    "relevance": 3,
    "abstract": "Multilingual BERT (mBERT) provides sentence representations for 104 languages, which are useful for many multi-lingual tasks. Previous work probed the cross-linguality of mBERT using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the semantic properties of mBERT. We show that mBERT representations can be split into a language-specific component and a language-neutral component, and that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment and sentence retrieval but is not yet good enough for the more difficult task of MT quality estimation. Our work presents interesting challenges which must be solved to build better language-neutral representations, particularly for tasks requiring linguistic transfer of semantics.",
    "citations": 127
  },
  {
    "title": "Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation",
    "year": 2020,
    "authors": "Nils Reimers, Iryna Gurevych",
    "url": "https://api.semanticscholar.org/CorpusId:216036089",
    "relevance": 3,
    "abstract": "We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 10 languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.",
    "citations": 1224
  },
  {
    "title": "It\u2019s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT",
    "year": 2020,
    "authors": "Hila Gonen, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg",
    "url": "https://www.semanticscholar.org/paper/15bcf3b7aa6511a55d7066419453a6d5906b2db8",
    "relevance": 3,
    "abstract": "Recent works have demonstrated that multilingual BERT (mBERT) learns rich cross-lingual representations, that allow for transfer across languages. We study the word-level translation information embedded in mBERT and present two simple methods that expose remarkable translation capabilities with no fine-tuning. The results suggest that most of this information is encoded in a non-linear way, while some of it can also be recovered with purely linear tools. As part of our analysis, we test the hypothesis that mBERT learns representations which contain both a language-encoding component and an abstract, cross-lingual component, and explicitly identify an empirical language-identity subspace within mBERT representations.",
    "citations": 57
  },
  {
    "title": "Inducing Language-Agnostic Multilingual Representations",
    "year": 2020,
    "authors": "Wei Zhao, Steffen Eger, Johannes Bjerva, Isabelle Augenstein",
    "url": "https://api.semanticscholar.org/CorpusId:221186888",
    "relevance": 3,
    "abstract": "Cross-lingual representations have the potential to make NLP techniques available to the vast majority of languages in the world. However, they currently require large pretraining corpora or access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this: (i) re-aligning the vector spaces of target languages (all together) to a pivot source language; (ii) removing language-specific means and variances, which yields better discriminativeness of embeddings as a by-product; and (iii) increasing input similarity across languages by removing morphological contractions and sentence reordering. We evaluate on XNLI and reference-free MT evaluation across 19 typologically diverse languages. Our findings expose the limitations of these approaches\u2014unlike vector normalization, vector space re-alignment and text normalization do not achieve consistent gains across encoders and languages. Due to the approaches\u2019 additive effects, their combination decreases the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R) on average across all tasks and languages, however.",
    "citations": 70
  },
  {
    "title": "Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study",
    "year": 2020,
    "authors": "Saurabh Kulshreshtha, Jos\u00e9 Luis Redondo Garc\u00eda, Ching-Yun Chang",
    "url": "https://api.semanticscholar.org/CorpusId:222067073",
    "relevance": 3,
    "abstract": "Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work propose several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs dictionary-based supervision and rotational vs fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.",
    "citations": 51
  },
  {
    "title": "What does it mean to be language-agnostic? Probing multilingual sentence encoders for typological properties",
    "year": 2020,
    "authors": "Rochelle Choenni, Ekaterina Shutova",
    "url": "https://api.semanticscholar.org/CorpusId:221971055",
    "relevance": 3,
    "abstract": "Multilingual sentence encoders have seen much success in cross-lingual model transfer for downstream NLP tasks. Yet, we know relatively little about the properties of individual languages or the general patterns of linguistic variation that they encode. We propose methods for probing sentence representations from state-of-the-art multilingual encoders (LASER, M-BERT, XLM and XLM-R) with respect to a range of typological properties pertaining to lexical, morphological and syntactic structure. In addition, we investigate how this information is distributed across all layers of the models. Our results show interesting differences in encoding linguistic variation associated with different pretraining strategies.",
    "citations": 40
  },
  {
    "title": "On Learning Universal Representations Across Languages",
    "year": 2020,
    "authors": "Xiangpeng Wei, Yue Hu, Rongxiang Weng, Luxi Xing, Heng Yu, Weihua Luo",
    "url": "https://api.semanticscholar.org/CorpusId:220920191",
    "relevance": 3,
    "abstract": "Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations, and show the effectiveness on cross-lingual understanding and generation. We propose Hierarchical Contrastive Learning (HiCTL) to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on three benchmarks: language understanding tasks (QQP, QNLI, SST-2, MRPC, STS-B and MNLI) in the GLUE benchmark, cross-lingual natural language inference (XNLI) and machine translation. Experimental results show that the HiCTL obtains an absolute gain of 1.0%/2.2% accuracy on GLUE/XNLI as well as achieves substantial improvements of +1.7-+3.6 BLEU on both the high-resource and low-resource English-to-X translation tasks over strong baselines. We will release the source codes as soon as possible.",
    "citations": 91
  },
  {
    "title": "Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations",
    "year": 2024,
    "authors": "Zhihui Xie, Handong Zhao, Tong Yu, Shuai Li",
    "url": "https://api.semanticscholar.org/CorpusId:256461245",
    "relevance": 3,
    "abstract": "Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the subspace is found, we can directly project the original embeddings into the null space to boost language agnosticism without finetuning. We systematically evaluate our method on various tasks including the challenging language-agnostic QA retrieval task. Empirical results show that applying our method consistently leads to improvements over commonly used ML-LMs.",
    "citations": 19
  },
  {
    "title": "Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?",
    "year": 2022,
    "authors": "Ningyu Xu, Tao Gui, Ruotian Ma, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang",
    "url": "https://www.semanticscholar.org/paper/bfff952fb890f3eb4ba22718f1df70a030741b74",
    "relevance": 3,
    "abstract": "Multilingual BERT (mBERT) has demonstrated considerable cross-lingual syntactic ability, whereby it enables effective zero-shot cross-lingual transfer of syntactic knowledge. The transfer is more successful between some languages, but it is not well understood what leads to this variation and whether it fairly reflects difference between languages. In this work, we investigate the distributions of grammatical relations induced from mBERT in the context of 24 typologically different languages. We demonstrate that the distance between the distributions of different languages is highly consistent with the syntactic difference in terms of linguistic formalisms. Such difference learnt via self-supervision plays a crucial role in the zero-shot transfer performance and can be predicted by variation in morphosyntactic properties between languages. These results suggest that mBERT properly encodes languages in a way consistent with linguistic diversity and provide insights into the mechanism of cross-lingual transfer.",
    "citations": 14
  },
  {
    "title": "On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation",
    "year": 2020,
    "authors": "Wei Zhao, Goran Glavavs, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger",
    "url": "https://api.semanticscholar.org/CorpusId:218487791",
    "relevance": 3,
    "abstract": "Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish \u201ctranslationese\u201d, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.",
    "citations": 63
  },
  {
    "title": "The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer",
    "year": 2022,
    "authors": "Pavel Efimov, Leonid Boytsov, E. Arslanova, Pavel Braslavski",
    "url": "https://api.semanticscholar.org/CorpusId:248157267",
    "relevance": 3,
    "abstract": "Large multilingual language models such as mBERT or XLM-R enable zero-shot cross-lingual transfer in various IR and NLP tasks. Cao et al. (2020) proposed a data- and compute-efficient method for cross-lingual adjustment of mBERT that uses a small parallel corpus to make embeddings of related words across languages similar to each other. They showed it to be effective in NLI for five European languages. In contrast we experiment with a typologically diverse set of languages (Spanish, Russian, Vietnamese, and Hindi) and extend their original implementations to new tasks (XSR, NER, and QA) and an additional training regime (continual learning). Our study reproduced gains in NLI for four languages, showed improved NER, XSR, and cross-lingual QA results in three languages (though some cross-lingual QA gains were not statistically significant), while mono-lingual QA performance never improved and sometimes degraded. Analysis of distances between contextualized embeddings of related and unrelated words (across languages) showed that fine-tuning leads to\"forgetting\"some of the cross-lingual alignment information. Based on this observation, we further improved NLI performance using continual learning.",
    "citations": 11
  },
  {
    "title": "mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?",
    "year": 2024,
    "authors": "Tianze Hua, Tian Yun, Ellie Pavlick",
    "url": "https://api.semanticscholar.org/CorpusId:269282665",
    "relevance": 3,
    "abstract": "Many pretrained multilingual models exhibit cross-lingual transfer ability, which is often attributed to a learned language-neutral representation during pretraining. However, it remains unclear what factors contribute to the learning of a language-neutral representation, and whether the learned language-neutral representation suffices to facilitate cross-lingual transfer. We propose a synthetic task, Multilingual Othello (mOthello), as a testbed to delve into these two questions. We find that: (1) models trained with naive multilingual pretraining fail to learn a language-neutral representation across all input languages; (2) the introduction of\"anchor tokens\"(i.e., lexical items that are identical across languages) helps cross-lingual representation alignment; and (3) the learning of a language-neutral representation alone is not sufficient to facilitate cross-lingual transfer. Based on our findings, we propose a novel approach - multilingual pretraining with unified output space - that both induces the learning of language-neutral representation and facilitates cross-lingual transfer.",
    "citations": 16
  },
  {
    "title": "Emergent Structures and Training Dynamics in Large Language Models",
    "year": 2022,
    "authors": "R. Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan",
    "url": "https://api.semanticscholar.org/CorpusId:247656607",
    "relevance": 3,
    "abstract": "Large language models have achieved success on a number of downstream tasks, particularly in a few and zero-shot manner. As a consequence, researchers have been investigating both the kind of information these networks learn and how such information can be encoded in the parameters of the model. We survey the literature on changes in the network during training, drawing from work outside of NLP when necessary, and on learned representations of linguistic features in large language models. We note in particular the lack of sufficient research on the emergence of functional units, subsections of the network where related functions are grouped or organised, within large language models and motivate future work that grounds the study of language models in an analysis of their changing internal structure during training time.",
    "citations": 16
  },
  {
    "title": "A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference",
    "year": 2022,
    "authors": "Kerem Zaman, Yonatan Belinkov",
    "url": "https://api.semanticscholar.org/CorpusId:248119025",
    "relevance": 3,
    "abstract": "Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of faithfulness and plausibility.First, we introduce a novel cross-lingual strategy to measure faithfulness based on word alignments, which eliminates the drawbacks of erasure-based evaluations.We then perform a comprehensive evaluation of attribution methods, considering different output mechanisms and aggregation methods.Finally, we augment the XNLI dataset with highlight-based explanations, providing a multilingual NLI dataset with highlights, to support future exNLP studies. Our results show that attribution methods performing best for plausibility and faithfulness are different.",
    "citations": 8
  },
  {
    "title": "Cross-lingual Similarity of Multilingual Representations Revisited",
    "year": 2022,
    "authors": "Maksym Del, Mark Fishel",
    "url": "https://api.semanticscholar.org/CorpusId:253762088",
    "relevance": 3,
    "abstract": "Related works used indexes like CKA and variants of CCA to measure the similarity of cross-lingual representations in multilingual language models. In this paper, we argue that assumptions of CKA/CCA align poorly with one of the motivating goals of cross-lingual learning analysis, i.e., explaining zero-shot cross-lingual transfer. We highlight what valuable aspects of cross-lingual similarity these indexes fail to capture and provide a motivating case study demonstrating the problem empirically. Then, we introduce Average Neuron-Wise Correlation (ANC) as a straightforward alternative that is exempt from the difficulties of CKA/CCA and is good specifically in a cross-lingual context. Finally, we use ANC to construct evidence that the previously introduced \u201cfirst align, then predict\u201d pattern takes place not only in masked language models (MLMs) but also in multilingual models with causal language modeling objectives (CLMs). Moreover, we show that the pattern extends to the scaled versions of the MLMs and CLMs (up to 85x original mBERT). Our code is publicly available at https://github.com/TartuNLP/xsim",
    "citations": 6
  },
  {
    "title": "Probing Multilingual BERT for Genetic and Typological Signals",
    "year": 2020,
    "authors": "Taraka Rama, Lisa Beinborn, Steffen Eger",
    "url": "https://api.semanticscholar.org/CorpusId:226245971",
    "relevance": 3,
    "abstract": "We probe the layers in multilingual BERT (mBERT) for phylogenetic and geographic language signals across 100 languages and compute language distances based on the mBERT representations. We 1) employ the language distances to infer and evaluate language trees, finding that they are close to the reference family tree in terms of quartet tree distance, 2) perform distance matrix regression analysis, finding that the language distances can be best explained by phylogenetic and worst by structural factors and 3) present a novel measure for measuring diachronic meaning stability (based on cross-lingual representation variability) which correlates significantly with published ranked lists based on linguistic approaches. Our results contribute to the nascent field of typological interpretability of cross-lingual text representations.",
    "citations": 27
  },
  {
    "title": "On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning",
    "year": 2021,
    "authors": "Marc Tanti, Lonneke van der Plas, Claudia Borg, Albert Gatt",
    "url": "https://api.semanticscholar.org/CorpusId:237513452",
    "relevance": 3,
    "abstract": "Recent work has shown evidence that the knowledge acquired by multilingual BERT (mBERT) has two components: a language-specific and a language-neutral one. This paper analyses the relationship between them, in the context of fine-tuning on two tasks \u2013 POS tagging and natural language inference \u2013 which require the model to bring to bear different degrees of language-specific knowledge. Visualisations reveal that mBERT loses the ability to cluster representations by language after fine-tuning, a result that is supported by evidence from language identification experiments. However, further experiments on \u2018unlearning\u2019 language-specific representations using gradient reversal and iterative adversarial learning are shown not to add further improvement to the language-independent component over and above the effect of fine-tuning. The results presented here suggest that the process of fine-tuning causes a reorganisation of the model\u2019s limited representational capacity, enhancing language-independent representations at the expense of language-specific ones.",
    "citations": 12
  },
  {
    "title": "Similarity of Sentence Representations in Multilingual LMs: Resolving Conflicting Literature and a Case Study of Baltic Languages",
    "year": 2021,
    "authors": "Maksym Del, Mark Fishel",
    "url": "https://api.semanticscholar.org/CorpusId:249921326",
    "relevance": 3,
    "abstract": "Low-resource languages, such as Baltic languages, benefit from Large Multilingual Models (LMs) that possess remarkable cross-lingual transfer performance capabilities. This work is an interpretation and analysis study into cross-lingual representations of Multilingual LMs. Previous works hypothesized that these LMs internally project representations of different languages into a shared cross-lingual space. However, the literature produced contradictory results. In this paper, we revisit the prior work claiming that\"BERT is not an Interlingua\"and show that different languages do converge to a shared space in such language models with another choice of pooling strategy or similarity index. Then, we perform cross-lingual representational analysis for the two most popular multilingual LMs employing 378 pairwise language comparisons. We discover that while most languages share joint cross-lingual space, some do not. However, we observe that Baltic languages do belong to that shared space. The code is available at https://github.com/TartuNLP/xsim.",
    "citations": 5
  },
  {
    "title": "Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction",
    "year": 2021,
    "authors": "Shubhanshu Mishra, A. Haghighi",
    "url": "https://api.semanticscholar.org/CorpusId:239049874",
    "relevance": 3,
    "abstract": "We evaluate a simple approach to improving zero-shot multilingual transfer of mBERT on social media corpus by adding a pretraining task called translation pair prediction (TPP), which predicts whether a pair of cross-lingual texts are a valid translation. Our approach assumes access to translations (exact or approximate) between source-target language pairs, where we fine-tune a model on source language task data and evaluate the model in the target language. In particular, we focus on language pairs where transfer learning is difficult for mBERT: those where source and target languages are different in script, vocabulary, and linguistic typology. We show improvements from TPP pretraining over mBERT alone in zero-shot transfer from English to Hindi, Arabic, and Japanese on two social media tasks: NER (a 37% average relative improvement in F1 across target languages) and sentiment classification (12% relative improvement in F1) on social media text, while also benchmarking on a non-social media task of Universal Dependency POS tagging (6.7% relative improvement in accuracy). Our results are promising given the lack of social media bitext corpus. Our code can be found at: https://github.com/twitter-research/multilingual-alignment-tpp.",
    "citations": 4
  },
  {
    "title": "Enhancing Cross-lingual Semantic Annotations using Deep Network Sentence Embeddings",
    "year": 2021,
    "authors": "Ying-Chi Lin, Phillip Hoffmann, E. Rahm",
    "url": "https://api.semanticscholar.org/CorpusId:231959652",
    "relevance": 3,
    "abstract": "Annotating documents using concepts of ontologies enhances data quality and interoperability. Such semantic annotations also facilitate the comparison of multiple studies and even cross-lingual results. The FDA therefore requires that all submitted medical forms have to be annotated. In this work we aim at annotating medical forms in German. These standardized forms are used in health care practice and biomedical research and are translated/adapted to various languages. We focus on annotations that cover the whole question in the form as required by the FDA. We need to map these non-English questions to English concepts as many of these concepts do not exist in other languages. Due to the process of translation and adaptation, the corresponding non-English forms deviate from the original forms syntactically. This causes the conventional string matching methods to produce low annotation quality results. Consequently, we propose a new approach that incorporates semantics into the mapping procedure. By utilizing sentence embeddings generated by deep networks in the cross-lingual annotation process, we achieve a recall of 84.62%. This is an improvement of 134% compared to conventional string matching. Likewise, we also achieve an improvement of 51% in precision and 65% in F-measure.",
    "citations": 3
  },
  {
    "title": "Do Explicit Alignments Robustly Improve Massively Multilingual Encoders?",
    "year": 2020,
    "authors": "Shijie Wu, Mark Dredze",
    "url": "https://api.semanticscholar.org/CorpusId:236940581",
    "relevance": 3,
    "abstract": "Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.",
    "citations": 7
  },
  {
    "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity",
    "year": 2020,
    "authors": "Ivan Vulic, Simon Baker, E. Ponti, Ulla Petti, Ira Leviant, Kelly Wing, Olga Majewska, Eden Bar, Matt Malone, T. Poibeau, Roi Reichart, A. Korhonen",
    "url": "https://api.semanticscholar.org/CorpusId:212644529",
    "relevance": 2,
    "abstract": "Abstract We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering data sets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language data set is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 crosslingual semantic similarity data sets. Because of its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and crosslingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and crosslingual representation models, including static and contextualized word embeddings (such as fastText, monolingual and multilingual BERT, XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised crosslingual word embeddings. We also present a step-by-step data set creation protocol for creating consistent, Multi-Simlex\u2013style resources for additional languages. We make these contributions\u2014the public release of Multi-SimLex data sets, their creation protocol, strong baseline results, and in-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning\u2014available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.",
    "citations": 90
  },
  {
    "title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study",
    "year": 2019,
    "authors": "Karthikeyan K, Zihan Wang, Stephen Mayhew, Dan Roth",
    "url": "https://api.semanticscholar.org/CorpusId:209183618",
    "relevance": 2,
    "abstract": "Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT) -- surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives. The experimental study is done in the context of three typologically different languages -- Spanish, Hindi, and Russian -- and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that the lexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an integral part of it. All our models and implementations can be found on our project page: this http URL .",
    "citations": 368
  },
  {
    "title": "Identifying Elements Essential for BERT\u2019s Multilinguality",
    "year": 2020,
    "authors": "Philipp Dufter, Hinrich Sch\u00fctze",
    "url": "https://api.semanticscholar.org/CorpusId:226262235",
    "relevance": 2,
    "abstract": "It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure. We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data. Overall, we identify four architectural and two linguistic elements that influence multilinguality. Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment. Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.",
    "citations": 93
  },
  {
    "title": "Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure",
    "year": 2022,
    "authors": "Yuan Chai, Yaobo Liang, Nan Duan",
    "url": "https://api.semanticscholar.org/CorpusId:247476003",
    "relevance": 2,
    "abstract": "Multilingual pre-trained language models, such as mBERT and XLM-R, have shown impressive cross-lingual ability. Surprisingly, both of them use multilingual masked language model (MLM) without any cross-lingual supervision or aligned data. Despite the encouraging results, we still lack a clear understanding of why cross-lingual ability could emerge from multilingual MLM. In our work, we argue that cross-language ability comes from the commonality between languages. Specifically, we study three language properties: constituent order, composition and word co-occurrence. First, we create an artificial language by modifying property in source language. Then we study the contribution of modified property through the change of cross-language transfer results on target language. We conduct experiments on six languages and two cross-lingual NLP tasks (textual entailment, sentence retrieval). Our main conclusion is that the contribution of constituent order and word co-occurrence is limited, while the composition is more crucial to the success of cross-linguistic transfer.",
    "citations": 26
  },
  {
    "title": "Identifying Necessary Elements for BERT's Multilinguality",
    "year": 2020,
    "authors": "Philipp Dufter, Hinrich Sch\u00fctze",
    "url": "https://api.semanticscholar.org/CorpusId:218470235",
    "relevance": 2,
    "abstract": "It has been shown that multilingual BERT (mBERT) yields high quality multilingual rep- resentations and enables effective zero-shot transfer. This is suprising given that mBERT does not use any kind of crosslingual sig- nal during training. While recent literature has studied this effect, the exact reason for mBERT\u2019s multilinguality is still unknown. We aim to identify architectural properties of BERT as well as linguistic properties of lan- guages that are necessary for BERT to become multilingual. To allow for fast experimenta- tion we propose an efficient setup with small BERT models and synthetic as well as natu- ral data. Overall, we identify six elements that are potentially necessary for BERT to be mul- tilingual. Architectural factors that contribute to multilinguality are underparameterization, shared special tokens (e.g., \u201c[CLS]\u201d), shared position embeddings and replacing masked to- kens with random tokens. Factors related to training data that are beneficial for multilin- guality are similar word order and comparabil- ity of corpora.",
    "citations": 18
  },
  {
    "title": "Are All Languages Created Equal in Multilingual BERT?",
    "year": 2020,
    "authors": "Shijie Wu, Mark Dredze",
    "url": "https://www.semanticscholar.org/paper/14489ec7893e373a0dcc9555c52b99b2b3a429f6",
    "relevance": 1,
    "abstract": "Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.",
    "citations": 371
  },
  {
    "title": "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT",
    "year": 2019,
    "authors": "Shijie Wu, Mark Dredze",
    "url": "https://api.semanticscholar.org/CorpusId:126167342",
    "relevance": 1,
    "abstract": "Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.",
    "citations": 725
  },
  {
    "title": "Zero-shot cross-lingual transfer language selection using linguistic similarity",
    "year": 2023,
    "authors": "J. Eronen, M. Ptaszynski, Fumito Masui",
    "url": "https://www.semanticscholar.org/paper/55feee6e57672a90f7c85ff854d6ef014c273f3b",
    "relevance": 1,
    "abstract": "",
    "citations": 50
  },
  {
    "title": "AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing",
    "year": 2021,
    "authors": "Katikapalli Subramanyam Kalyan, A. Rajasekharan, S. Sangeetha",
    "url": "https://api.semanticscholar.org/CorpusId:236987275",
    "relevance": 1,
    "abstract": "Transformer-based pretrained language models (T-PTLMs) have achieved great success in almost every NLP task. The evolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning and transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using self-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to downstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a brief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining tasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T-PTLMs and then give brief overview of various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T-PTLMs. Finally, we highlight some of the future research directions which will further improve these models. We strongly believe that this comprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent happenings in T-PTLMs.",
    "citations": 317
  },
  {
    "title": "Pre-trained models for natural language processing: A survey",
    "year": 2020,
    "authors": "Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang",
    "url": "https://api.semanticscholar.org/CorpusId:212747830",
    "relevance": 1,
    "abstract": "Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.",
    "citations": 1636
  },
  {
    "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization",
    "year": 2020,
    "authors": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson",
    "url": "https://api.semanticscholar.org/CorpusId:214641214",
    "relevance": 1,
    "abstract": "Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.",
    "citations": 1093
  },
  {
    "title": "ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora",
    "year": 2020,
    "authors": "Ouyang Xuan, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",
    "url": "https://api.semanticscholar.org/CorpusId:229923118",
    "relevance": 1,
    "abstract": "Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for low-resource languages. In this paper, we propose Ernie-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental results show that Ernie-M outperforms existing cross-lingual models and delivers new state-of-the-art results in various cross-lingual downstream tasks. The codes and pre-trained models will be made publicly available.",
    "citations": 120
  },
  {
    "title": "From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers",
    "year": 2020,
    "authors": "Anne Lauscher, Vinit Ravishankar, Ivan Vulic, Goran Glavas",
    "url": "https://api.semanticscholar.org/CorpusId:218487379",
    "relevance": 1,
    "abstract": "Massively multilingual transformers pretrained with language modeling objectives (e.g., mBERT, XLM-R) have become a de facto default transfer paradigm for zero-shot cross-lingual transfer in NLP, offering unmatched transfer performance. Current downstream evaluations, however, verify their efficacy predominantly in transfer settings involving languages with sufficient amounts of pretraining data, and with lexically and typologically close languages. In this work, we analyze their limitations and show that cross-lingual transfer via massively multilingual transformers, much like transfer via cross-lingual word embeddings, is substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER), as well as two high-level semantic tasks (NLI, QA), empirically correlate transfer performance with linguistic similarity between the source and target languages, but also with the size of pretraining corpora of target languages. We also demonstrate a surprising effectiveness of inexpensive few-shot transfer (i.e., fine-tuning on a few target-language instances after fine-tuning in the source) across the board. This suggests that additional research efforts should be invested to reach beyond the limiting zero-shot conditions.",
    "citations": 68
  },
  {
    "title": "Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection",
    "year": 2022,
    "authors": "J. Eronen, M. Ptaszynski, Fumito Masui, Masaki Arata, Gniewosz Leliwa, Michal Wroczynski",
    "url": "https://api.semanticscholar.org/CorpusId:249263180",
    "relevance": 1,
    "abstract": "",
    "citations": 38
  },
  {
    "title": "Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer",
    "year": 2024,
    "authors": "Jianyu Zheng, Fengfei Fan, Jianquan Li",
    "url": "https://api.semanticscholar.org/CorpusId:269362723",
    "relevance": 1,
    "abstract": "Unsupervised cross-lingual transfer involves transferring knowledge between languages without explicit supervision. Although numerous studies have been conducted to improve performance in such tasks by focusing on cross-lingual knowledge, particularly lexical and syntactic knowledge, current approaches are limited as they only incorporate syntactic or lexical information. Since each type of information offers unique advantages and no previous attempts have combined both, we attempt to explore the potential of this approach. In this paper, we present a novel framework called \u201cLexicon-Syntax Enhanced Multilingual BERT\u201d that combines both lexical and syntactic knowledge. Specifically, we use Multilingual BERT (mBERT) as the base model and employ two techniques to enhance its learning capabilities. The code-switching technique is used to implicitly teach the model lexical alignment information, while a syntactic-based graph attention network is designed to help the model encode syntactic structure. To integrate both types of knowledge, we input code-switched sequences into both the syntactic module and the mBERT base model simultaneously. Our extensive experimental results demonstrate this framework can consistently outperform all baselines of zero-shot cross-lingual transfer, with the gains of 1.0 3.7 points on text classification, named entity recognition (ner), and semantic parsing tasks.",
    "citations": 3
  },
  {
    "title": "Probing language identity encoded in pre-trained multilingual models: a typological view",
    "year": 2022,
    "authors": "Jianyu Zheng, Ying Liu",
    "url": "https://api.semanticscholar.org/CorpusId:247494088",
    "relevance": 1,
    "abstract": "Pre-trained multilingual models have been extensively used in cross-lingual information processing tasks. Existing work focuses on improving the transferring performance of pre-trained multilingual models but ignores the linguistic properties that models preserve at encoding time\u2014\u201clanguage identity\u201d. We investigated the capability of state-of-the-art pre-trained multilingual models (mBERT, XLM, XLM-R) to preserve language identity through language typology. We explored model differences and variations in terms of languages, typological features, and internal hidden layers. We found the order of ability in preserving language identity of whole model and each of its hidden layers is: mBERT > XLM-R > XLM. Furthermore, all three models capture morphological, lexical, word order and syntactic features well, but perform poorly on nominal and verbal features. Finally, our results show that the ability of XLM-R and XLM remains stable across layers, but the ability of mBERT fluctuates severely. Our findings summarize the ability of each pre-trained multilingual model and its hidden layer to store language identity and typological features. It provides insights for later researchers in processing cross-lingual information.",
    "citations": 7
  },
  {
    "title": "Transfer Learning for Multi-lingual Tasks - a Survey",
    "year": 2021,
    "authors": "A. Jafari, Behnam Heidary, R. Farahbakhsh, Mostafa Salehi, M. Jalili",
    "url": "https://api.semanticscholar.org/CorpusId:238354350",
    "relevance": 1,
    "abstract": "These days different platforms such as social media provide their clients from different backgrounds and languages the possibility to connect and exchange information. It is not surprising anymore to see comments from different languages in posts published by international celebrities or data providers. In this era, understanding cross languages content and multilingualism in natural language processing (NLP) are hot topics, and multiple efforts have tried to leverage existing technologies in NLP to tackle this challenging research problem. In this survey, we provide a comprehensive overview of the existing literature with a focus on transfer learning techniques in multilingual tasks. We also identify potential opportunities for further research in this domain.",
    "citations": 5
  },
  {
    "title": "Low-Resource Named Entity Recognition via the Pre-Training Model",
    "year": 2021,
    "authors": "Siqi Chen, Yijie Pei, Zunwang Ke, Wushour Silamu",
    "url": "https://api.semanticscholar.org/CorpusId:235453422",
    "relevance": 1,
    "abstract": "Named entity recognition (NER) is an important task in the processing of natural language, which needs to determine entity boundaries and classify them into pre-defined categories. For low-resource languages, most state-of-the-art systems require tens of thousands of annotated sentences to obtain high performance. However, there is minimal annotated data available about Uyghur and Hungarian (UH languages) NER tasks. There are also specificities in each task\u2014differences in words and word order across languages make it a challenging problem. In this paper, we present an effective solution to providing a meaningful and easy-to-use feature extractor for named entity recognition tasks: fine-tuning the pre-trained language model. Therefore, we propose a fine-tuning method for a low-resource language model, which constructs a fine-tuning dataset through data augmentation; then the dataset of a high-resource language is added; and finally the cross-language pre-trained model is fine-tuned on this dataset. In addition, we propose an attention-based fine-tuning strategy that uses symmetry to better select relevant semantic and syntactic information from pre-trained language models and apply these symmetry features to name entity recognition tasks. We evaluated our approach on Uyghur and Hungarian datasets, which showed wonderful performance compared to some strong baselines. We close with an overview of the available resources for named entity recognition and some of the open research questions.",
    "citations": 29
  },
  {
    "title": "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark",
    "year": 2023,
    "authors": "Lukasz Augustyniak, Szymon Wo'zniak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, M. Morzy, Tomasz Kajdanowicz",
    "url": "https://api.semanticscholar.org/CorpusId:259145309",
    "relevance": 1,
    "abstract": "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.",
    "citations": 14
  },
  {
    "title": "Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models",
    "year": 2024,
    "authors": "Sara Rajaee, C. Monz",
    "url": "https://api.semanticscholar.org/CorpusId:267412633",
    "relevance": 1,
    "abstract": "Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks of existing cross-lingual test data and evaluation setups, calling for a more nuanced understanding of the cross-lingual capabilities of multilingual models.",
    "citations": 11
  },
  {
    "title": "AmericasNLI: Machine translation and natural language inference systems for Indigenous languages of the Americas",
    "year": 2022,
    "authors": "Katharina Kann, Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, J. Ortega, Annette Rios Gonzales, Angela Fan, Ximena Gutierrez-Vasques, Luis Chiruzzo, G. Gim\u00e9nez-Lugo, Ricardo Ramos, Ivan Vladimir Meza Ruiz, Elisabeth Mager, Vishrav Chaudhary, Graham Neubig, Alexis Palmer, Rolando Coto-Solano, Ngoc Thang Vu",
    "url": "https://api.semanticscholar.org/CorpusId:254129330",
    "relevance": 1,
    "abstract": "Little attention has been paid to the development of human language technology for truly low-resource languages\u2014i.e., languages with limited amounts of digitally available text data, such as Indigenous languages. However, it has been shown that pretrained multilingual models are able to perform crosslingual transfer in a zero-shot setting even for low-resource languages which are unseen during pretraining. Yet, prior work evaluating performance on unseen languages has largely been limited to shallow token-level tasks. It remains unclear if zero-shot learning of deeper semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, a natural language inference dataset covering 10 Indigenous languages of the Americas. We conduct experiments with pretrained models, exploring zero-shot learning in combination with model adaptation. Furthermore, as AmericasNLI is a multiway parallel dataset, we use it to benchmark the performance of different machine translation models for those languages. Finally, using a standard transformer model, we explore translation-based approaches for natural language inference. We find that the zero-shot performance of pretrained models without adaptation is poor for all languages in AmericasNLI, but model adaptation via continued pretraining results in improvements. All machine translation models are rather weak, but, surprisingly, translation-based approaches to natural language inference outperform all other models on that task.",
    "citations": 17
  },
  {
    "title": "MSR India at SemEval-2020 Task 9: Multilingual Models Can Do Code-Mixing Too",
    "year": 2020,
    "authors": "A. Srinivasan",
    "url": "https://www.semanticscholar.org/paper/43a22b1b92df7cd8c820727ac5e51f9db7d91fab",
    "relevance": 1,
    "abstract": "In this paper, we present our system for the SemEval 2020 task on code-mixed sentiment analysis. Our system makes use of large transformer based multilingual embeddings like mBERT. Recent work has shown that these models posses the ability to solve code-mixed tasks in addition to their originally demonstrated cross-lingual abilities. We evaluate the stock versions of these models for the sentiment analysis task and also show that their performance can be improved by using unlabelled code-mixed data. Our submission (username Genius1237) achieved the second rank on the English-Hindi subtask with an F1 score of 0.726.",
    "citations": 12
  },
  {
    "title": "Cross-Language Speaker Attribute Prediction Using MIL and RL",
    "year": 2026,
    "authors": "Sunny Shu, Seyed Sahand Mohamadi Ziabari, A. M. M. Alsahag",
    "url": "https://api.semanticscholar.org/CorpusId:284544610",
    "relevance": 1,
    "abstract": "We study multilingual speaker attribute prediction under linguistic variation, domain mismatch, and data imbalance across languages. We propose RLMIL-DAT, a multilingual extension of the reinforced multiple instance learning framework that combines reinforcement learning based instance selection with domain adversarial training to encourage language invariant utterance representations. We evaluate the approach on a five language Twitter corpus in a few shot setting and on a VoxCeleb2 derived corpus covering forty languages in a zero shot setting for gender and age prediction. Across a wide range of model configurations and multiple random seeds, RLMIL-DAT consistently improves Macro F1 compared to standard multiple instance learning and the original reinforced multiple instance learning framework. The largest gains are observed for gender prediction, while age prediction remains more challenging and shows smaller but positive improvements. Ablation experiments indicate that domain adversarial training is the primary contributor to the performance gains, enabling effective transfer from high resource English to lower resource languages by discouraging language specific cues in the shared encoder. In the zero shot setting on the smaller VoxCeleb2 subset, improvements are generally positive but less consistent, reflecting limited statistical power and the difficulty of generalizing to many unseen languages. Overall, the results demonstrate that combining instance selection with adversarial domain adaptation is an effective and robust strategy for cross lingual speaker attribute prediction.",
    "citations": 0
  },
  {
    "title": "HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon",
    "year": 2025,
    "authors": "James Ald Teves, Ray Daniel Cal, Josh Magdiel Villaluz, Jean Malolos, Mico C. Magtira, Ramon Rodriguez, Mideth B. Abisado, Joseph Marvin Imperial",
    "url": "https://api.semanticscholar.org/CorpusId:282056725",
    "relevance": 1,
    "abstract": "The language of Hiligaynon, spoken predominantly by the people of Panay Island, Negros Occidental, and Soccsksargen in the Philippines, remains underrepresented in language processing research due to the absence of annotated corpora and baseline models. This study introduces HiligayNER, the first publicly available baseline model for the task of Named Entity Recognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains over 8,000 annotated sentences collected from publicly available news articles, social media posts, and literary texts. Two Transformer-based models, mBERT and XLM-RoBERTa, were fine-tuned on this collected corpus to build versions of HiligayNER. Evaluation results show strong performance, with both models achieving over 80% in precision, recall, and F1-score across entity types. Furthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates promising transferability, suggesting the broader applicability of HiligayNER for multilingual NLP in low-resource settings. This work aims to contribute to language technology development for underrepresented Philippine languages, specifically for Hiligaynon, and support future research in regional language processing.",
    "citations": 0
  },
  {
    "title": "Too Many Cooks Spoil the Model: Are Bilingual Models for Slovene Better than a Large Multilingual Model?",
    "year": 2023,
    "authors": "Pranaydeep Singh, Aaron Maladry, Els Lefever",
    "url": "https://api.semanticscholar.org/CorpusId:258486899",
    "relevance": 1,
    "abstract": "This paper investigates whether adding data of typologically closer languages improves the performance of transformer-based models for three different downstream tasks, namely Part-of-Speech tagging, Named Entity Recognition, and Sentiment Analysis, compared to a monolingual and plain multilingual language model. For the presented pilot study, we performed experiments for the use case of Slovene, a low(er)-resourced language belonging to the Slavic language family. The experiments were carried out in a controlled setting, where a monolingual model for Slovene was compared to combined language models containing Slovene, trained with the same amount of Slovene data. The experimental results show that adding typologically closer languages indeed improves the performance of the Slovene language model, and even succeeds in outperforming the large multilingual XLM-RoBERTa model for NER and PoS-tagging. We also reveal that, contrary to intuition, distantly or unrelated languages also combine admirably with Slovene, often out-performing XLM-R as well. All the bilingual models used in the experiments are publicly available at https://github.com/pranaydeeps/BLAIR",
    "citations": 5
  },
  {
    "title": "SIDLR: Slot and Intent Detection Models for Low-Resource Language Varieties",
    "year": 2023,
    "authors": "S. Kwon, Gagan Bhatia, El Moatez Billah Nagoudi, Alcides Alcoba Inciarte, M. Abdul-Mageed",
    "url": "https://api.semanticscholar.org/CorpusId:258378300",
    "relevance": 1,
    "abstract": "Intent detection and slot filling are two critical tasks in spoken and natural language understandingfor task-oriented dialog systems. In this work, we describe our participation in slot and intent detection for low-resource language varieties (SID4LR) (Aepli et al., 2023). We investigate the slot and intent detection (SID) tasks using a wide range of models and settings. Given the recent success of multitask promptedfinetuning of the large language models, we also test the generalization capability of the recent encoder-decoder model mT0 (Muennighoff et al., 2022) on new tasks (i.e., SID) in languages they have never intentionally seen. We show that our best model outperforms the baseline by a large margin (up to +30 F1 points) in both SID tasks.",
    "citations": 5
  },
  {
    "title": "SinaAI at SemEval-2023 Task 3: A Multilingual Transformer Language Model-based Approach for the Detection of News Genre, Framing and Persuasion Techniques",
    "year": 2023,
    "authors": "Aryan Sadeghi, Reza Alipour, Kamyar Taeb, Parimehr Morassafar, Nima Salemahim, Ehsaneddin Asgari",
    "url": "https://api.semanticscholar.org/CorpusId:259376594",
    "relevance": 1,
    "abstract": "This paper describes SinaAI\u2019s participation in SemEval-2023 Task 3, which involves detecting propaganda in news articles across multiple languages. The task comprises three sub-tasks: (i) genre detection, (ii) news framing,and (iii) persuasion technique identification.The employed dataset includes news articles in nine languages and domains, including English, French, Italian, German, Polish, Russian, Georgian, Greek, and Spanish, with labeled instances of news framing, genre, and persuasion techniques. Our approach combines fine-tuning multilingual language models such as XLM, LaBSE, and mBERT with data augmentation techniques. Our experimental results show that XLM outperforms other models in terms of F1-Micro in and F1-Macro, and the ensemble of XLM and LaBSE achieved the best performance. Our study highlights the effectiveness of multilingual sentence embedding models in multilingual propaganda detection. Our models achieved highest score for two languages (greek and italy) in sub-task 1 and one language (Russian) for sub-task 2.",
    "citations": 5
  },
  {
    "title": "Multilingual Multiword Expression Identification Using Lateral Inhibition and Domain Adaptation",
    "year": 2023,
    "authors": "Andrei-Marius Avram, V. Mititelu, V. Pais, Dumitru-Clementin Cercel, Stefan Trausan-Matu",
    "url": "https://api.semanticscholar.org/CorpusId:259055967",
    "relevance": 1,
    "abstract": "Correctly identifying multiword expressions (MWEs) is an important task for most natural language processing systems since their misidentification can result in ambiguity and misunderstanding of the underlying text. In this work, we evaluate the performance of the mBERT model for MWE identification in a multilingual context by training it on all 14 languages available in version 1.2 of the PARSEME corpus. We also incorporate lateral inhibition and language adversarial training into our methodology to create language-independent embeddings and improve its capabilities in identifying multiword expressions. The evaluation of our models shows that the approach employed in this work achieves better results compared to the best system of the PARSEME 1.2 competition, MTLB-STRUCT, on 11 out of 14 languages for global MWE identification and on 12 out of 14 languages for unseen MWE identification. Additionally, averaged across all languages, our best approach outperforms the MTLB-STRUCT system by 1.23% on global MWE identification and by 4.73% on unseen global MWE identification.",
    "citations": 4
  },
  {
    "title": "Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining",
    "year": 2022,
    "authors": "Asa Cooper Stickland, Sailik Sengupta, Jason Krone, Saab Mansour, He He",
    "url": "https://api.semanticscholar.org/CorpusId:256827260",
    "relevance": 1,
    "abstract": "Advances in neural modeling have achieved state-of-the-art (SOTA) results on public natural language processing (NLP) benchmarks, at times surpassing human performance. However, there is a gap between public benchmarks and real-world applications where noise, such as typographical or grammatical mistakes, is abundant and can result in degraded performance. Unfortunately, works which evaluate the robustness of neural models on noisy data and propose improvements, are limited to the English language. Upon analyzing noise in different languages, we observe that noise types vary greatly across languages. Thus, existing investigations do not generalize trivially to multilingual settings. To benchmark the performance of pretrained multilingual language models, we construct noisy datasets covering five languages and four NLP tasks and observe a clear gap in the performance between clean and noisy data in the zero-shot cross-lingual setting. After investigating several ways to boost the robustness of multilingual models in this setting, we propose Robust Contrastive Pretraining (RCP). RCP combines data augmentation with a contrastive loss term at the pretraining stage and achieves large improvements on noisy (and original test data) across two sentence-level (+3.2%) and two sequence-labeling (+10 F1-score) multilingual classification tasks.",
    "citations": 11
  },
  {
    "title": "Probing Multilingual Language Models for Discourse",
    "year": 2021,
    "authors": "Murathan Kurfali, R. Ostling",
    "url": "https://api.semanticscholar.org/CorpusId:235377293",
    "relevance": 1,
    "abstract": "Pre-trained multilingual language models have become an important building block in multilingual Natural Language Processing. In the present paper, we investigate a range of such models to find out how well they transfer discourse-level knowledge across languages. This is done with a systematic evaluation on a broader set of discourse-level tasks than has been previously been assembled. We find that the XLM-RoBERTa family of models consistently show the best performance, by simultaneously being good monolingual models and degrading relatively little in a zero-shot setting. Our results also indicate that model distillation may hurt the ability of cross-lingual transfer of sentence representations, while language dissimilarity at most has a modest effect. We hope that our test suite, covering 5 tasks with a total of 22 languages in 10 distinct families, will serve as a useful evaluation platform for multilingual performance at and beyond the sentence level.",
    "citations": 18
  },
  {
    "title": "Toward Zero-Shot and Zero-Resource Multilingual Question Answering",
    "year": 2022,
    "authors": "Chia-Chih Kuo, Kuan-Yu Chen",
    "url": "https://api.semanticscholar.org/CorpusId:252344707",
    "relevance": 1,
    "abstract": "In recent years, multilingual question answering has been an emergent research topic and has attracted much attention. Although systems for English and other rich-resource languages that rely on various advanced deep learning-based techniques have been highly developed, most of them in low-resource languages are impractical due to data insufficiency. Accordingly, many studies have attempted to improve the performance of low-resource languages in a zero-shot or few-shot manner based on multilingual bidirectional encoder representations from transformers (mBERT) by transferring knowledge learned from rich-resource languages to low-resource languages. Most methods require either a large amount of unlabeled data or a small set of labeled data for low-resource languages. In Wikipedia, 169 languages have less than 10,000 articles, and 48 languages have less than 1,000 articles. This reason motivates us to conduct a zero-shot multilingual question answering task under a zero-resource scenario. Thus, this study proposes a framework to fine-tune the original mBERT using data from rich-resource languages, and the resulting model can be used for low-resource languages in a zero-shot and zero-resource manner. Compared to several baseline systems, which require millions of unlabeled data for low-resource languages, the performance of our proposed framework is not only highly comparative but is also better for languages used in training.",
    "citations": 10
  },
  {
    "title": "How Language-Dependent is Emotion Detection? Evidence from Multilingual BERT",
    "year": 2022,
    "authors": "Luna De Bruyne, Pranaydeep Singh, Orph\u00e9e De Clercq, Els Lefever, Veronique Hoste",
    "url": "https://api.semanticscholar.org/CorpusId:256461004",
    "relevance": 1,
    "abstract": "As emotion analysis in text has gained a lot of attention in the field of natural language processing, differences in emotion expression across languages could have consequences for how emotion detection models work. We evaluate the language-dependence of an mBERT-based emotion detection model by comparing language identification performance before and after fine-tuning on emotion detection, and performing (adjusted) zero-shot experiments to assess whether emotion detection models rely on language-specific information. When dealing with typologically dissimilar languages, we found evidence for the language-dependence of emotion detection.",
    "citations": 10
  },
  {
    "title": "Exploiting In-Domain Bilingual Corpora for Zero-Shot Transfer Learning in NLU of Intra-Sentential Code-Switching Chatbot Interactions",
    "year": 2022,
    "authors": "Maia Aguirre, Manex Serras, Laura Garc\u00eda-Sardi\u00f1a, Jacobo Lopez-Fernandez, Ariane M\u00e9ndez, A. D. Pozo",
    "url": "https://api.semanticscholar.org/CorpusId:257806456",
    "relevance": 1,
    "abstract": "Code-switching (CS) is a very common phenomenon in regions with various co-existing languages. Since CS is such a frequent habit in informal communications, both spoken and written, it also arises naturally in Human-Machine Interactions. Therefore, in order for natural language understanding (NLU) not to be degraded, CS must be taken into account when developing chatbots. The co-existence of multiple languages in a single NLU model has become feasible with multilingual language representation models such as mBERT. In this paper, the efficacy of zero-shot cross-lingual transfer learning with mBERT for NLU is evaluated on a Basque-Spanish CS chatbot corpus, comparing the performance of NLU models trained using in-domain chatbot utterances in Basque and/or Spanish without CS. The results obtained indicate that training joint multi-intent classification and entity recognition models on both languages simultaneously achieves best performance, better capturing the CS patterns.",
    "citations": 4
  },
  {
    "title": "How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?",
    "year": 2022,
    "authors": "Hailong Jin, Tiansi Dong, Lei Hou, Juanzi Li, Hui Chen, Zelin Dai, Yincen Qu",
    "url": "https://api.semanticscholar.org/CorpusId:248780131",
    "relevance": 1,
    "abstract": "Cross-lingual Entity Typing (CLET) aims at improving the quality of entity type prediction by transferring semantic knowledge learned from rich-resourced languages to low-resourced languages. In this paper, by utilizing multilingual transfer learning via the mixture-of-experts approach, our model dynamically capture the relationship between target language and each source language, and effectively generalize to predict types of unseen entities in new languages. Extensive experiments on multi-lingual datasets show that our method significantly outperforms multiple baselines and can robustly handle negative transfer. We questioned the relationship between language similarity and the performance of CLET. A series of experiments refute the commonsense that the more source the better, and suggest the Similarity Hypothesis for CLET.",
    "citations": 4
  },
  {
    "title": "Punctuation Restoration in Spanish Customer Support Transcripts using Transfer Learning",
    "year": 2022,
    "authors": "Xiliang Zhu, Shayna Gardiner, David Rossouw, T. Rold'an, Simon Corston-Oliver",
    "url": "https://api.semanticscholar.org/CorpusId:249152161",
    "relevance": 1,
    "abstract": "t",
    "citations": 1
  },
  {
    "title": "Improving Indonesian Text Classification Using Multilingual Language Model",
    "year": 2020,
    "authors": "Ilham Firdausi Putra, A. Purwarianti",
    "url": "https://api.semanticscholar.org/CorpusId:221655795",
    "relevance": 1,
    "abstract": "Compared to English, the amount of labeled data for Indonesian text classification tasks is very small. Recently developed multilingual language models have shown its ability to create multilingual representations effectively. This paper investigates the effect of combining English and Indonesian data on building Indonesian text classification (e.g., sentiment analysis and hate speech) using multilingual language models. Using the feature-based approach, we observe its performance on various data sizes and total added English data. The experiment showed that the addition of English data, especially if the amount of Indonesian data is small, improves performance. Using the fine-tuning approach, we further showed its effectiveness in utilizing the English language to build Indonesian text classification models.",
    "citations": 12
  },
  {
    "title": "Multilingual text categorization and sentiment analysis: a comparative analysis of the utilization of multilingual approaches for classifying twitter data",
    "year": 2023,
    "authors": "George Manias, Argyro Mavrogiorgou, Athanasios Kiourtis, Chrysostomos Symvoulidis, D. Kyriazis",
    "url": "https://api.semanticscholar.org/CorpusId:258584732",
    "relevance": 1,
    "abstract": "Text categorization and sentiment analysis are two of the most typical natural language processing tasks with various emerging applications implemented and utilized in different domains, such as health care and policy making. At the same time, the tremendous growth in the popularity and usage of social media, such as Twitter, has resulted on an immense increase in user-generated data, as mainly represented by the corresponding texts in users\u2019 posts. However, the analysis of these specific data and the extraction of actionable knowledge and added value out of them is a challenging task due to the domain diversity and the high multilingualism that characterizes these data. The latter highlights the emerging need for the implementation and utilization of domain-agnostic and multilingual solutions. To investigate a portion of these challenges this research work performs a comparative analysis of multilingual approaches for classifying both the sentiment and the text of an examined multilingual corpus. In this context, four multilingual BERT-based classifiers and a zero-shot classification approach are utilized and compared in terms of their accuracy and applicability in the classification of multilingual data. Their comparison has unveiled insightful outcomes and has a twofold interpretation. Multilingual BERT-based classifiers achieve high performances and transfer inference when trained and fine-tuned on multilingual data. While also the zero-shot approach presents a novel technique for creating multilingual solutions in a faster, more efficient, and scalable way. It can easily be fitted to new languages and new tasks while achieving relatively good results across many languages. However, when efficiency and scalability are less important than accuracy, it seems that this model, and zero-shot models in general, can not be compared to fine-tuned and trained multilingual BERT-based classifiers.",
    "citations": 64
  },
  {
    "title": "Comparative Approaches to Sentiment Analysis Using Datasets in Major European and Arabic Languages",
    "year": 2025,
    "authors": "Mikhail Krasitskii, O. Kolesnikova, L. Chanona-Hern\u00e1ndez, Grigori Sidorov, A. Gelbukh",
    "url": "https://api.semanticscholar.org/CorpusId:275446314",
    "relevance": 1,
    "abstract": "This study explores transformer-based models such as BERT, mBERT, and XLM-R for multi-lingual sentiment analysis across diverse linguistic structures. Key contributions include the identification of XLM-R\u2019s superior adaptability in morphologically complex languages, achieving accuracy levels above 88%. The work highlights fine-tuning strategies and emphasizes their significance for improving sentiment classification in underrepresented languages.",
    "citations": 5
  },
  {
    "title": "Beyond the English Web: Zero-Shot Cross-Lingual and Lightweight Monolingual Classification of Registers",
    "year": 2021,
    "authors": "Liina Repo, Valtteri Skantsi, Samuel R\u00f6nnqvist, Saara Hellstr\u00f6m, Miika Oinonen, Anna Salmela, D. Biber, Jesse Egbert, Sampo Pyysalo, Veronika Laippala",
    "url": "https://api.semanticscholar.org/CorpusId:231925122",
    "relevance": 1,
    "abstract": "We explore cross-lingual transfer of register classification for web documents. Registers, that is, text varieties such as blogs or news are one of the primary predictors of linguistic variation and thus affect the automatic processing of language. We introduce two new register-annotated corpora, FreCORE and SweCORE, for French and Swedish. We demonstrate that deep pre-trained language models perform strongly in these languages and outperform previous state-of-the-art in English and Finnish. Specifically, we show 1) that zero-shot cross-lingual transfer from the large English CORE corpus can match or surpass previously published monolingual models, and 2) that lightweight monolingual classification requiring very little training data can reach or surpass our zero-shot performance. We further analyse classification results finding that certain registers continue to pose challenges in particular for cross-lingual transfer.",
    "citations": 22
  },
  {
    "title": "Automatic Sexism Detection with Multilingual Transformer Models AIT FHSTP@EXIST2021",
    "year": 2021,
    "authors": "Mina Sch\u00fctz, Jaqueline Boeck, Daria Liakhovets, D. Slijep\u010devi\u0107, Armin Kirchknopf, Manuel Hecht, Johannes Bogensperger, S. Schlarb, Alexander Schindler, M. Zeppelzauer",
    "url": "https://api.semanticscholar.org/CorpusId:235441725",
    "relevance": 1,
    "abstract": "Sexism has become an increasingly major problem on social networks during the last years. The first shared task on sEXism Identification in Social neTworks (EXIST) at IberLEF 2021 is an international competition in the field of Natural Language Processing (NLP) with the aim to automatically identify sexism in social media content by applying machine learning methods. Thereby sexism detection is formulated as a coarse (binary) classification problem and a fine-grained classification task that distinguishes multiple types of sexist content (e.g., dominance, stereotyping, and objectification). This paper presents the contribution of the AIT_FHSTP team at the EXIST2021 benchmark for both tasks. To solve the tasks we applied two multilingual transformer models, one based on multilingual BERT and one based on XLM-R. Our approach uses two different strategies to adapt the transformers to the detection of sexist content: first, unsupervised pre-training with additional data and second, supervised fine-tuning with additional and augmented data. For both tasks our best model is XLM-R with unsupervised pre-training on the EXIST data and additional datasets and fine-tuning on the provided dataset. The best run for the binary classification (task 1) achieves a macro F1-score of 0.7752 and scores 5th rank in the benchmark; for the multiclass classification (task 2) our best submission scores 6th rank with a macro F1-score of 0.5589.",
    "citations": 31
  },
  {
    "title": "AI unveiled personalities: Profiling optimistic and pessimistic attitudes in Hindi dataset using transformer\u2010based models",
    "year": 2024,
    "authors": "Dipika Jain, Akshi Kumar",
    "url": "https://api.semanticscholar.org/CorpusId:268391713",
    "relevance": 1,
    "abstract": "Both optimism and pessimism are intricately intertwined with an individual's inherent personality traits and people of all personality types can exhibit a wide range of attitudes and behaviours, including levels of optimism and pessimism. This paper undertakes a comprehensive analysis of optimistic and pessimistic tendencies present within Hindi textual data, employing transformer\u2010based models. The research represents a pioneering effort to define and establish an interaction between the personality and attitude chakras within the realm of human psychology. Introducing an innovative \u201cChakra\u201d system to illustrate complex interrelationships within human psychology, this work aligns the Myers\u2010Briggs Type Indicator (MBTI) personality traits with optimistic and pessimistic attitudes, enriching our understanding of emotional projection in text. The study employs meticulously fine\u2010tuned transformer models\u2014specifically mBERT, XLM\u2010RoBERTa, IndicBERT, mDeBERTa and a novel stacked mDeBERTa\u2014trained on the novel Hindi dataset \u2018\u092e\u0928\u094b\u092d\u093e\u0935\u2019 (pronounced as Manobhav). Remarkably, the proposed Stacked mDeBERTa model outperforms others, recording an accuracy of 0.7785 along with elevated precision, recall, and F1 score values. Notably, its ROC AUC score of 0.7226 underlines its robustness in distinguishing between positive and negative emotional attitudes. The comparative analysis highlights the superiority of the Stacked mDeBERTa model in effectively capturing emotional attitudes in Hindi text.",
    "citations": 2
  },
  {
    "title": "skLEP: A Slovak General Language Understanding Benchmark",
    "year": 2025,
    "authors": "Marek Suppa, Andrej Ridzik, Daniel Hl\u00e1dek, Tomas Javurek, Viktoria Ondrejova, Krist\u00edna S\u00e1sikov\u00e1, Martin Tamajka, Mari\u00e1n Simko",
    "url": "https://api.semanticscholar.org/CorpusId:280012311",
    "relevance": 1,
    "abstract": "In this work, we introduce skLEP, the first comprehensive benchmark specifically designed for evaluating Slovak natural language understanding (NLU) models. We have compiled skLEP to encompass nine diverse tasks that span token-level, sentence-pair, and document-level challenges, thereby offering a thorough assessment of model capabilities. To create this benchmark, we curated new, original datasets tailored for Slovak and meticulously translated established English NLU resources. Within this paper, we also present the first systematic and extensive evaluation of a wide array of Slovak-specific, multilingual, and English pre-trained language models using the skLEP tasks. Finally, we also release the complete benchmark data, an open-source toolkit facilitating both fine-tuning and evaluation of models, and a public leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering reproducibility and drive future research in Slovak NLU.",
    "citations": 1
  },
  {
    "title": "Team JUSTR00 at SemEval-2023 Task 3: Transformers for News Articles Classification",
    "year": 2023,
    "authors": "Ahmed Al-Qarqaz, Malak Abdullah",
    "url": "https://api.semanticscholar.org/CorpusId:259376866",
    "relevance": 1,
    "abstract": "The SemEval-2023 Task 3 competition offers participants a multi-lingual dataset with three schemes one for each subtask. The competition challenges participants to construct machine learning systems that can categorize news articles based on their nature and style of writing. We esperiment with many state-of-the-art transformer-based language models proposed in the natural language processing literature and report the results of the best ones. Our top performing model is based on a transformer called \u201cLongformer\u201d and has achieved an F1-Micro score of 0.256 on the English version of subtask-1 and F1-Macro of 0.442 on subtask-2 on the test data. We also experiment with a number of state-of-the-art multi-lingual transformer-based models and report the results of the best performing ones.",
    "citations": 2
  },
  {
    "title": "Cross-lingual Language Model Pretraining",
    "year": 2019,
    "authors": "Guillaume Lample, Alexis Conneau",
    "url": "https://api.semanticscholar.org/CorpusId:58981712",
    "relevance": 1,
    "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT\u201916 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT\u201916 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",
    "citations": 2922
  },
  {
    "title": "mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs",
    "year": 2021,
    "authors": "Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang Xian-Ling Mao, Saksham Singhal, Heyan Huang, Furu Wei",
    "url": "https://api.semanticscholar.org/CorpusId:233296501",
    "relevance": 1,
    "abstract": "Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, machine translation, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for text-to-text pre-training. We evaluate the methods on seven multilingual benchmark datasets, including sentence classification, named entity recognition, question answering, and abstractive summarization. Experimental results show that the proposed mT6 improves cross-lingual transferability over mT5.",
    "citations": 78
  }
]