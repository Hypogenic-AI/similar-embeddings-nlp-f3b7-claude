\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{wu2020emerging,pires2019multilingual}
\citation{miller1995wordnet}
\citation{wu2020emerging,pires2019multilingual,conneau2020unsupervised}
\citation{zhang2019crosslingual}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{mikolov2013exploiting}
\citation{conneau2018word}
\citation{ruder2019survey}
\citation{devlin2019bert}
\citation{conneau2020unsupervised}
\citation{pires2019multilingual}
\citation{wu2020emerging}
\citation{dufter2021first}
\citation{libovicky2020language}
\citation{zhang2019crosslingual}
\citation{upadhyay2017beyond}
\citation{scarlini2020sensembert}
\citation{raganato2020xlwic}
\citation{martelli2021semeval}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\citation{tenney2019bert}
\citation{wu2020emerging}
\citation{devlin2019bert}
\citation{conneau2020unsupervised}
\citation{conneau2018word}
\citation{camacho2017semeval}
\citation{martelli2021semeval}
\citation{miller1995wordnet}
\citation{libovicky2020language}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\newlabel{sec:method}{{3}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Models}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:models}{{3.1}{3}{Models}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Datasets}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:data}{{3.2}{3}{Datasets}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Embedding Extraction}{3}{subsection.3.3}\protected@file@percent }
\newlabel{sec:embedding}{{3.3}{3}{Embedding Extraction}{subsection.3.3}{}}
\newlabel{eq:centering}{{1}{4}{Embedding Extraction}{equation.3.1}{}}
\newlabel{eq:cosine}{{2}{4}{Embedding Extraction}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Experimental Protocol}{4}{subsection.3.4}\protected@file@percent }
\newlabel{sec:experiments}{{3.4}{4}{Experimental Protocol}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{4}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experiment 1: Translation Equivalents Have High Cross-Lingual Similarity}{4}{subsection.4.1}\protected@file@percent }
\newlabel{sec:exp1}{{4.1}{4}{Experiment 1: Translation Equivalents Have High Cross-Lingual Similarity}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experiment 2: Polysemy Degrades Cross-Lingual Alignment}{4}{subsection.4.2}\protected@file@percent }
\newlabel{sec:exp2}{{4.2}{4}{Experiment 2: Polysemy Degrades Cross-Lingual Alignment}{subsection.4.2}{}}
\citation{tenney2019bert}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Cosine similarity of translation pairs vs.\ random pairs (last layer, centered). All differences are significant ($p < 10^{-300}$, Mann--Whitney $U$ test). Best Cohen's $d$\xspace  in {\bf  bold}.\relax }}{5}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:exp1}{{1}{5}{Cosine similarity of translation pairs vs.\ random pairs (last layer, centered). All differences are significant ($p < 10^{-300}$, Mann--Whitney $U$ test). Best \cohensd in {\bf bold}.\relax }{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Cosine similarity distributions for translation pairs (colored) vs.\ random pairs (gray) across five language pairs. Translation pairs are clearly separated from random pairs in both models, with the gap largest for typologically similar languages.\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:exp1}{{1}{5}{Cosine similarity distributions for translation pairs (colored) vs.\ random pairs (gray) across five language pairs. Translation pairs are clearly separated from random pairs in both models, with the gap largest for typologically similar languages.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Experiment 3: Context Recovers Sense-Level Alignment}{5}{subsection.4.3}\protected@file@percent }
\newlabel{sec:exp3}{{4.3}{5}{Experiment 3: Context Recovers Sense-Level Alignment}{subsection.4.3}{}}
\citation{conneau2020unsupervised}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Mean cosine similarity by polysemy category (last layer, centered). Cohen's $d$\xspace  compares monosemous vs.\ highly polysemous words. $\rho $: Spearman correlation between sense count and similarity ($^{***}$: $p < 0.001$). Best similarity in {\bf  bold}.\relax }}{6}{table.caption.4}\protected@file@percent }
\newlabel{tab:exp2}{{2}{6}{Mean cosine similarity by polysemy category (last layer, centered). \cohensd compares monosemous vs.\ highly polysemous words. $\rho $: Spearman correlation between sense count and similarity ($^{***}$: $p < 0.001$). Best similarity in {\bf bold}.\relax }{table.caption.4}{}}
\newlabel{fig:exp2_box}{{2a}{6}{Similarity by polysemy category\relax }{figure.caption.5}{}}
\newlabel{sub@fig:exp2_box}{{a}{6}{Similarity by polysemy category\relax }{figure.caption.5}{}}
\newlabel{fig:exp2_scatter}{{2b}{6}{Similarity vs.\ sense count\relax }{figure.caption.5}{}}
\newlabel{sub@fig:exp2_scatter}{{b}{6}{Similarity vs.\ sense count\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {\em  (Left)}Cosine similarity distributions by polysemy category. Monosemous words (1 sense) consistently show higher similarity than polysemous words. {\em  (Right)}Negative correlation between \textsc  {WordNet}\xspace  sense count and cross-lingual cosine similarity.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:exp2}{{2}{6}{\figleft Cosine similarity distributions by polysemy category. Monosemous words (1 sense) consistently show higher similarity than polysemous words. \figright Negative correlation between \wordnet sense count and cross-lingual cosine similarity.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Experiment 4: Correlation with Human Judgments}{6}{subsection.4.4}\protected@file@percent }
\newlabel{sec:exp4}{{4.4}{6}{Experiment 4: Correlation with Human Judgments}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{6}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{6}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Interpretation of Results}{6}{subsection.5.1}\protected@file@percent }
\citation{libovicky2020language}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Same-sense vs.\ different-sense cosine similarity from \textsc  {MCL-WiC}\xspace  (layer\nobreakspace  {}10, centered). All differences are significant ($p < 10^{-30}$). Best Cohen's $d$\xspace  in {\bf  bold}.\relax }}{7}{table.caption.6}\protected@file@percent }
\newlabel{tab:exp3}{{3}{7}{Same-sense vs.\ different-sense cosine similarity from \mclwic (layer~10, centered). All differences are significant ($p < 10^{-30}$). Best \cohensd in {\bf bold}.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cohen's $d$\xspace  for same-sense vs.\ different-sense discrimination across transformer layers (centered embeddings). Sense discrimination increases monotonically from early to upper-middle layers, peaking at layer\nobreakspace  {}10 in both models.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:exp3_layers}{{3}{7}{\cohensd for same-sense vs.\ different-sense discrimination across transformer layers (centered embeddings). Sense discrimination increases monotonically from early to upper-middle layers, peaking at layer~10 in both models.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Implications}{7}{subsection.5.2}\protected@file@percent }
\citation{wu2020emerging,libovicky2020language}
\bibdata{references}
\bibcite{camacho2017semeval}{{1}{2017}{{Camacho-Collados et~al.}}{{Camacho-Collados, Pilehvar, Collier, and Navigli}}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Correlation with human similarity judgments on \textsc  {SemEval-2017}\xspace  Task\nobreakspace  {}2 (last layer). $\rho $: Spearman; $r$: Pearson. Centering consistently improves correlations. Best $\rho $ in {\bf  bold}.\relax }}{8}{table.caption.8}\protected@file@percent }
\newlabel{tab:exp4}{{4}{8}{Correlation with human similarity judgments on \semeval Task~2 (last layer). $\rho $: Spearman; $r$: Pearson. Centering consistently improves correlations. Best $\rho $ in {\bf bold}.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Limitations}{8}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{8}{Conclusion}{section.6}{}}
\bibcite{conneau2018word}{{2}{2018}{{Conneau et~al.}}{{Conneau, Lample, Ranzato, Denoyer, and J{\'e}gou}}}
\bibcite{conneau2020unsupervised}{{3}{2020}{{Conneau et~al.}}{{Conneau, Khandelwal, Goyal, Chaudhary, Wenzek, Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov}}}
\bibcite{devlin2019bert}{{4}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{dufter2021first}{{5}{2021}{{Dufter and Sch{\"u}tze}}{{}}}
\bibcite{libovicky2020language}{{6}{2020}{{Libovick{\'y} et~al.}}{{Libovick{\'y}, Rosa, and Fraser}}}
\bibcite{martelli2021semeval}{{7}{2021}{{Martelli et~al.}}{{Martelli, Kalach, Tola, and Navigli}}}
\bibcite{mikolov2013exploiting}{{8}{2013}{{Mikolov et~al.}}{{Mikolov, Le, and Sutskever}}}
\bibcite{miller1995wordnet}{{9}{1995}{{Miller}}{{}}}
\bibcite{pires2019multilingual}{{10}{2019}{{Pires et~al.}}{{Pires, Schlinger, and Garrette}}}
\bibcite{raganato2020xlwic}{{11}{2020}{{Raganato et~al.}}{{Raganato, Pasini, Camacho-Collados, and Pilehvar}}}
\bibcite{ruder2019survey}{{12}{2019}{{Ruder et~al.}}{{Ruder, Vuli{\'c}, and S{\o }gaard}}}
\bibcite{scarlini2020sensembert}{{13}{2020}{{Scarlini et~al.}}{{Scarlini, Pasini, and Navigli}}}
\bibcite{tenney2019bert}{{14}{2019}{{Tenney et~al.}}{{Tenney, Das, and Pavlick}}}
\bibcite{upadhyay2017beyond}{{15}{2017}{{Upadhyay et~al.}}{{Upadhyay, Chang, Taddy, Kalai, and Zou}}}
\bibcite{wu2020emerging}{{16}{2020}{{Wu et~al.}}{{Wu, Conneau, Li, Zettlemoyer, and Stoyanov}}}
\bibcite{zhang2019crosslingual}{{17}{2019}{{Zhang et~al.}}{{Zhang, Yin, Zhu, and Zweigenbaum}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Results}{11}{appendix.A}\protected@file@percent }
\newlabel{sec:appendix}{{A}{11}{Additional Results}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Experiment 1: Layer Analysis}{11}{subsection.A.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Translation pair cosine similarity across layers for both models. Similarity generally increases with layer depth, with the last layer providing the highest type-level alignment.\relax }}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig:exp1_layers}{{4}{11}{Translation pair cosine similarity across layers for both models. Similarity generally increases with layer depth, with the last layer providing the highest type-level alignment.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Experiment 3: Raw Similarity Without Centering}{11}{subsection.A.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Same-sense vs.\ different-sense similarity \emph  {without centering} (last layer). \textsc  {XLM-R}\xspace  produces near-saturated similarities that mask meaningful sense discrimination.\relax }}{11}{table.caption.11}\protected@file@percent }
\newlabel{tab:exp3_raw}{{5}{11}{Same-sense vs.\ different-sense similarity \emph {without centering} (last layer). \xlmr produces near-saturated similarities that mask meaningful sense discrimination.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Experiment 3: Similarity Across Layers}{11}{subsection.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Experiment 4: Visualization}{11}{subsection.A.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Same-sense and different-sense cosine similarity across layers (centered). The gap between conditions widens from early to upper-middle layers, with both converging slightly at the last layer.\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:exp3_sim}{{5}{12}{Same-sense and different-sense cosine similarity across layers (centered). The gap between conditions widens from early to upper-middle layers, with both converging slightly at the last layer.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Spearman correlations with human similarity judgments on \textsc  {SemEval-2017}\xspace  Task\nobreakspace  {}2. \textsc  {mBERT}\xspace  achieves higher correlations than \textsc  {XLM-R}\xspace  , and centering provides a consistent boost for both models.\relax }}{12}{figure.caption.13}\protected@file@percent }
\newlabel{fig:exp4_vis}{{6}{12}{Spearman correlations with human similarity judgments on \semeval Task~2. \mbert achieves higher correlations than \xlmr , and centering provides a consistent boost for both models.\relax }{figure.caption.13}{}}
